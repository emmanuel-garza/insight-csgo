{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bittfgpuconda9469bb1a84b8495baabdd4607b460ff6",
   "display_name": "Python 3.7.6 64-bit ('tf-gpu': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modues to be used\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# # The GPU id to use, usually either \"0\" or \"1\";\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\";  \n",
    " \n",
    "# Do other imports now...\n",
    "# import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from pickle files\n",
    "\n",
    "# -> Player dictionary with the dataFrames\n",
    "f = open('../data/dict_player.pickle', 'rb')\n",
    "dict_player = pickle.load(f)\n",
    "\n",
    "# -> Map dataFrames\n",
    "f = open('../data/df_map.pickle','rb')\n",
    "df_map = pickle.load(f)\n",
    "\n",
    "# -> Map dictionary\n",
    "f = open('../data/dict_map.pickle','rb')\n",
    "dict_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_WEIGHT = 365\n",
    "\n",
    "time_1 = datetime.now()\n",
    "\n",
    "# If we want matches only between only ranked teams\n",
    "df_tmp = df_map[ (df_map['date']>datetime(2019,1,1)) &\n",
    "    ( (df_map['team_rank_1']>0) & (df_map['team_rank_2']>0) ) ]\n",
    "\n",
    "\n",
    "df_two_year = df_map[ (df_map['date']>datetime(2018,1,1)) ]\n",
    "\n",
    "\n",
    "dict_train = {}\n",
    "\n",
    "print( len( df_tmp['map_id']) )\n",
    "\n",
    "#2000 -> ~4min\n",
    "for map_id in df_tmp['map_id'][:]:\n",
    "\n",
    "    map_date = df_tmp.loc[map_id]['date']\n",
    "\n",
    "    dict_train[map_id] = {}    \n",
    "\n",
    "    # Here we are taking the rankings to be non-zero\n",
    "    rank_1 = df_tmp.loc[map_id]['team_rank_1']\n",
    "    rank_2 = df_tmp.loc[map_id]['team_rank_2']\n",
    "\n",
    "    team_id_1 = df_tmp.loc[map_id]['team_id_1']\n",
    "    team_id_2 = df_tmp.loc[map_id]['team_id_2']\n",
    "\n",
    "    # # History between the two teams\n",
    "    df_history = df_two_year[ \n",
    "        (df_two_year['date'] < map_date) &\n",
    "        (((df_two_year['team_id_1']==team_id_1) & (df_two_year['team_id_2']==team_id_2)) | \n",
    "        ((df_two_year['team_id_2']==team_id_1) & (df_two_year['team_id_1']==team_id_2))) ]\n",
    "\n",
    "    # Here we take the history against teams that are in a similar rank to that of the opponent\n",
    "    # df_history_1 = df_two_year[ \n",
    "    #     (df_two_year['date'] < map_date) & (\n",
    "    #     ((df_two_year['team_id_1']==team_id_1) & (df_two_year['team_rank_2']>(rank_2-10)) & (df_two_year['team_rank_2']<(rank_2+10)) ) | \n",
    "    #     ((df_two_year['team_id_2']==team_id_1) & (df_two_year['team_rank_1']>(rank_2-10)) & (df_two_year['team_rank_1']<(rank_2+10)) ) \n",
    "    #     ) ]\n",
    "\n",
    "    win_vec_1 = (df_two_year[\n",
    "        (df_two_year['date'] < map_date) &\n",
    "        (df_two_year['team_id_1']==team_id_1) & \n",
    "        (df_two_year['team_rank_2']>(rank_2-10)) & \n",
    "        (df_two_year['team_rank_2']<(rank_2+10)) ]['winner'].values == 1)*1\n",
    "\n",
    "    np.append( win_vec_1, \n",
    "        (df_two_year[\n",
    "            (df_two_year['date'] < map_date) &\n",
    "            (df_two_year['team_id_2']==team_id_1) & \n",
    "            (df_two_year['team_rank_1']>(rank_2-10)) & \n",
    "            (df_two_year['team_rank_1']<(rank_2+10)) ]['winner'].values == 2)*1\n",
    "        )\n",
    "\n",
    "    if len(win_vec_1) > 0:\n",
    "        win_rate_1 = np.mean(win_vec_1) - 0.5\n",
    "    else:\n",
    "        win_rate_1 = 0.0\n",
    "    \n",
    "\n",
    "    win_vec_2 = (df_two_year[\n",
    "        (df_two_year['date'] < map_date) &\n",
    "        (df_two_year['team_id_1']==team_id_2) & \n",
    "        (df_two_year['team_rank_2']>(rank_1-10)) & \n",
    "        (df_two_year['team_rank_2']<(rank_1+10)) ]['winner'].values == 1)*1\n",
    "\n",
    "    np.append( win_vec_2, \n",
    "        (df_two_year[\n",
    "            (df_two_year['date'] < map_date) &\n",
    "            (df_two_year['team_id_2']==team_id_2) & \n",
    "            (df_two_year['team_rank_1']>(rank_1-10)) & \n",
    "            (df_two_year['team_rank_1']<(rank_1+10)) ]['winner'].values == 2)*1\n",
    "        )\n",
    "\n",
    "    if len(win_vec_2) > 0:\n",
    "        win_rate_2 = np.mean(win_vec_2) - 0.5\n",
    "    else:\n",
    "        win_rate_2 = 0.0\n",
    "    \n",
    "   \n",
    "    # We get the scores for the matches between the two teams\n",
    "    score_history_1 = df_history[ \n",
    "        (df_history['team_id_1']==team_id_1) ]['team_score_1'].values\n",
    "    \n",
    "    np.append( score_history_1, df_history[ \n",
    "        (df_history['team_id_2']==team_id_1) ]['team_score_2'].values )\n",
    "\n",
    "    score_history_2 = df_history[ \n",
    "        (df_history['team_id_2']==team_id_2) ]['team_score_2'].values\n",
    "    \n",
    "    np.append( score_history_2, df_history[ \n",
    "        (df_history['team_id_1']==team_id_2) ]['team_score_1'].values )\n",
    "\n",
    "    winner = df_tmp.loc[map_id]['winner']\n",
    "\n",
    "    if rank_1 < rank_2:\n",
    "        fav = 0\n",
    "        dict_train[map_id]['rank_dif'] = rank_2-rank_1\n",
    "\n",
    "        if len(score_history_1) > 0:\n",
    "            dict_train[map_id]['history_dif'] = np.mean( score_history_1-score_history_2 )\n",
    "        else:\n",
    "            dict_train[map_id]['history_dif'] = 0.0\n",
    "\n",
    "        dict_train[map_id]['shift_win_rate_0'] = win_rate_1\n",
    "        dict_train[map_id]['shift_win_rate_1'] = win_rate_2\n",
    "\n",
    "        if winner == 1:\n",
    "            dict_train[map_id]['fav_win'] = 1\n",
    "        else:\n",
    "            dict_train[map_id]['fav_win'] = 0\n",
    "\n",
    "    else:\n",
    "        fav = 1\n",
    "        dict_train[map_id]['rank_dif'] = rank_1-rank_2\n",
    "\n",
    "        if len(score_history_1) > 0:\n",
    "            dict_train[map_id]['history_dif'] = np.mean( score_history_2-score_history_1 )\n",
    "        else:\n",
    "            dict_train[map_id]['history_dif'] = 0.0\n",
    "\n",
    "        dict_train[map_id]['shift_win_rate_0'] = win_rate_2\n",
    "        dict_train[map_id]['shift_win_rate_1'] = win_rate_1\n",
    "\n",
    "        if winner == 2:\n",
    "            dict_train[map_id]['fav_win'] = 1\n",
    "        else:\n",
    "            dict_train[map_id]['fav_win'] = 0\n",
    "\n",
    "    count_team = -1\n",
    "    for team_id in dict_map[map_id]:\n",
    "\n",
    "        count_team = count_team + 1\n",
    "        weighted_rating    = []\n",
    "\n",
    "        weighted_kast      = []\n",
    "        weighted_kpr       = []\n",
    "        weighted_round_dif = []\n",
    "\n",
    "        prize = []\n",
    "\n",
    "        for player_id in dict_map[map_id][team_id]['players_id']:            \n",
    "\n",
    "            df_aux = dict_player[player_id]\n",
    "\n",
    "            date_vec = (map_date-df_aux['date']).astype('timedelta64[D]')\n",
    "\n",
    "            df_aux = df_aux[ (date_vec>1) & (date_vec<DAYS_WEIGHT) ]\n",
    "\n",
    "            try:\n",
    "                m = float(1.0)/ sum( date_vec[(date_vec>1) & (date_vec<DAYS_WEIGHT)]-DAYS_WEIGHT )\n",
    "            except:\n",
    "                m = 0.0\n",
    "\n",
    "            w_i = m * (date_vec[(date_vec>1) & (date_vec<DAYS_WEIGHT)] - DAYS_WEIGHT)\n",
    "            \n",
    "            # w_i = 1.0\n",
    "\n",
    "            weighted_rating.append( sum(df_aux['rating'] * w_i) )\n",
    "            prize.append( df_aux['prize'].sum() )\n",
    "\n",
    "            weighted_kast.append( sum(df_aux['KAST'] * w_i / 100.0) )\n",
    "            weighted_kpr.append( sum(df_aux['kills_per_round'] * w_i) )\n",
    "            \n",
    "            weighted_round_dif.append( sum( (df_aux['team_score']-df_aux['op_score']) * w_i) )\n",
    "\n",
    "        order = np.argsort( weighted_rating )\n",
    "\n",
    "        if count_team == fav:\n",
    "            count_p = -1\n",
    "            for ind in order:\n",
    "                count_p = count_p + 1\n",
    "                dict_train[map_id]['t_0_p_'+str(count_p)+'_rating'] = weighted_rating[ind]\n",
    "                dict_train[map_id]['t_0_p_'+str(count_p)+'_kast']   = weighted_kast[ind]\n",
    "                dict_train[map_id]['t_0_p_'+str(count_p)+'_kpr']    = weighted_kpr[ind]\n",
    "        else:\n",
    "            count_p = -1\n",
    "            for ind in order:\n",
    "                count_p = count_p + 1\n",
    "                dict_train[map_id]['t_1_p_'+str(count_p)+'_rating'] = weighted_rating[ind]\n",
    "                dict_train[map_id]['t_1_p_'+str(count_p)+'_kast']   = weighted_kast[ind]\n",
    "                dict_train[map_id]['t_1_p_'+str(count_p)+'_kpr']    = weighted_kpr[ind]\n",
    "\n",
    "        if count_team == fav:\n",
    "\n",
    "            dict_train[map_id]['t0_prize'] = np.average( prize )\n",
    "            dict_train[map_id]['t_0_avg_rating'] = np.average( weighted_rating )\n",
    "            \n",
    "        else:\n",
    "\n",
    "            dict_train[map_id]['t1_prize'] = np.average( prize )\n",
    "            dict_train[map_id]['t_1_avg_rating'] = np.average( weighted_rating )\n",
    "            \n",
    "    \n",
    "\n",
    "time_2 = datetime.now()\n",
    "\n",
    "print( time_2-time_1 )\n",
    "#date_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_train_1000 = dict_train\n",
    "# dict_train = dict_train_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we process the results we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.DataFrame.from_dict(dict_train,orient='index')\n",
    "\n",
    "for map_id in df_train.index:\n",
    "    df_train.at[map_id,'rating_dif'] = (dict_train[map_id]['t_0_avg_rating'] - dict_train[map_id]['t_1_avg_rating'])\n",
    "\n",
    "    if dict_train[map_id]['t0_prize'] > 0:\n",
    "        dict_train[map_id]['t0_prize_rating'] = np.log(dict_train[map_id]['t0_prize']) / 12.0\n",
    "    else:\n",
    "        dict_train[map_id]['t0_prize_rating'] = 0.0\n",
    "\n",
    "    if dict_train[map_id]['t1_prize'] > 0:\n",
    "        dict_train[map_id]['t1_prize_rating'] = np.log(dict_train[map_id]['t1_prize']) / 12.0\n",
    "    else:\n",
    "        dict_train[map_id]['t1_prize_rating'] = 0.0\n",
    "\n",
    "\n",
    "    df_train.at[map_id,'prize_rating_dif'] = dict_train[map_id]['t0_prize_rating'] - dict_train[map_id]['t1_prize_rating'] \n",
    "    #df_train.at[map_id,'kast_dif'] = 1.0*(dict_train[map_id]['t_0_avg_kast'] - dict_train[map_id]['t_1_avg_kast'])**1\n",
    "    #df_train.at[map_id,'rank_rating'] = df_train.at[map_id,'t_1_avg_rating'] / df_train.at[map_id,'rank_dif']\n",
    "\n",
    "\n",
    "df_train = df_train.fillna(0)\n",
    "\n",
    "print(df_train.keys())\n",
    "\n",
    "print(df_train['fav_win'].mean())\n",
    "\n",
    "df_train.tail(n=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the features that we want\n",
    "# ['rank_dif', 'history_dif', 'shift_win_rate_0', 'shift_win_rate_1',\n",
    "#        'fav_win', 't0_prize', 't_0_avg_rating', 't1_prize', 't_1_avg_rating',\n",
    "#        't0_prize_rating', 't1_prize_rating', 'rating_dif', 'prize_rating_dif']\n",
    "\n",
    "# Drop all except prize_Rating_dif and rating_dif\n",
    "# df_tmp = df_train.drop( ['rank_dif', 'history_dif', 'shift_win_rate_0', 'shift_win_rate_1',\n",
    "#        't_0_p_0_rating', 't_0_p_0_kast', 't_0_p_0_kpr',\n",
    "#        't_0_p_1_rating', 't_0_p_1_kast', 't_0_p_1_kpr', 't_0_p_2_rating',\n",
    "#        't_0_p_2_kast', 't_0_p_2_kpr', 't_0_p_3_rating', 't_0_p_3_kast',\n",
    "#        't_0_p_3_kpr', 't_0_p_4_rating', 't_0_p_4_kast', 't_0_p_4_kpr',\n",
    "#        't0_prize', 't_0_avg_rating', 't_1_p_0_rating', 't_1_p_0_kast',\n",
    "#        't_1_p_0_kpr', 't_1_p_1_rating', 't_1_p_1_kast', 't_1_p_1_kpr',\n",
    "#        't_1_p_2_rating', 't_1_p_2_kast', 't_1_p_2_kpr', 't_1_p_3_rating',\n",
    "#        't_1_p_3_kast', 't_1_p_3_kpr', 't_1_p_4_rating', 't_1_p_4_kast',\n",
    "#        't_1_p_4_kpr', 't1_prize', 't_1_avg_rating', 't0_prize_rating',\n",
    "#        't1_prize_rating'], axis=1 )\n",
    "\n",
    "# # Drop all except prize_Rating_dif and rating_dif and rank_dif\n",
    "# df_tmp = df_train.drop( ['history_dif', 'shift_win_rate_0', 'shift_win_rate_1',\n",
    "#        't_0_p_0_rating', 't_0_p_0_kast', 't_0_p_0_kpr',\n",
    "#        't_0_p_1_rating', 't_0_p_1_kast', 't_0_p_1_kpr', 't_0_p_2_rating',\n",
    "#        't_0_p_2_kast', 't_0_p_2_kpr', 't_0_p_3_rating', 't_0_p_3_kast',\n",
    "#        't_0_p_3_kpr', 't_0_p_4_rating', 't_0_p_4_kast', 't_0_p_4_kpr',\n",
    "#        't0_prize', 't_0_avg_rating', 't_1_p_0_rating', 't_1_p_0_kast',\n",
    "#        't_1_p_0_kpr', 't_1_p_1_rating', 't_1_p_1_kast', 't_1_p_1_kpr',\n",
    "#        't_1_p_2_rating', 't_1_p_2_kast', 't_1_p_2_kpr', 't_1_p_3_rating',\n",
    "#        't_1_p_3_kast', 't_1_p_3_kpr', 't_1_p_4_rating', 't_1_p_4_kast',\n",
    "#        't_1_p_4_kpr', 't1_prize', 't_1_avg_rating', 't0_prize_rating',\n",
    "#        't1_prize_rating'], axis=1 )\n",
    "\n",
    "\n",
    "df_tmp = df_train.drop(\n",
    "    ['rank_dif', 'history_dif', \n",
    "       't_0_p_0_kast', 't_0_p_0_kpr',\n",
    "       't_0_p_1_kast', 't_0_p_1_kpr', \n",
    "       't_0_p_2_kast', 't_0_p_2_kpr', 't_0_p_3_kast',\n",
    "       't_0_p_3_kpr', 't_0_p_4_kast', 't_0_p_4_kpr',\n",
    "       't0_prize',  't_1_p_0_kast',\n",
    "       't_1_p_0_kpr',  't_1_p_1_kast', 't_1_p_1_kpr',\n",
    "        't_1_p_2_kast', 't_1_p_2_kpr', \n",
    "       't_1_p_3_kast', 't_1_p_3_kpr', 't_1_p_4_kast',\n",
    "       't_1_p_4_kpr', 't1_prize', 't_1_avg_rating', 't_1_avg_rating', 't0_prize_rating',\n",
    "       't1_prize_rating' ],axis=1)\n",
    "\n",
    "#df_tmp = df_train.drop([ 't0_prize', 't1_prize'],axis=1)\n",
    "\n",
    "# Drop nothing\n",
    "#df_tmp = df_train.drop(['rank_dif','t0_prize', 't1_prize','rating_dif','prize_rating_dif','t_1_avg_rating'],axis=1)\n",
    "\n",
    "# Divide data set into training and testing sets      \n",
    "X_train, X_test, y_train, y_test = train_test_split(df_tmp.drop(['fav_win'],axis=1),df_tmp['fav_win'], test_size=0.25)\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_train.drop(['fav_win','rank_dif'],axis=1),df_train['fav_win'], test_size=0.25)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_orig,y_orig, test_size=0.25)\n",
    "\n",
    "# Make an instance of the model, and increase the maximum number of iterations to avoid convergence problems\n",
    "logmodel = LogisticRegression(max_iter=800)\n",
    "\n",
    "# Training for the model\n",
    "logmodel.fit(X_train,y_train)\n",
    "\n",
    "# Now we use the testing data set to make predictions and evaluate the model's performance\n",
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "print('>'+classification_report(y_test,predictions,target_names=['Underdog wins','Favorite wins']))\n",
    "\n",
    "#predictions[:] = 1\n",
    "\n",
    "#c_mat = confusion_matrix(y_test.values, predictions)\n",
    "c_mat = confusion_matrix(y_test, predictions)\n",
    "print( c_mat )\n",
    "\n",
    "accuracy = (c_mat[0][0] + c_mat[1][1]) / np.sum(c_mat)\n",
    "print(accuracy)\n",
    "\n",
    "y_scores = logmodel.predict_proba(X_test)[:,1]\n",
    "\n",
    "roc_auc_score(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the features that we want\n",
    "# ['rank_dif', 'history_dif', 'shift_win_rate_0', 'shift_win_rate_1',\n",
    "#        'fav_win', 't0_prize', 't_0_avg_rating', 't1_prize', 't_1_avg_rating',\n",
    "#        't0_prize_rating', 't1_prize_rating', 'rating_dif', 'prize_rating_dif']\n",
    "\n",
    "# Drop all except prize_Rating_dif and rating_dif\n",
    "# df_tmp = df_train.drop( ['rank_dif', 'history_dif', 'shift_win_rate_0', 'shift_win_rate_1',\n",
    "#        't_0_p_0_rating', 't_0_p_0_kast', 't_0_p_0_kpr',\n",
    "#        't_0_p_1_rating', 't_0_p_1_kast', 't_0_p_1_kpr', 't_0_p_2_rating',\n",
    "#        't_0_p_2_kast', 't_0_p_2_kpr', 't_0_p_3_rating', 't_0_p_3_kast',\n",
    "#        't_0_p_3_kpr', 't_0_p_4_rating', 't_0_p_4_kast', 't_0_p_4_kpr',\n",
    "#        't0_prize', 't_0_avg_rating', 't_1_p_0_rating', 't_1_p_0_kast',\n",
    "#        't_1_p_0_kpr', 't_1_p_1_rating', 't_1_p_1_kast', 't_1_p_1_kpr',\n",
    "#        't_1_p_2_rating', 't_1_p_2_kast', 't_1_p_2_kpr', 't_1_p_3_rating',\n",
    "#        't_1_p_3_kast', 't_1_p_3_kpr', 't_1_p_4_rating', 't_1_p_4_kast',\n",
    "#        't_1_p_4_kpr', 't1_prize', 't_1_avg_rating', 't0_prize_rating',\n",
    "#        't1_prize_rating'], axis=1 )\n",
    "\n",
    "# # Drop all except prize_Rating_dif and rating_dif and rank_dif\n",
    "# df_tmp = df_train.drop( ['history_dif', 'shift_win_rate_0', 'shift_win_rate_1',\n",
    "#        't_0_p_0_rating', 't_0_p_0_kast', 't_0_p_0_kpr',\n",
    "#        't_0_p_1_rating', 't_0_p_1_kast', 't_0_p_1_kpr', 't_0_p_2_rating',\n",
    "#        't_0_p_2_kast', 't_0_p_2_kpr', 't_0_p_3_rating', 't_0_p_3_kast',\n",
    "#        't_0_p_3_kpr', 't_0_p_4_rating', 't_0_p_4_kast', 't_0_p_4_kpr',\n",
    "#        't0_prize', 't_0_avg_rating', 't_1_p_0_rating', 't_1_p_0_kast',\n",
    "#        't_1_p_0_kpr', 't_1_p_1_rating', 't_1_p_1_kast', 't_1_p_1_kpr',\n",
    "#        't_1_p_2_rating', 't_1_p_2_kast', 't_1_p_2_kpr', 't_1_p_3_rating',\n",
    "#        't_1_p_3_kast', 't_1_p_3_kpr', 't_1_p_4_rating', 't_1_p_4_kast',\n",
    "#        't_1_p_4_kpr', 't1_prize', 't_1_avg_rating', 't0_prize_rating',\n",
    "#        't1_prize_rating'], axis=1 )\n",
    "\n",
    "\n",
    "df_tmp = df_train.drop(\n",
    "    ['rank_dif', 'history_dif', \n",
    "       't_0_p_0_kast', 't_0_p_0_kpr',\n",
    "       't_0_p_1_kast', 't_0_p_1_kpr', \n",
    "       't_0_p_2_kast', 't_0_p_2_kpr', 't_0_p_3_kast',\n",
    "       't_0_p_3_kpr', 't_0_p_4_kast', 't_0_p_4_kpr',\n",
    "       't0_prize',  't_1_p_0_kast',\n",
    "       't_1_p_0_kpr',  't_1_p_1_kast', 't_1_p_1_kpr',\n",
    "        't_1_p_2_kast', 't_1_p_2_kpr', \n",
    "       't_1_p_3_kast', 't_1_p_3_kpr', 't_1_p_4_kast',\n",
    "       't_1_p_4_kpr', 't1_prize', 't_1_avg_rating', 't_1_avg_rating', 't0_prize_rating',\n",
    "       't1_prize_rating' ],axis=1)\n",
    "\n",
    "#df_tmp = df_train.drop([ 't0_prize', 't1_prize'],axis=1)\n",
    "\n",
    "# Drop nothing\n",
    "#df_tmp = df_train.drop(['rank_dif','t0_prize', 't1_prize','rating_dif','prize_rating_dif','t_1_avg_rating'],axis=1)\n",
    "\n",
    "# Divide data set into training and testing sets      \n",
    "X_train, X_test, y_train, y_test = train_test_split(df_tmp.drop(['fav_win'],axis=1),df_tmp['fav_win'], test_size=0.25)\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_train.drop(['fav_win','rank_dif'],axis=1),df_train['fav_win'], test_size=0.25)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_orig,y_orig, test_size=0.25)\n",
    "\n",
    "# Make an instance of the model, and increase the maximum number of iterations to avoid convergence problems\n",
    "logmodel = LogisticRegression(max_iter=800)\n",
    "\n",
    "# Training for the model\n",
    "logmodel.fit(X_train,y_train)\n",
    "\n",
    "# Now we use the testing data set to make predictions and evaluate the model's performance\n",
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "print('--------------------------------------------------------------')\n",
    "print('Logistic')\n",
    "print('')\n",
    "print('>'+classification_report(y_test,predictions,target_names=['Underdog wins','Favorite wins']))\n",
    "\n",
    "#predictions[:] = 1\n",
    "\n",
    "#c_mat = confusion_matrix(y_test.values, predictions)\n",
    "c_mat = confusion_matrix(y_test, predictions)\n",
    "print( c_mat )\n",
    "\n",
    "# accuracy = (c_mat[0][0] + c_mat[1][1]) / np.sum(c_mat)\n",
    "# print(accuracy)\n",
    "\n",
    "# y_scores = logmodel.predict_proba(X_test)[:,1]\n",
    "\n",
    "# roc_auc_score(y_test, y_scores)\n",
    "\n",
    "\n",
    "# Divide data set into training and testing sets      \n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_tmp.drop(['fav_win'],axis=1),df_tmp['fav_win'], test_size=0.25)\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "# accuracy = accuracy_score(y_test, predictions)\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "# c_mat = confusion_matrix(y_test, predictions)\n",
    "# print( c_mat )\n",
    "\n",
    "print('--------------------------------------------------------------')\n",
    "print('XGBoost')\n",
    "print('')\n",
    "print('>'+classification_report(y_test,predictions,target_names=['Underdog wins','Favorite wins']))\n",
    "\n",
    "c_mat = confusion_matrix(y_test, predictions)\n",
    "print( c_mat )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Divide data set into training and testing sets      \n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_tmp.drop(['fav_win'],axis=1),df_tmp['fav_win'], test_size=0.25)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20,max_depth=10, random_state=1,bootstrap=True)\n",
    "clf.fit(X_train, y_train)\n",
    "#RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "\n",
    "# Now we use the testing data set to make predictions and evaluate the model's performance\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print('--------------------------------------------------------------')\n",
    "print('Random Forest')\n",
    "print('')\n",
    "print('>'+classification_report(y_test,predictions,target_names=['Underdog wins','Favorite wins']))\n",
    "\n",
    "c_mat = confusion_matrix(y_test, predictions)\n",
    "print( c_mat )\n",
    "\n",
    "\n",
    "# SVM\n",
    "clf_svm = svm.SVC(probability=True)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "#RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "\n",
    "# Now we use the testing data set to make predictions and evaluate the model's performance\n",
    "predictions = clf_svm.predict(X_test)\n",
    "\n",
    "print('--------------------------------------------------------------')\n",
    "print('Support Vector Machines')\n",
    "print('')\n",
    "print('>'+classification_report(y_test,predictions,target_names=['Underdog wins','Favorite wins']))\n",
    "\n",
    "c_mat = confusion_matrix(y_test, predictions)\n",
    "print( c_mat )\n",
    "\n",
    "\n",
    "\n",
    "# AUC ROC\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=3)\n",
    "\n",
    "plt.figure(figsize=(10,8));\n",
    "ax = plt.axes()\n",
    "plot_roc_curve(logmodel, X_test, y_test, ax=ax, lw=6)  # doctest: +SKIP\n",
    "\n",
    "plot_roc_curve(model, X_test, y_test, ax=ax, lw=6)  # doctest: +SKIP\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test, ax=ax, lw=6)  # doctest: +SKIP\n",
    "\n",
    "plot_roc_curve(clf_svm, X_test, y_test, ax=ax, lw=6)  # doctest: +SKIP\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=6, color='r',\n",
    "        label='Chance', alpha=.8 )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC ROC\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=6)\n",
    "\n",
    "mpl.rcParams.update({'text.color' : \"white\",\n",
    "                     'axes.labelcolor' : \"white\",\n",
    "                     'xtick.color' : \"white\",\n",
    "                     'ytick.color' : \"white\"})\n",
    "\n",
    "# mpl.rcParams.update({'text.color' : \"black\",\n",
    "#                      'axes.labelcolor' : \"black\",\n",
    "#                      'xtick.color' : \"black\",\n",
    "#                      'ytick.color' : \"black\"})\n",
    "\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "ax = plt.axes()\n",
    "plot_roc_curve(logmodel, X_test, y_test, ax=ax, lw=12)  # doctest: +SKIP\n",
    "\n",
    "plot_roc_curve(model, X_test, y_test, ax=ax, lw=12)  # doctest: +SKIP\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test, ax=ax, lw=12)  # doctest: +SKIP\n",
    "\n",
    "plot_roc_curve(clf_svm, X_test, y_test, ax=ax, lw=12)  # doctest: +SKIP\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=12, color='r',\n",
    "        label='Chance', alpha=.8 )\n",
    "\n",
    "plt.savefig('ROC_AUX.svg', transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.drop(['fav_win'],axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel.coef_[0,:]\n",
    "#X_train.shape[1]\n",
    "#importances\n",
    "#indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "\n",
    "# importances = logmodel.coef_[0,:]\n",
    "# std = importances * 0.0\n",
    "\n",
    "#std = 0.0*logmodel.coef_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'model_jan30.sav'\n",
    "# pickle.dump( logmodel, open(filename,'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_tmp.values\n",
    "X = dataset[:,0:15].astype(float)\n",
    "Y = dataset[:,15]\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order columns\n",
    "time_1 = datetime.now()\n",
    "\n",
    "df_tmp.keys()\n",
    "df_tmp = df_tmp[['shift_win_rate_0', 'shift_win_rate_1', 't_0_p_0_rating',\n",
    "       't_0_p_1_rating', 't_0_p_2_rating', 't_0_p_3_rating', 't_0_p_4_rating',\n",
    "       't_0_avg_rating', 't_1_p_0_rating', 't_1_p_1_rating', 't_1_p_2_rating',\n",
    "       't_1_p_3_rating', 't_1_p_4_rating', 'rating_dif', 'prize_rating_dif','fav_win']]\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "n_input = 15\n",
    "\n",
    "n_test = 10466\n",
    "\n",
    "n_splits = 10  # Default 10\n",
    "n_epochs = 100 # Default 100\n",
    "n_batch  = 5   # Default 5\n",
    "\n",
    "dataset = df_tmp.values\n",
    "X = dataset[0:n_test,0:15].astype(float)\n",
    "Y = dataset[0:n_test,15]\n",
    "\n",
    "encoded_Y = Y\n",
    "\n",
    "# encode class values as integers\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(Y)\n",
    "# encoded_Y = encoder.transform(Y)\n",
    "# with tf.device('/device:GPU:0'):\n",
    "\n",
    "# baseline model\n",
    "def create_baseline():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(n_input, input_dim=n_input, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# evaluate model with standardized dataset\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "time_2 = datetime.now()\n",
    "\n",
    "print( time_2-time_1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 6} ) \n",
    "# sess = tf.Session(config=config) \n",
    "# keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "import keras\n",
    "\n",
    "config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "sess = tf.compat.v1.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}