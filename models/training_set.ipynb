{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bittfconda04cc3002c149454bb73c3f879bac7c68",
   "display_name": "Python 3.7.6 64-bit ('tf': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is exclusively to create the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modues to be used\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original data\n",
    "\n",
    "# -> Player dictionary with the dataFrames\n",
    "f = open('../data/dict_player.pickle', 'rb')\n",
    "dict_player = pickle.load(f)\n",
    "\n",
    "# -> Map dataFrames\n",
    "f = open('../data/df_map.pickle','rb')\n",
    "df_map = pickle.load(f)\n",
    "\n",
    "# -> Map dictionary\n",
    "f = open('../data/dict_map.pickle','rb')\n",
    "dict_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "420"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_map.loc[ df_map['team_rank_1']==0, 'team_rank_1'] = 420\n",
    "\n",
    "df_map.loc[ df_map['team_rank_2']==0, 'team_rank_2'] = 420\n",
    "\n",
    "df_map['team_rank_1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>map_id</th>\n      <th>map</th>\n      <th>date</th>\n      <th>event_id</th>\n      <th>event_name</th>\n      <th>team_id_1</th>\n      <th>team_name_1</th>\n      <th>team_score_1</th>\n      <th>team_kills_1</th>\n      <th>team_deaths_1</th>\n      <th>team_assists_1</th>\n      <th>team_rank_1</th>\n      <th>team_id_2</th>\n      <th>team_name_2</th>\n      <th>team_score_2</th>\n      <th>team_kills_2</th>\n      <th>team_deaths_2</th>\n      <th>team_assists_2</th>\n      <th>team_rank_2</th>\n      <th>winner</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97594</th>\n      <td>97594</td>\n      <td>inf</td>\n      <td>2020-01-10 01:00:00</td>\n      <td>5164</td>\n      <td>Aorus League 2019 #4 Northern Cone</td>\n      <td>10330</td>\n      <td>Supremacy</td>\n      <td>16</td>\n      <td>99.0</td>\n      <td>93.0</td>\n      <td>16.0</td>\n      <td>420</td>\n      <td>10578</td>\n      <td>Infamous</td>\n      <td>12</td>\n      <td>93.0</td>\n      <td>100.0</td>\n      <td>17.0</td>\n      <td>420</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97592</th>\n      <td>97592</td>\n      <td>nuke</td>\n      <td>2020-01-10 01:00:00</td>\n      <td>5164</td>\n      <td>Aorus League 2019 #4 Northern Cone</td>\n      <td>10330</td>\n      <td>Supremacy</td>\n      <td>16</td>\n      <td>98.0</td>\n      <td>69.0</td>\n      <td>17.0</td>\n      <td>420</td>\n      <td>10578</td>\n      <td>Infamous</td>\n      <td>8</td>\n      <td>69.0</td>\n      <td>98.0</td>\n      <td>13.0</td>\n      <td>420</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97595</th>\n      <td>97595</td>\n      <td>inf</td>\n      <td>2020-01-10 00:35:00</td>\n      <td>5104</td>\n      <td>IEM Katowice 2020 North America Closed Qualifier</td>\n      <td>5752</td>\n      <td>Cloud9</td>\n      <td>16</td>\n      <td>100.0</td>\n      <td>91.0</td>\n      <td>14.0</td>\n      <td>18</td>\n      <td>9215</td>\n      <td>MIBR</td>\n      <td>12</td>\n      <td>90.0</td>\n      <td>100.0</td>\n      <td>16.0</td>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97593</th>\n      <td>97593</td>\n      <td>ovp</td>\n      <td>2020-01-10 00:35:00</td>\n      <td>5104</td>\n      <td>IEM Katowice 2020 North America Closed Qualifier</td>\n      <td>5752</td>\n      <td>Cloud9</td>\n      <td>16</td>\n      <td>77.0</td>\n      <td>46.0</td>\n      <td>11.0</td>\n      <td>18</td>\n      <td>9215</td>\n      <td>MIBR</td>\n      <td>4</td>\n      <td>46.0</td>\n      <td>77.0</td>\n      <td>10.0</td>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97591</th>\n      <td>97591</td>\n      <td>trn</td>\n      <td>2020-01-10 00:35:00</td>\n      <td>5104</td>\n      <td>IEM Katowice 2020 North America Closed Qualifier</td>\n      <td>5752</td>\n      <td>Cloud9</td>\n      <td>13</td>\n      <td>90.0</td>\n      <td>105.0</td>\n      <td>12.0</td>\n      <td>18</td>\n      <td>9215</td>\n      <td>MIBR</td>\n      <td>16</td>\n      <td>105.0</td>\n      <td>90.0</td>\n      <td>20.0</td>\n      <td>14</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       map_id   map                date  event_id  \\\n97594   97594   inf 2020-01-10 01:00:00      5164   \n97592   97592  nuke 2020-01-10 01:00:00      5164   \n97595   97595   inf 2020-01-10 00:35:00      5104   \n97593   97593   ovp 2020-01-10 00:35:00      5104   \n97591   97591   trn 2020-01-10 00:35:00      5104   \n\n                                             event_name  team_id_1  \\\n97594                Aorus League 2019 #4 Northern Cone      10330   \n97592                Aorus League 2019 #4 Northern Cone      10330   \n97595  IEM Katowice 2020 North America Closed Qualifier       5752   \n97593  IEM Katowice 2020 North America Closed Qualifier       5752   \n97591  IEM Katowice 2020 North America Closed Qualifier       5752   \n\n      team_name_1  team_score_1  team_kills_1  team_deaths_1  team_assists_1  \\\n97594   Supremacy            16          99.0           93.0            16.0   \n97592   Supremacy            16          98.0           69.0            17.0   \n97595      Cloud9            16         100.0           91.0            14.0   \n97593      Cloud9            16          77.0           46.0            11.0   \n97591      Cloud9            13          90.0          105.0            12.0   \n\n       team_rank_1  team_id_2 team_name_2  team_score_2  team_kills_2  \\\n97594          420      10578    Infamous            12          93.0   \n97592          420      10578    Infamous             8          69.0   \n97595           18       9215        MIBR            12          90.0   \n97593           18       9215        MIBR             4          46.0   \n97591           18       9215        MIBR            16         105.0   \n\n       team_deaths_2  team_assists_2  team_rank_2  winner  \n97594          100.0            17.0          420       1  \n97592           98.0            13.0          420       1  \n97595          100.0            16.0           14       1  \n97593           77.0            10.0           14       1  \n97591           90.0            20.0           14       2  "
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map.sort_values(['date'],ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "29013"
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(( df_map[ (df_map['date']>datetime(2016,6,6)) & (df_map['team_rank_1']<150) &  (df_map['team_rank_2']<150) ] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Converting data to DataFrame\n0:58:55.156098\n"
    }
   ],
   "source": [
    "# All unranked teams make them 420\n",
    "df_map.loc[ df_map['team_rank_1']==0, 'team_rank_1'] = 420\n",
    "df_map.loc[ df_map['team_rank_2']==0, 'team_rank_2'] = 420\n",
    "\n",
    "df_map['team_rank_1'].max()\n",
    "\n",
    "# Loop throughout all of the players and put it in the right map\n",
    "DAYS_WEIGHT = 90\n",
    "\n",
    "MAX_RANK = 150\n",
    "time_1 = datetime.now()\n",
    "\n",
    "\n",
    "# df_map_ranked = ( df_map[ (df_map['date']>datetime(2016,6,6)) & (df_map['team_rank_1']>0) &  (df_map['team_rank_2']>0) ] )\n",
    "df_map_ranked = ( df_map[ (df_map['date']>datetime(2016,6,6)) & (df_map['team_rank_1']<MAX_RANK) &  (df_map['team_rank_2']<MAX_RANK) ] )\n",
    "\n",
    "df_map_ranked = df_map_ranked.sort_values(['date'],ascending=False)\n",
    "\n",
    "metric_vec = ['KAST', 'ADR', 'first_kills_diff', 'rating',\n",
    "       'kills_per_round', 'deaths_per_round', 'impact', \n",
    "       'team_score',\n",
    "       'op_score', 'win', 'team_rank', 'prize']\n",
    "\n",
    "map_training_dict = {}\n",
    "\n",
    "\n",
    "# Function to append values of all the players\n",
    "def append_val(team_01,stat_vec,stat_name,dict_inout):\n",
    "\n",
    "    n = len(stat_vec)\n",
    "    order = np.argsort( stat_vec )[::-1][:n]\n",
    "\n",
    "    count_p = -1\n",
    "    for ind in order:\n",
    "        count_p = count_p + 1\n",
    "        dict_inout['t_'+team_01+'_p_'+str(count_p)+'_'+stat_name] = stat_vec[ind]\n",
    "    \n",
    "    return dict_inout\n",
    "\n",
    "for map_id in df_map_ranked['map_id'][:]:\n",
    "    \n",
    "    map_training_dict[map_id] = {}\n",
    "\n",
    "    # Keys for the dictionary\n",
    "    map_training_dict[map_id]['t1_win']    = 0\n",
    "    map_training_dict[map_id]['fav_win']   = 0\n",
    "    map_training_dict[map_id]['fav_ind']   = 0\n",
    "    map_training_dict[map_id]['map']       = df_map_ranked.at[map_id,'map']\n",
    "    map_training_dict[map_id]['score_dif'] = 0\n",
    "    map_training_dict[map_id]['t1_rank']   = 0\n",
    "    map_training_dict[map_id]['t2_rank']   = 0\n",
    "\n",
    "    # Get who started CT\n",
    "    # for t in range(0,2):\n",
    "    #     for p in range(0,5):\n",
    "    #         for metric in metric_vec:\n",
    "    #             map_training_dict[map_id][\n",
    "    #                 't_'+str(t)+'_p_'+str(p)+'_'+metric] = {}\n",
    "\n",
    "    with open('/home/emmanuel/Desktop/csgo-csv/json_maps/hltv_map_'+str(map_id)+'.json') as f:\n",
    "\n",
    "        data = json.load(f)\n",
    "        id_ct_start = data['roundHistory'][0]['ctTeam']\n",
    "\n",
    "    team_1  = df_map.at[map_id,'team_id_1']\n",
    "    rank_1  = df_map.at[map_id,'team_rank_1']\n",
    "    score_1 = df_map.at[map_id,'team_score_1']\n",
    "\n",
    "    team_2  = df_map.at[map_id,'team_id_2']\n",
    "    rank_2  = df_map.at[map_id,'team_rank_2']\n",
    "    score_2 = df_map.at[map_id,'team_score_2']\n",
    "\n",
    "    if team_1 != id_ct_start:\n",
    "        # We swap them\n",
    "        team_aux  = team_1\n",
    "        rank_aux  = rank_1\n",
    "        score_aux = score_1\n",
    "\n",
    "        team_1  = team_2\n",
    "        rank_1  = rank_2\n",
    "        score_1 = score_2\n",
    "\n",
    "        team_2  = team_aux\n",
    "        rank_2  = rank_aux\n",
    "        score_2 = score_aux\n",
    "\n",
    "    map_training_dict[map_id]['score_dif'] = score_1-score_2\n",
    "    map_training_dict[map_id]['t1_rank']   = rank_1\n",
    "    map_training_dict[map_id]['t2_rank']   = rank_2\n",
    "    \n",
    "\n",
    "    if score_1 > score_2:\n",
    "        map_training_dict[map_id]['t1_win'] = 1\n",
    "    else:\n",
    "        map_training_dict[map_id]['t1_win'] = 0\n",
    "\n",
    "\n",
    "    if map_training_dict[map_id]['t1_rank'] < map_training_dict[map_id]['t2_rank']:\n",
    "        map_training_dict[map_id]['fav_ind'] = 1\n",
    "    else:\n",
    "        map_training_dict[map_id]['fav_ind'] = 2\n",
    "\n",
    "\n",
    "    if ( (map_training_dict[map_id]['score_dif'] > 0) & (map_training_dict[map_id]['t1_rank']<map_training_dict[map_id]['t2_rank']) )|( (map_training_dict[map_id]['score_dif'] < 0) & (map_training_dict[map_id]['t2_rank']<map_training_dict[map_id]['t1_rank']) ):\n",
    "        map_training_dict[map_id]['fav_win'] = 1\n",
    "\n",
    "\n",
    "    # Now we fill the player statistics -----------------------------------------#\n",
    "    map_date = df_map_ranked.loc[map_id]['date']\n",
    "\n",
    "    # Here we are taking the rankings to be non-zero\n",
    "    team_vec = [team_1, team_2]\n",
    "    \n",
    "    for ind in range(0,2):\n",
    "\n",
    "        team_id = team_vec[ind]\n",
    "\n",
    "        rating_vec    = []\n",
    "        prize_rtg_vec = []\n",
    "        hs_vec        = []\n",
    "        kills_per_rd_vec  = []\n",
    "        deaths_per_rd_vec = []\n",
    "        adr_vec = []\n",
    "        kast_vec = []\n",
    "        assists_per_rd_vec = []\n",
    "        flash_per_rd_vec   = []\n",
    "        first_kills_dif_vec = []\n",
    "        team_rank_vec = []\n",
    "        score_dif_vec = []\n",
    "        win_rate_vec = []\n",
    "        win_rate_map_vec = []\n",
    "\n",
    "        kd_per_round_vec  = []\n",
    "\n",
    "        scaled_win_vec = []\n",
    "        scaled_rating_vec = [] \n",
    "        scaled_score_dif_vec = []\n",
    "        scaled_kd_vec = []\n",
    "\n",
    "        momentum_vec = []\n",
    "        map_rating_vec = []\n",
    "\n",
    "        for player_id in dict_map[map_id][team_id]['players_id']:            \n",
    "            \n",
    "            # Get the data for this player\n",
    "            df_aux   = dict_player[player_id]\n",
    "            date_vec = (map_date-df_aux['date']).astype('timedelta64[D]')\n",
    "\n",
    "            # Note the prize rating we take a whole year back\n",
    "            prize = df_aux[ (date_vec>1) & (date_vec<365) ]['prize'].sum()\n",
    "            if prize > 0.0:\n",
    "                prize_rtg_vec.append( np.log( prize )/12.0 )\n",
    "            else:\n",
    "                prize_rtg_vec.append( 0.0 )\n",
    "\n",
    "\n",
    "            df_aux   = df_aux[ (date_vec>1) & (date_vec<DAYS_WEIGHT) ]\n",
    "\n",
    "            # What if we use only the historical data for this map in particular\n",
    "            # df_aux = df_aux[ (date_vec>1) & (date_vec<DAYS_WEIGHT) & (df_aux['map']==df_map_ranked.at[map_id,'map']) ]\n",
    "\n",
    "            # Append the average values of this player \n",
    "            rating_vec.append         ( df_aux['rating'].mean() )\n",
    "            hs_vec.append             ( (df_aux['hs_kills']/(df_aux['team_score']+df_aux['op_score'])).mean() )        \n",
    "            kills_per_rd_vec.append   ( df_aux['kills_per_round'].mean() )\n",
    "            deaths_per_rd_vec.append  ( df_aux['deaths_per_round'].mean() )\n",
    "            adr_vec.append            ( df_aux['ADR'].mean() )\n",
    "            kast_vec.append           ( df_aux['KAST'].mean() )\n",
    "            assists_per_rd_vec.append ( (df_aux['assists']/(df_aux['team_score']+df_aux['op_score'])).mean() )\n",
    "            flash_per_rd_vec.append   ( (df_aux['flash_assists']/(df_aux['team_score']+df_aux['op_score'])).mean() )\n",
    "            first_kills_dif_vec.append( (df_aux['first_kills_diff']/(df_aux['team_score']+df_aux['op_score'])).mean() )\n",
    "            team_rank_vec.append      ( df_aux['team_rank'].mean() )\n",
    "            score_dif_vec.append      ( (df_aux['team_score']-df_aux['op_score']).mean() )\n",
    "            win_rate_vec.append       ( df_aux['win'].mean() )\n",
    "\n",
    "            \n",
    "\n",
    "            kd_per_round_vec.append  ( (df_aux['kills_per_round']-df_aux['deaths_per_round']).mean() )\n",
    "\n",
    "            # Get the opponent rank\n",
    "            df_player_maps = df_map.loc[df_aux.index.values]\n",
    "            op_rank_vec    = (df_aux['team_id']!=df_player_maps['team_id_1'])*df_player_maps['team_rank_1'] + (df_aux['team_id']!=df_player_maps['team_id_2'])*df_player_maps['team_rank_2']\n",
    "\n",
    "            # Scaled Variables\n",
    "            \n",
    "            # Linear scaling\n",
    "            # alpha = 0.1\n",
    "            # op_rank_vec = alpha + (1.0-alpha)*(420.0-op_rank_vec)/420.0\n",
    "            \n",
    "            # scaled_win_vec.append       ( (df_aux['win']*op_rank_vec).mean() )\n",
    "            # scaled_rating_vec.append    ( (df_aux['rating']*op_rank_vec).mean() )\n",
    "            # scaled_score_dif_vec.append ( ((df_aux['team_score']-df_aux['op_score'])*op_rank_vec).mean() )\n",
    "\n",
    "            scaled_win_vec.append       ( (df_aux['win']/op_rank_vec).mean() )\n",
    "            scaled_rating_vec.append    ( (df_aux['rating']/op_rank_vec).mean() )\n",
    "            scaled_score_dif_vec.append ( ((df_aux['team_score']-df_aux['op_score'])/op_rank_vec).mean() )\n",
    "\n",
    "            scaled_kd_vec.append ( ((df_aux['kills_per_round']-df_aux['deaths_per_round'])/op_rank_vec).mean() )\n",
    "\n",
    "            momentum_vec.append ( df_aux['win'].head(n=15).mean() )\n",
    "\n",
    "            df_aux_map = df_aux[ df_aux['map']==df_map_ranked.at[map_id,'map'] ]\n",
    "            map_rating_vec.append( (df_aux_map['team_score']-df_aux_map['op_score']).mean() )\n",
    "            win_rate_map_vec.append ( (df_aux_map['win']/op_rank_vec).mean() )\n",
    "\n",
    "\n",
    "        t = str(ind)\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,prize_rtg_vec,'prize_rating',map_training_dict[map_id])\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,rating_vec,'rating',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,hs_vec,'hs_perc',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,kills_per_rd_vec,'kills_per_rd',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,deaths_per_rd_vec,'deaths_per_rd',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,adr_vec,'adr',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,kast_vec,'kast',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,assists_per_rd_vec,'assists_per_rd',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,flash_per_rd_vec,'flash_per_rd',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,first_kills_dif_vec,'first_kills_dif',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,team_rank_vec,'team_rank',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,score_dif_vec,'score_dif',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,win_rate_vec,'win_rate',map_training_dict[map_id])\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,win_rate_map_vec,'win_rate_map',map_training_dict[map_id])\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,kd_per_round_vec,'kd_per_round',map_training_dict[map_id])\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,scaled_win_vec,'scaled_win',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,scaled_rating_vec,'scaled_rating',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,scaled_score_dif_vec,'scaled_score_dif',map_training_dict[map_id])\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,scaled_kd_vec,'scaled_kd',map_training_dict[map_id])\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,momentum_vec,'momentum',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,map_rating_vec,'map_rating',map_training_dict[map_id])\n",
    "    \n",
    "print('Converting data to DataFrame')\n",
    "df_ct_start = pd.DataFrame.from_dict( map_training_dict,orient='index')\n",
    "time_2 = datetime.now()\n",
    "\n",
    "print( time_2-time_1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final_train_df.sav'\n",
    "pickle.dump( df_ct_start, open(filename,'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t1_win</th>\n      <th>fav_win</th>\n      <th>fav_ind</th>\n      <th>map</th>\n      <th>score_dif</th>\n      <th>t1_rank</th>\n      <th>t2_rank</th>\n      <th>t_0_p_0_prize_rating</th>\n      <th>t_0_p_1_prize_rating</th>\n      <th>t_0_p_2_prize_rating</th>\n      <th>...</th>\n      <th>t_1_p_0_scaled_score_dif</th>\n      <th>t_1_p_1_scaled_score_dif</th>\n      <th>t_1_p_2_scaled_score_dif</th>\n      <th>t_1_p_3_scaled_score_dif</th>\n      <th>t_1_p_4_scaled_score_dif</th>\n      <th>t_1_p_0_scaled_kd</th>\n      <th>t_1_p_1_scaled_kd</th>\n      <th>t_1_p_2_scaled_kd</th>\n      <th>t_1_p_3_scaled_kd</th>\n      <th>t_1_p_4_scaled_kd</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>92838</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>mrg</td>\n      <td>-6</td>\n      <td>19</td>\n      <td>8</td>\n      <td>0.854439</td>\n      <td>0.852957</td>\n      <td>0.834175</td>\n      <td>...</td>\n      <td>-0.104174</td>\n      <td>-0.104174</td>\n      <td>-0.104174</td>\n      <td>-0.104174</td>\n      <td>-0.104174</td>\n      <td>0.004472</td>\n      <td>0.001304</td>\n      <td>-0.002880</td>\n      <td>-0.007629</td>\n      <td>-0.010502</td>\n    </tr>\n    <tr>\n      <th>92845</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>nuke</td>\n      <td>-6</td>\n      <td>1</td>\n      <td>28</td>\n      <td>1.032929</td>\n      <td>1.032929</td>\n      <td>1.032929</td>\n      <td>...</td>\n      <td>0.026481</td>\n      <td>-0.162239</td>\n      <td>-0.162239</td>\n      <td>-0.162239</td>\n      <td>-0.162239</td>\n      <td>0.005567</td>\n      <td>0.001263</td>\n      <td>-0.005819</td>\n      <td>-0.005870</td>\n      <td>-0.016589</td>\n    </tr>\n    <tr>\n      <th>92846</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>nuke</td>\n      <td>3</td>\n      <td>8</td>\n      <td>19</td>\n      <td>0.930477</td>\n      <td>0.898823</td>\n      <td>0.898823</td>\n      <td>...</td>\n      <td>-0.573629</td>\n      <td>-0.586402</td>\n      <td>-0.608132</td>\n      <td>-0.608132</td>\n      <td>-0.608132</td>\n      <td>0.001080</td>\n      <td>-0.022336</td>\n      <td>-0.027359</td>\n      <td>-0.028091</td>\n      <td>-0.028422</td>\n    </tr>\n    <tr>\n      <th>92855</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>d2</td>\n      <td>10</td>\n      <td>25</td>\n      <td>35</td>\n      <td>0.810288</td>\n      <td>0.810288</td>\n      <td>0.810288</td>\n      <td>...</td>\n      <td>0.009982</td>\n      <td>0.009182</td>\n      <td>0.008159</td>\n      <td>0.008159</td>\n      <td>0.008159</td>\n      <td>0.002185</td>\n      <td>0.000116</td>\n      <td>-0.000172</td>\n      <td>-0.000673</td>\n      <td>-0.001232</td>\n    </tr>\n    <tr>\n      <th>92856</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>vertigo</td>\n      <td>-11</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0.962391</td>\n      <td>0.960731</td>\n      <td>0.960731</td>\n      <td>...</td>\n      <td>0.236011</td>\n      <td>0.236011</td>\n      <td>0.236011</td>\n      <td>0.236011</td>\n      <td>0.236011</td>\n      <td>0.024258</td>\n      <td>0.014221</td>\n      <td>0.004867</td>\n      <td>-0.001036</td>\n      <td>-0.017150</td>\n    </tr>\n    <tr>\n      <th>92860</th>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>ovp</td>\n      <td>2</td>\n      <td>23</td>\n      <td>13</td>\n      <td>0.910906</td>\n      <td>0.910906</td>\n      <td>0.910906</td>\n      <td>...</td>\n      <td>0.226649</td>\n      <td>0.226649</td>\n      <td>0.226649</td>\n      <td>0.226649</td>\n      <td>-0.174910</td>\n      <td>0.017286</td>\n      <td>0.015257</td>\n      <td>0.001157</td>\n      <td>0.000660</td>\n      <td>-0.003709</td>\n    </tr>\n    <tr>\n      <th>92868</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>d2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1.063768</td>\n      <td>1.063768</td>\n      <td>1.063768</td>\n      <td>...</td>\n      <td>-0.160592</td>\n      <td>-0.747932</td>\n      <td>-0.747932</td>\n      <td>-0.747932</td>\n      <td>-0.747932</td>\n      <td>0.017874</td>\n      <td>0.011226</td>\n      <td>-0.021122</td>\n      <td>-0.045754</td>\n      <td>-0.049187</td>\n    </tr>\n    <tr>\n      <th>92873</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>trn</td>\n      <td>-2</td>\n      <td>30</td>\n      <td>49</td>\n      <td>0.894170</td>\n      <td>0.813129</td>\n      <td>0.774684</td>\n      <td>...</td>\n      <td>0.027949</td>\n      <td>0.027949</td>\n      <td>0.026007</td>\n      <td>0.026007</td>\n      <td>-0.040171</td>\n      <td>0.002331</td>\n      <td>0.000786</td>\n      <td>0.000098</td>\n      <td>-0.002182</td>\n      <td>-0.003897</td>\n    </tr>\n    <tr>\n      <th>92875</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>inf</td>\n      <td>-11</td>\n      <td>13</td>\n      <td>23</td>\n      <td>0.925763</td>\n      <td>0.924979</td>\n      <td>0.924979</td>\n      <td>...</td>\n      <td>0.062518</td>\n      <td>0.062518</td>\n      <td>0.062518</td>\n      <td>-0.057540</td>\n      <td>-0.287111</td>\n      <td>0.009369</td>\n      <td>0.007487</td>\n      <td>-0.002798</td>\n      <td>-0.003668</td>\n      <td>-0.021964</td>\n    </tr>\n    <tr>\n      <th>92878</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>ovp</td>\n      <td>-8</td>\n      <td>17</td>\n      <td>10</td>\n      <td>0.921959</td>\n      <td>0.921959</td>\n      <td>0.921959</td>\n      <td>...</td>\n      <td>-0.376298</td>\n      <td>-0.376298</td>\n      <td>-0.376298</td>\n      <td>-0.376298</td>\n      <td>-1.467185</td>\n      <td>0.015798</td>\n      <td>0.013924</td>\n      <td>0.002607</td>\n      <td>-0.004257</td>\n      <td>-0.041581</td>\n    </tr>\n    <tr>\n      <th>92880</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>inf</td>\n      <td>-6</td>\n      <td>49</td>\n      <td>30</td>\n      <td>0.965904</td>\n      <td>0.964151</td>\n      <td>0.793351</td>\n      <td>...</td>\n      <td>0.008224</td>\n      <td>-0.012128</td>\n      <td>-0.044622</td>\n      <td>-0.044818</td>\n      <td>-0.044818</td>\n      <td>0.002289</td>\n      <td>0.000578</td>\n      <td>0.000152</td>\n      <td>-0.001813</td>\n      <td>-0.003450</td>\n    </tr>\n    <tr>\n      <th>92881</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>d2</td>\n      <td>-14</td>\n      <td>28</td>\n      <td>8</td>\n      <td>0.810495</td>\n      <td>0.810495</td>\n      <td>0.810495</td>\n      <td>...</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>0.003818</td>\n      <td>0.000995</td>\n      <td>-0.002446</td>\n      <td>-0.007852</td>\n      <td>-0.009963</td>\n    </tr>\n    <tr>\n      <th>92883</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>inf</td>\n      <td>3</td>\n      <td>10</td>\n      <td>17</td>\n      <td>0.974488</td>\n      <td>0.973790</td>\n      <td>0.973790</td>\n      <td>...</td>\n      <td>0.254121</td>\n      <td>0.005015</td>\n      <td>-0.073845</td>\n      <td>-0.073845</td>\n      <td>-0.073845</td>\n      <td>0.012696</td>\n      <td>0.004571</td>\n      <td>0.001422</td>\n      <td>-0.000607</td>\n      <td>-0.006693</td>\n    </tr>\n    <tr>\n      <th>92884</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>trn</td>\n      <td>-2</td>\n      <td>24</td>\n      <td>35</td>\n      <td>0.811009</td>\n      <td>0.788964</td>\n      <td>0.788344</td>\n      <td>...</td>\n      <td>0.001799</td>\n      <td>0.001107</td>\n      <td>0.000282</td>\n      <td>0.000282</td>\n      <td>0.000282</td>\n      <td>0.002005</td>\n      <td>-0.000053</td>\n      <td>-0.000413</td>\n      <td>-0.000917</td>\n      <td>-0.001395</td>\n    </tr>\n    <tr>\n      <th>92885</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>d2</td>\n      <td>-4</td>\n      <td>30</td>\n      <td>49</td>\n      <td>0.894170</td>\n      <td>0.813129</td>\n      <td>0.774684</td>\n      <td>...</td>\n      <td>0.027949</td>\n      <td>0.027949</td>\n      <td>0.026007</td>\n      <td>0.026007</td>\n      <td>-0.040171</td>\n      <td>0.002331</td>\n      <td>0.000786</td>\n      <td>0.000098</td>\n      <td>-0.002182</td>\n      <td>-0.003897</td>\n    </tr>\n    <tr>\n      <th>92887</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>inf</td>\n      <td>-5</td>\n      <td>8</td>\n      <td>28</td>\n      <td>0.930477</td>\n      <td>0.898823</td>\n      <td>0.898823</td>\n      <td>...</td>\n      <td>0.099574</td>\n      <td>-0.098644</td>\n      <td>-0.098644</td>\n      <td>-0.098644</td>\n      <td>-0.098644</td>\n      <td>0.009913</td>\n      <td>0.004549</td>\n      <td>-0.000354</td>\n      <td>-0.008737</td>\n      <td>-0.016101</td>\n    </tr>\n    <tr>\n      <th>92888</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>d2</td>\n      <td>-2</td>\n      <td>19</td>\n      <td>21</td>\n      <td>0.920716</td>\n      <td>0.841084</td>\n      <td>0.831741</td>\n      <td>...</td>\n      <td>0.051476</td>\n      <td>0.005999</td>\n      <td>-0.002113</td>\n      <td>-0.070051</td>\n      <td>-0.070051</td>\n      <td>0.009136</td>\n      <td>0.001202</td>\n      <td>0.000671</td>\n      <td>-0.000732</td>\n      <td>-0.012684</td>\n    </tr>\n    <tr>\n      <th>92889</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>mrg</td>\n      <td>-11</td>\n      <td>28</td>\n      <td>8</td>\n      <td>0.810495</td>\n      <td>0.810495</td>\n      <td>0.810495</td>\n      <td>...</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>0.003818</td>\n      <td>0.000995</td>\n      <td>-0.002446</td>\n      <td>-0.007852</td>\n      <td>-0.009963</td>\n    </tr>\n    <tr>\n      <th>92892</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>mrg</td>\n      <td>-2</td>\n      <td>19</td>\n      <td>21</td>\n      <td>0.920716</td>\n      <td>0.841084</td>\n      <td>0.831741</td>\n      <td>...</td>\n      <td>0.051476</td>\n      <td>0.005999</td>\n      <td>-0.002113</td>\n      <td>-0.070051</td>\n      <td>-0.070051</td>\n      <td>0.009136</td>\n      <td>0.001202</td>\n      <td>0.000671</td>\n      <td>-0.000732</td>\n      <td>-0.012684</td>\n    </tr>\n    <tr>\n      <th>92899</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>trn</td>\n      <td>-4</td>\n      <td>26</td>\n      <td>37</td>\n      <td>0.751052</td>\n      <td>0.751052</td>\n      <td>0.751052</td>\n      <td>...</td>\n      <td>0.012881</td>\n      <td>0.012142</td>\n      <td>0.012142</td>\n      <td>-0.056612</td>\n      <td>-0.511936</td>\n      <td>0.003121</td>\n      <td>0.000714</td>\n      <td>0.000556</td>\n      <td>-0.000020</td>\n      <td>-0.016166</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows Ã— 197 columns</p>\n</div>",
      "text/plain": "       t1_win  fav_win  fav_ind      map  score_dif  t1_rank  t2_rank  \\\n92838       0        1        2      mrg         -6       19        8   \n92845       0        0        1     nuke         -6        1       28   \n92846       1        1        1     nuke          3        8       19   \n92855       1        1        1       d2         10       25       35   \n92856       0        1        2  vertigo        -11        5        2   \n92860       1        0        2      ovp          2       23       13   \n92868       1        1        1       d2          2        2        5   \n92873       0        0        1      trn         -2       30       49   \n92875       0        0        1      inf        -11       13       23   \n92878       0        1        2      ovp         -8       17       10   \n92880       0        1        2      inf         -6       49       30   \n92881       0        1        2       d2        -14       28        8   \n92883       1        1        1      inf          3       10       17   \n92884       0        0        1      trn         -2       24       35   \n92885       0        0        1       d2         -4       30       49   \n92887       0        0        1      inf         -5        8       28   \n92888       0        0        1       d2         -2       19       21   \n92889       0        1        2      mrg        -11       28        8   \n92892       0        0        1      mrg         -2       19       21   \n92899       0        0        1      trn         -4       26       37   \n\n       t_0_p_0_prize_rating  t_0_p_1_prize_rating  t_0_p_2_prize_rating  ...  \\\n92838              0.854439              0.852957              0.834175  ...   \n92845              1.032929              1.032929              1.032929  ...   \n92846              0.930477              0.898823              0.898823  ...   \n92855              0.810288              0.810288              0.810288  ...   \n92856              0.962391              0.960731              0.960731  ...   \n92860              0.910906              0.910906              0.910906  ...   \n92868              1.063768              1.063768              1.063768  ...   \n92873              0.894170              0.813129              0.774684  ...   \n92875              0.925763              0.924979              0.924979  ...   \n92878              0.921959              0.921959              0.921959  ...   \n92880              0.965904              0.964151              0.793351  ...   \n92881              0.810495              0.810495              0.810495  ...   \n92883              0.974488              0.973790              0.973790  ...   \n92884              0.811009              0.788964              0.788344  ...   \n92885              0.894170              0.813129              0.774684  ...   \n92887              0.930477              0.898823              0.898823  ...   \n92888              0.920716              0.841084              0.831741  ...   \n92889              0.810495              0.810495              0.810495  ...   \n92892              0.920716              0.841084              0.831741  ...   \n92899              0.751052              0.751052              0.751052  ...   \n\n       t_1_p_0_scaled_score_dif  t_1_p_1_scaled_score_dif  \\\n92838                 -0.104174                 -0.104174   \n92845                  0.026481                 -0.162239   \n92846                 -0.573629                 -0.586402   \n92855                  0.009982                  0.009182   \n92856                  0.236011                  0.236011   \n92860                  0.226649                  0.226649   \n92868                 -0.160592                 -0.747932   \n92873                  0.027949                  0.027949   \n92875                  0.062518                  0.062518   \n92878                 -0.376298                 -0.376298   \n92880                  0.008224                 -0.012128   \n92881                 -0.110326                 -0.110326   \n92883                  0.254121                  0.005015   \n92884                  0.001799                  0.001107   \n92885                  0.027949                  0.027949   \n92887                  0.099574                 -0.098644   \n92888                  0.051476                  0.005999   \n92889                 -0.110326                 -0.110326   \n92892                  0.051476                  0.005999   \n92899                  0.012881                  0.012142   \n\n       t_1_p_2_scaled_score_dif  t_1_p_3_scaled_score_dif  \\\n92838                 -0.104174                 -0.104174   \n92845                 -0.162239                 -0.162239   \n92846                 -0.608132                 -0.608132   \n92855                  0.008159                  0.008159   \n92856                  0.236011                  0.236011   \n92860                  0.226649                  0.226649   \n92868                 -0.747932                 -0.747932   \n92873                  0.026007                  0.026007   \n92875                  0.062518                 -0.057540   \n92878                 -0.376298                 -0.376298   \n92880                 -0.044622                 -0.044818   \n92881                 -0.110326                 -0.110326   \n92883                 -0.073845                 -0.073845   \n92884                  0.000282                  0.000282   \n92885                  0.026007                  0.026007   \n92887                 -0.098644                 -0.098644   \n92888                 -0.002113                 -0.070051   \n92889                 -0.110326                 -0.110326   \n92892                 -0.002113                 -0.070051   \n92899                  0.012142                 -0.056612   \n\n       t_1_p_4_scaled_score_dif  t_1_p_0_scaled_kd  t_1_p_1_scaled_kd  \\\n92838                 -0.104174           0.004472           0.001304   \n92845                 -0.162239           0.005567           0.001263   \n92846                 -0.608132           0.001080          -0.022336   \n92855                  0.008159           0.002185           0.000116   \n92856                  0.236011           0.024258           0.014221   \n92860                 -0.174910           0.017286           0.015257   \n92868                 -0.747932           0.017874           0.011226   \n92873                 -0.040171           0.002331           0.000786   \n92875                 -0.287111           0.009369           0.007487   \n92878                 -1.467185           0.015798           0.013924   \n92880                 -0.044818           0.002289           0.000578   \n92881                 -0.110326           0.003818           0.000995   \n92883                 -0.073845           0.012696           0.004571   \n92884                  0.000282           0.002005          -0.000053   \n92885                 -0.040171           0.002331           0.000786   \n92887                 -0.098644           0.009913           0.004549   \n92888                 -0.070051           0.009136           0.001202   \n92889                 -0.110326           0.003818           0.000995   \n92892                 -0.070051           0.009136           0.001202   \n92899                 -0.511936           0.003121           0.000714   \n\n       t_1_p_2_scaled_kd  t_1_p_3_scaled_kd  t_1_p_4_scaled_kd  \n92838          -0.002880          -0.007629          -0.010502  \n92845          -0.005819          -0.005870          -0.016589  \n92846          -0.027359          -0.028091          -0.028422  \n92855          -0.000172          -0.000673          -0.001232  \n92856           0.004867          -0.001036          -0.017150  \n92860           0.001157           0.000660          -0.003709  \n92868          -0.021122          -0.045754          -0.049187  \n92873           0.000098          -0.002182          -0.003897  \n92875          -0.002798          -0.003668          -0.021964  \n92878           0.002607          -0.004257          -0.041581  \n92880           0.000152          -0.001813          -0.003450  \n92881          -0.002446          -0.007852          -0.009963  \n92883           0.001422          -0.000607          -0.006693  \n92884          -0.000413          -0.000917          -0.001395  \n92885           0.000098          -0.002182          -0.003897  \n92887          -0.000354          -0.008737          -0.016101  \n92888           0.000671          -0.000732          -0.012684  \n92889          -0.002446          -0.007852          -0.009963  \n92892           0.000671          -0.000732          -0.012684  \n92899           0.000556          -0.000020          -0.016166  \n\n[20 rows x 197 columns]"
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feat = pd.DataFrame.from_dict(map_training_dict,orient='index')\n",
    "\n",
    "df_all_feat = df_all_feat.fillna(0)\n",
    "\n",
    "# filename = 'df_all_feat_40000.sav'\n",
    "# pickle.dump( df_all_feat, open(filename,'wb') )\n",
    "\n",
    "df_all_feat.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.5008789163478441\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fav_win</th>\n      <th>t1_win</th>\n      <th>fav_ind</th>\n      <th>prize_rating_dif</th>\n      <th>rating_dif</th>\n      <th>hs_perc_dif</th>\n      <th>kills_per_rd_dif</th>\n      <th>deaths_per_rd_dif</th>\n      <th>adr_dif</th>\n      <th>kast_dif</th>\n      <th>...</th>\n      <th>scaled_score_dif_dif</th>\n      <th>win_rate_map_dif</th>\n      <th>kd_per_round_dif</th>\n      <th>scaled_kd_dif</th>\n      <th>momentum_dif</th>\n      <th>map_rating_dif</th>\n      <th>tier_1</th>\n      <th>tier_2</th>\n      <th>fav_ind_2</th>\n      <th>fav_ind_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97490</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.703403</td>\n      <td>-0.348008</td>\n      <td>-0.425264</td>\n      <td>-0.416823</td>\n      <td>0.214299</td>\n      <td>-0.376182</td>\n      <td>-0.254745</td>\n      <td>...</td>\n      <td>-1.124069</td>\n      <td>-0.126876</td>\n      <td>-0.541530</td>\n      <td>-0.874358</td>\n      <td>-1.353951</td>\n      <td>-1.084605</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97491</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>1.343076</td>\n      <td>2.005799</td>\n      <td>0.646436</td>\n      <td>1.535165</td>\n      <td>1.321096</td>\n      <td>1.654750</td>\n      <td>2.972975</td>\n      <td>...</td>\n      <td>0.053010</td>\n      <td>-0.002953</td>\n      <td>0.504626</td>\n      <td>0.025537</td>\n      <td>2.080160</td>\n      <td>-0.470765</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97492</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.255373</td>\n      <td>0.010410</td>\n      <td>-0.169967</td>\n      <td>0.017783</td>\n      <td>-0.082043</td>\n      <td>-0.083886</td>\n      <td>0.005859</td>\n      <td>...</td>\n      <td>-0.770420</td>\n      <td>0.006047</td>\n      <td>0.074568</td>\n      <td>-0.689170</td>\n      <td>-1.846296</td>\n      <td>-0.973705</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97493</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.703403</td>\n      <td>0.348008</td>\n      <td>0.425264</td>\n      <td>0.416823</td>\n      <td>-0.214299</td>\n      <td>0.376182</td>\n      <td>0.254745</td>\n      <td>...</td>\n      <td>1.124069</td>\n      <td>-0.379142</td>\n      <td>0.541530</td>\n      <td>0.874358</td>\n      <td>1.353951</td>\n      <td>-1.892290</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97494</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.255373</td>\n      <td>-0.010410</td>\n      <td>0.169967</td>\n      <td>-0.017783</td>\n      <td>0.082043</td>\n      <td>0.083886</td>\n      <td>-0.005859</td>\n      <td>...</td>\n      <td>0.770420</td>\n      <td>0.101901</td>\n      <td>-0.074568</td>\n      <td>0.689170</td>\n      <td>1.846296</td>\n      <td>0.354458</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97495</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.970588</td>\n      <td>0.168361</td>\n      <td>-0.103084</td>\n      <td>0.014974</td>\n      <td>-0.523512</td>\n      <td>0.006196</td>\n      <td>0.153213</td>\n      <td>...</td>\n      <td>0.527784</td>\n      <td>-0.224120</td>\n      <td>0.383600</td>\n      <td>0.272026</td>\n      <td>-1.200093</td>\n      <td>-1.079990</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97496</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.970588</td>\n      <td>-0.168361</td>\n      <td>0.103084</td>\n      <td>-0.014974</td>\n      <td>0.523512</td>\n      <td>-0.006196</td>\n      <td>-0.153213</td>\n      <td>...</td>\n      <td>-0.527784</td>\n      <td>0.283226</td>\n      <td>-0.383600</td>\n      <td>-0.272026</td>\n      <td>1.200093</td>\n      <td>1.199989</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97497</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.693781</td>\n      <td>0.018406</td>\n      <td>-0.026862</td>\n      <td>0.029209</td>\n      <td>0.035314</td>\n      <td>-0.036594</td>\n      <td>0.041797</td>\n      <td>...</td>\n      <td>0.531486</td>\n      <td>-0.272841</td>\n      <td>0.002416</td>\n      <td>0.576347</td>\n      <td>-1.144704</td>\n      <td>0.663215</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97498</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.771150</td>\n      <td>-0.803193</td>\n      <td>-0.494025</td>\n      <td>-0.387243</td>\n      <td>0.735595</td>\n      <td>-0.533368</td>\n      <td>-0.543970</td>\n      <td>...</td>\n      <td>-0.461861</td>\n      <td>0.172086</td>\n      <td>-0.881853</td>\n      <td>-0.300319</td>\n      <td>-0.640049</td>\n      <td>-1.964203</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97501</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.264992</td>\n      <td>-0.410980</td>\n      <td>0.347706</td>\n      <td>-0.215865</td>\n      <td>0.624402</td>\n      <td>-0.281998</td>\n      <td>-0.114748</td>\n      <td>...</td>\n      <td>-1.710859</td>\n      <td>1.203514</td>\n      <td>-0.642904</td>\n      <td>-1.274541</td>\n      <td>-1.200093</td>\n      <td>-3.026346</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97502</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.642939</td>\n      <td>0.724167</td>\n      <td>0.588465</td>\n      <td>0.342473</td>\n      <td>-0.820158</td>\n      <td>0.171508</td>\n      <td>0.650176</td>\n      <td>...</td>\n      <td>0.236444</td>\n      <td>-0.210784</td>\n      <td>0.899636</td>\n      <td>0.084383</td>\n      <td>1.581074</td>\n      <td>0.117691</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97503</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>1.021586</td>\n      <td>-0.288187</td>\n      <td>0.188016</td>\n      <td>-0.078390</td>\n      <td>0.612461</td>\n      <td>0.324453</td>\n      <td>-0.605446</td>\n      <td>...</td>\n      <td>0.181880</td>\n      <td>0.119229</td>\n      <td>-0.505766</td>\n      <td>0.226516</td>\n      <td>-1.731043</td>\n      <td>-2.018751</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97504</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.771150</td>\n      <td>0.803193</td>\n      <td>0.494025</td>\n      <td>0.387243</td>\n      <td>-0.735595</td>\n      <td>0.533368</td>\n      <td>0.543970</td>\n      <td>...</td>\n      <td>0.461861</td>\n      <td>-0.665881</td>\n      <td>0.881853</td>\n      <td>0.300319</td>\n      <td>0.640049</td>\n      <td>0.913838</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97506</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.264992</td>\n      <td>0.410980</td>\n      <td>-0.347706</td>\n      <td>0.215865</td>\n      <td>-0.624402</td>\n      <td>0.281998</td>\n      <td>0.114748</td>\n      <td>...</td>\n      <td>1.710859</td>\n      <td>-0.048629</td>\n      <td>0.642904</td>\n      <td>1.274541</td>\n      <td>1.200093</td>\n      <td>0.384494</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97507</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.534258</td>\n      <td>-0.487020</td>\n      <td>-0.229087</td>\n      <td>-0.363300</td>\n      <td>0.135014</td>\n      <td>-0.211192</td>\n      <td>-0.516925</td>\n      <td>...</td>\n      <td>-1.324828</td>\n      <td>0.532710</td>\n      <td>-0.435448</td>\n      <td>-0.698285</td>\n      <td>-1.680130</td>\n      <td>-0.201132</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97508</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-1.990059</td>\n      <td>0.339997</td>\n      <td>0.140693</td>\n      <td>0.242682</td>\n      <td>-0.352651</td>\n      <td>0.110891</td>\n      <td>0.325350</td>\n      <td>...</td>\n      <td>0.072093</td>\n      <td>-0.012094</td>\n      <td>0.476165</td>\n      <td>0.079838</td>\n      <td>0.698236</td>\n      <td>-0.284053</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97510</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.965587</td>\n      <td>0.224792</td>\n      <td>0.418576</td>\n      <td>0.077965</td>\n      <td>-0.207953</td>\n      <td>0.017104</td>\n      <td>0.436578</td>\n      <td>...</td>\n      <td>0.409916</td>\n      <td>0.236192</td>\n      <td>0.219801</td>\n      <td>0.201064</td>\n      <td>-0.480037</td>\n      <td>2.057520</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97511</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.220462</td>\n      <td>0.111585</td>\n      <td>0.293026</td>\n      <td>0.269131</td>\n      <td>0.387246</td>\n      <td>0.373988</td>\n      <td>-0.125272</td>\n      <td>...</td>\n      <td>-0.000647</td>\n      <td>-0.046242</td>\n      <td>-0.021412</td>\n      <td>-0.115335</td>\n      <td>0.160012</td>\n      <td>-0.258459</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97513</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.771150</td>\n      <td>0.803193</td>\n      <td>0.494025</td>\n      <td>0.387243</td>\n      <td>-0.735595</td>\n      <td>0.533368</td>\n      <td>0.543970</td>\n      <td>...</td>\n      <td>0.461861</td>\n      <td>-1.788999</td>\n      <td>0.881853</td>\n      <td>0.300319</td>\n      <td>0.640049</td>\n      <td>1.416910</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97515</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-1.021586</td>\n      <td>0.288187</td>\n      <td>-0.188016</td>\n      <td>0.078390</td>\n      <td>-0.612461</td>\n      <td>-0.324453</td>\n      <td>0.605446</td>\n      <td>...</td>\n      <td>-0.181880</td>\n      <td>-0.748435</td>\n      <td>0.505766</td>\n      <td>-0.226516</td>\n      <td>1.731043</td>\n      <td>0.339029</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97516</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.236261</td>\n      <td>-0.637298</td>\n      <td>-0.328874</td>\n      <td>-0.370155</td>\n      <td>0.944889</td>\n      <td>-0.382549</td>\n      <td>-0.301429</td>\n      <td>...</td>\n      <td>-1.488833</td>\n      <td>-0.259815</td>\n      <td>-1.013608</td>\n      <td>-1.302067</td>\n      <td>-1.920148</td>\n      <td>-3.507660</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97517</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.799881</td>\n      <td>-0.556012</td>\n      <td>0.156693</td>\n      <td>-0.232206</td>\n      <td>0.380123</td>\n      <td>-0.433129</td>\n      <td>-0.319243</td>\n      <td>...</td>\n      <td>-0.715729</td>\n      <td>0.660977</td>\n      <td>-0.485751</td>\n      <td>-0.290537</td>\n      <td>0.320025</td>\n      <td>-1.103848</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97518</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.721858</td>\n      <td>-0.150643</td>\n      <td>-0.186307</td>\n      <td>-0.305466</td>\n      <td>0.045970</td>\n      <td>-0.088524</td>\n      <td>-0.036318</td>\n      <td>...</td>\n      <td>1.546883</td>\n      <td>0.011189</td>\n      <td>-0.318440</td>\n      <td>1.655063</td>\n      <td>-0.100008</td>\n      <td>-1.040430</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97520</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.721858</td>\n      <td>0.150643</td>\n      <td>0.186307</td>\n      <td>0.305466</td>\n      <td>-0.045970</td>\n      <td>0.088524</td>\n      <td>0.036318</td>\n      <td>...</td>\n      <td>-1.546883</td>\n      <td>-1.528949</td>\n      <td>0.318440</td>\n      <td>-1.655063</td>\n      <td>0.100008</td>\n      <td>0.173145</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97521</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.236261</td>\n      <td>0.637298</td>\n      <td>0.328874</td>\n      <td>0.370155</td>\n      <td>-0.944889</td>\n      <td>0.382549</td>\n      <td>0.301429</td>\n      <td>...</td>\n      <td>1.488833</td>\n      <td>-0.280894</td>\n      <td>1.013608</td>\n      <td>1.302067</td>\n      <td>1.920148</td>\n      <td>-0.489226</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97522</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.799881</td>\n      <td>0.556012</td>\n      <td>-0.156693</td>\n      <td>0.232206</td>\n      <td>-0.380123</td>\n      <td>0.433129</td>\n      <td>0.319243</td>\n      <td>...</td>\n      <td>0.715729</td>\n      <td>-0.363708</td>\n      <td>0.485751</td>\n      <td>0.290537</td>\n      <td>-0.320025</td>\n      <td>1.198143</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97524</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.799881</td>\n      <td>0.556012</td>\n      <td>-0.156693</td>\n      <td>0.232206</td>\n      <td>-0.380123</td>\n      <td>0.433129</td>\n      <td>0.319243</td>\n      <td>...</td>\n      <td>0.715729</td>\n      <td>-0.628649</td>\n      <td>0.485751</td>\n      <td>0.290537</td>\n      <td>-0.320025</td>\n      <td>0.750762</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97525</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>1.878984</td>\n      <td>2.056819</td>\n      <td>0.847695</td>\n      <td>1.564899</td>\n      <td>1.218667</td>\n      <td>1.730773</td>\n      <td>2.796954</td>\n      <td>...</td>\n      <td>-0.508277</td>\n      <td>0.308164</td>\n      <td>0.604775</td>\n      <td>-0.230817</td>\n      <td>2.880222</td>\n      <td>2.418439</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97526</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.120922</td>\n      <td>0.894619</td>\n      <td>-0.090231</td>\n      <td>0.711220</td>\n      <td>-0.989398</td>\n      <td>0.539852</td>\n      <td>0.653537</td>\n      <td>...</td>\n      <td>0.690210</td>\n      <td>0.269808</td>\n      <td>1.364345</td>\n      <td>0.585934</td>\n      <td>1.753981</td>\n      <td>1.470844</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97527</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.779105</td>\n      <td>-0.441831</td>\n      <td>-0.385912</td>\n      <td>-0.302908</td>\n      <td>0.546595</td>\n      <td>-0.234576</td>\n      <td>-0.368883</td>\n      <td>...</td>\n      <td>-0.848640</td>\n      <td>0.039446</td>\n      <td>-0.669468</td>\n      <td>-0.705324</td>\n      <td>-0.800062</td>\n      <td>0.969222</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>97554</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.771150</td>\n      <td>0.835409</td>\n      <td>0.541206</td>\n      <td>0.432202</td>\n      <td>-0.742077</td>\n      <td>0.555418</td>\n      <td>0.552543</td>\n      <td>...</td>\n      <td>0.482713</td>\n      <td>-0.212935</td>\n      <td>0.928520</td>\n      <td>0.304460</td>\n      <td>0.800062</td>\n      <td>2.148157</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97555</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.771150</td>\n      <td>0.835409</td>\n      <td>0.541206</td>\n      <td>0.432202</td>\n      <td>-0.742077</td>\n      <td>0.555418</td>\n      <td>0.552543</td>\n      <td>...</td>\n      <td>0.482713</td>\n      <td>-0.597832</td>\n      <td>0.928520</td>\n      <td>0.304460</td>\n      <td>0.800062</td>\n      <td>0.980970</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97556</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.970588</td>\n      <td>-0.168361</td>\n      <td>0.103084</td>\n      <td>-0.014974</td>\n      <td>0.523512</td>\n      <td>-0.006196</td>\n      <td>-0.153213</td>\n      <td>...</td>\n      <td>-0.527784</td>\n      <td>0.027652</td>\n      <td>-0.383600</td>\n      <td>-0.272026</td>\n      <td>1.200093</td>\n      <td>0.897223</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97557</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.970588</td>\n      <td>0.168361</td>\n      <td>-0.103084</td>\n      <td>0.014974</td>\n      <td>-0.523512</td>\n      <td>0.006196</td>\n      <td>0.153213</td>\n      <td>...</td>\n      <td>0.527784</td>\n      <td>-0.224120</td>\n      <td>0.383600</td>\n      <td>0.272026</td>\n      <td>-1.200093</td>\n      <td>-1.079990</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97558</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.191483</td>\n      <td>0.234689</td>\n      <td>0.468567</td>\n      <td>0.267070</td>\n      <td>0.014484</td>\n      <td>0.221191</td>\n      <td>0.173091</td>\n      <td>...</td>\n      <td>0.302675</td>\n      <td>-0.038073</td>\n      <td>0.239814</td>\n      <td>0.415443</td>\n      <td>2.000154</td>\n      <td>0.778015</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97559</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.191483</td>\n      <td>0.234689</td>\n      <td>0.468567</td>\n      <td>0.267070</td>\n      <td>0.014484</td>\n      <td>0.221191</td>\n      <td>0.173091</td>\n      <td>...</td>\n      <td>0.302675</td>\n      <td>0.094363</td>\n      <td>0.239814</td>\n      <td>0.415443</td>\n      <td>2.000154</td>\n      <td>-0.238459</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97560</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.134590</td>\n      <td>0.164402</td>\n      <td>-0.426393</td>\n      <td>0.054914</td>\n      <td>-0.184825</td>\n      <td>0.041766</td>\n      <td>0.182883</td>\n      <td>...</td>\n      <td>-0.503648</td>\n      <td>0.737667</td>\n      <td>0.181892</td>\n      <td>-0.156702</td>\n      <td>0.160012</td>\n      <td>-1.550755</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97561</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.618888</td>\n      <td>-0.123898</td>\n      <td>-0.095970</td>\n      <td>-0.335539</td>\n      <td>-0.224445</td>\n      <td>-0.237833</td>\n      <td>0.139508</td>\n      <td>...</td>\n      <td>-0.314967</td>\n      <td>-0.451348</td>\n      <td>-0.155692</td>\n      <td>-0.292459</td>\n      <td>-0.400031</td>\n      <td>-2.939050</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97562</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.645872</td>\n      <td>0.110319</td>\n      <td>0.261398</td>\n      <td>-0.019481</td>\n      <td>-0.242438</td>\n      <td>-0.090697</td>\n      <td>0.074936</td>\n      <td>...</td>\n      <td>0.113908</td>\n      <td>0.376945</td>\n      <td>0.152913</td>\n      <td>0.602475</td>\n      <td>1.040080</td>\n      <td>0.216262</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97563</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.796709</td>\n      <td>0.413485</td>\n      <td>0.363348</td>\n      <td>0.176396</td>\n      <td>-0.419405</td>\n      <td>0.241609</td>\n      <td>0.413611</td>\n      <td>...</td>\n      <td>0.587881</td>\n      <td>5.078858</td>\n      <td>0.461233</td>\n      <td>1.641544</td>\n      <td>1.360105</td>\n      <td>0.060659</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97564</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.618888</td>\n      <td>0.123898</td>\n      <td>0.095970</td>\n      <td>0.335539</td>\n      <td>0.224445</td>\n      <td>0.237833</td>\n      <td>-0.139508</td>\n      <td>...</td>\n      <td>0.314967</td>\n      <td>-0.593199</td>\n      <td>0.155692</td>\n      <td>0.292459</td>\n      <td>0.400031</td>\n      <td>-0.592302</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97565</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.134590</td>\n      <td>-0.164402</td>\n      <td>0.426393</td>\n      <td>-0.054914</td>\n      <td>0.184825</td>\n      <td>-0.041766</td>\n      <td>-0.182883</td>\n      <td>...</td>\n      <td>0.503648</td>\n      <td>-0.367107</td>\n      <td>-0.181892</td>\n      <td>0.156702</td>\n      <td>-0.160012</td>\n      <td>0.290767</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97566</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.645872</td>\n      <td>-0.110319</td>\n      <td>-0.261398</td>\n      <td>0.019481</td>\n      <td>0.242438</td>\n      <td>0.090697</td>\n      <td>-0.074936</td>\n      <td>...</td>\n      <td>-0.113908</td>\n      <td>-0.986500</td>\n      <td>-0.152913</td>\n      <td>-0.602475</td>\n      <td>-1.040080</td>\n      <td>-0.669225</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97567</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.796709</td>\n      <td>-0.413485</td>\n      <td>-0.363348</td>\n      <td>-0.176396</td>\n      <td>0.419405</td>\n      <td>-0.241609</td>\n      <td>-0.413611</td>\n      <td>...</td>\n      <td>-0.587881</td>\n      <td>-5.561994</td>\n      <td>-0.461233</td>\n      <td>-1.641544</td>\n      <td>-1.360105</td>\n      <td>-0.772231</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97568</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.618888</td>\n      <td>-0.123898</td>\n      <td>-0.095970</td>\n      <td>-0.335539</td>\n      <td>-0.224445</td>\n      <td>-0.237833</td>\n      <td>0.139508</td>\n      <td>...</td>\n      <td>-0.314967</td>\n      <td>-0.067071</td>\n      <td>-0.155692</td>\n      <td>-0.292459</td>\n      <td>-0.400031</td>\n      <td>-0.096503</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97569</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.645872</td>\n      <td>0.110319</td>\n      <td>0.261398</td>\n      <td>-0.019481</td>\n      <td>-0.242438</td>\n      <td>-0.090697</td>\n      <td>0.074936</td>\n      <td>...</td>\n      <td>0.113908</td>\n      <td>3.995076</td>\n      <td>0.152913</td>\n      <td>0.602475</td>\n      <td>1.040080</td>\n      <td>0.478102</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97570</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.046457</td>\n      <td>0.385664</td>\n      <td>0.602953</td>\n      <td>0.395408</td>\n      <td>-0.271281</td>\n      <td>0.294637</td>\n      <td>0.149685</td>\n      <td>...</td>\n      <td>0.431114</td>\n      <td>-0.100313</td>\n      <td>0.561708</td>\n      <td>1.337135</td>\n      <td>0.400031</td>\n      <td>-1.483503</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97571</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.224278</td>\n      <td>0.151719</td>\n      <td>-0.143636</td>\n      <td>0.116527</td>\n      <td>0.076321</td>\n      <td>0.184806</td>\n      <td>0.124418</td>\n      <td>...</td>\n      <td>0.471735</td>\n      <td>-0.170158</td>\n      <td>0.055217</td>\n      <td>0.596868</td>\n      <td>1.360105</td>\n      <td>-0.014266</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97572</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.344176</td>\n      <td>0.125378</td>\n      <td>-0.190946</td>\n      <td>-0.005436</td>\n      <td>-0.271436</td>\n      <td>-0.050516</td>\n      <td>0.001336</td>\n      <td>...</td>\n      <td>-0.385132</td>\n      <td>0.989780</td>\n      <td>0.186534</td>\n      <td>-0.067549</td>\n      <td>-0.400031</td>\n      <td>-1.602183</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97573</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.046457</td>\n      <td>-0.385664</td>\n      <td>-0.602953</td>\n      <td>-0.395408</td>\n      <td>0.271281</td>\n      <td>-0.294637</td>\n      <td>-0.149685</td>\n      <td>...</td>\n      <td>-0.431114</td>\n      <td>-3.794668</td>\n      <td>-0.561708</td>\n      <td>-1.337135</td>\n      <td>-0.400031</td>\n      <td>0.037762</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97574</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.224278</td>\n      <td>-0.151719</td>\n      <td>0.143636</td>\n      <td>-0.116527</td>\n      <td>-0.076321</td>\n      <td>-0.184806</td>\n      <td>-0.124418</td>\n      <td>...</td>\n      <td>-0.471735</td>\n      <td>-0.222668</td>\n      <td>-0.055217</td>\n      <td>-0.596868</td>\n      <td>-1.360105</td>\n      <td>-0.812037</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97575</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.167106</td>\n      <td>0.119458</td>\n      <td>-0.013971</td>\n      <td>0.007506</td>\n      <td>-0.153676</td>\n      <td>-0.025360</td>\n      <td>0.244807</td>\n      <td>...</td>\n      <td>-0.067229</td>\n      <td>0.582988</td>\n      <td>0.115517</td>\n      <td>0.471349</td>\n      <td>1.040080</td>\n      <td>-0.103216</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97579</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.344176</td>\n      <td>-0.125378</td>\n      <td>0.190946</td>\n      <td>0.005436</td>\n      <td>0.271436</td>\n      <td>0.050516</td>\n      <td>-0.001336</td>\n      <td>...</td>\n      <td>0.385132</td>\n      <td>-0.446045</td>\n      <td>-0.186534</td>\n      <td>0.067549</td>\n      <td>0.400031</td>\n      <td>-0.486797</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97580</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.046457</td>\n      <td>-0.385664</td>\n      <td>-0.602953</td>\n      <td>-0.395408</td>\n      <td>0.271281</td>\n      <td>-0.294637</td>\n      <td>-0.149685</td>\n      <td>...</td>\n      <td>-0.431114</td>\n      <td>-5.307538</td>\n      <td>-0.561708</td>\n      <td>-1.337135</td>\n      <td>-0.400031</td>\n      <td>-2.187672</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97581</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.344176</td>\n      <td>-0.125378</td>\n      <td>0.190946</td>\n      <td>0.005436</td>\n      <td>0.271436</td>\n      <td>0.050516</td>\n      <td>-0.001336</td>\n      <td>...</td>\n      <td>0.385132</td>\n      <td>-0.274861</td>\n      <td>-0.186534</td>\n      <td>0.067549</td>\n      <td>0.400031</td>\n      <td>-0.276921</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97582</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.167106</td>\n      <td>-0.119458</td>\n      <td>0.013971</td>\n      <td>-0.007506</td>\n      <td>0.153676</td>\n      <td>0.025360</td>\n      <td>-0.244807</td>\n      <td>...</td>\n      <td>0.067229</td>\n      <td>-3.729163</td>\n      <td>-0.115517</td>\n      <td>-0.471349</td>\n      <td>-1.040080</td>\n      <td>-0.585793</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97586</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.167106</td>\n      <td>-0.119458</td>\n      <td>0.013971</td>\n      <td>-0.007506</td>\n      <td>0.153676</td>\n      <td>0.025360</td>\n      <td>-0.244807</td>\n      <td>...</td>\n      <td>0.067229</td>\n      <td>-0.723486</td>\n      <td>-0.115517</td>\n      <td>-0.471349</td>\n      <td>-1.040080</td>\n      <td>0.332305</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97591</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.534889</td>\n      <td>0.288002</td>\n      <td>0.219408</td>\n      <td>0.146491</td>\n      <td>0.113026</td>\n      <td>0.237025</td>\n      <td>0.285987</td>\n      <td>...</td>\n      <td>-0.920322</td>\n      <td>-0.089541</td>\n      <td>0.057358</td>\n      <td>-0.920074</td>\n      <td>-1.120086</td>\n      <td>0.692301</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97593</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.534889</td>\n      <td>-0.288002</td>\n      <td>-0.219408</td>\n      <td>-0.146491</td>\n      <td>-0.113026</td>\n      <td>-0.237025</td>\n      <td>-0.285987</td>\n      <td>...</td>\n      <td>0.920322</td>\n      <td>0.014856</td>\n      <td>-0.057358</td>\n      <td>0.920074</td>\n      <td>1.120086</td>\n      <td>-0.847025</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97595</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.534889</td>\n      <td>0.288002</td>\n      <td>0.219408</td>\n      <td>0.146491</td>\n      <td>0.113026</td>\n      <td>0.237025</td>\n      <td>0.285987</td>\n      <td>...</td>\n      <td>-0.920322</td>\n      <td>-0.606747</td>\n      <td>0.057358</td>\n      <td>-0.920074</td>\n      <td>-1.120086</td>\n      <td>0.946926</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>80 rows Ã— 28 columns</p>\n</div>",
      "text/plain": "       fav_win  t1_win  fav_ind  prize_rating_dif  rating_dif  hs_perc_dif  \\\n97490        0       0     -1.0          0.703403   -0.348008    -0.425264   \n97491        1       0     -1.0          1.343076    2.005799     0.646436   \n97492        0       0      1.0         -0.255373    0.010410    -0.169967   \n97493        1       0      1.0         -0.703403    0.348008     0.425264   \n97494        0       1     -1.0          0.255373   -0.010410     0.169967   \n97495        1       0      1.0         -0.970588    0.168361    -0.103084   \n97496        0       0     -1.0          0.970588   -0.168361     0.103084   \n97497        1       0      1.0         -0.693781    0.018406    -0.026862   \n97498        1       1     -1.0          0.771150   -0.803193    -0.494025   \n97501        1       1     -1.0          0.264992   -0.410980     0.347706   \n97502        0       1      1.0         -0.642939    0.724167     0.588465   \n97503        0       0     -1.0          1.021586   -0.288187     0.188016   \n97504        0       1      1.0         -0.771150    0.803193     0.494025   \n97506        1       0      1.0         -0.264992    0.410980    -0.347706   \n97507        1       1     -1.0          0.534258   -0.487020    -0.229087   \n97508        1       0      1.0         -1.990059    0.339997     0.140693   \n97510        1       1     -1.0          0.965587    0.224792     0.418576   \n97511        1       0      1.0         -0.220462    0.111585     0.293026   \n97513        0       1      1.0         -0.771150    0.803193     0.494025   \n97515        0       1      1.0         -1.021586    0.288187    -0.188016   \n97516        1       1     -1.0          0.236261   -0.637298    -0.328874   \n97517        1       1     -1.0          0.799881   -0.556012     0.156693   \n97518        0       0     -1.0          0.721858   -0.150643    -0.186307   \n97520        0       1      1.0         -0.721858    0.150643     0.186307   \n97521        1       0      1.0         -0.236261    0.637298     0.328874   \n97522        0       1      1.0         -0.799881    0.556012    -0.156693   \n97524        1       0      1.0         -0.799881    0.556012    -0.156693   \n97525        1       1     -1.0          1.878984    2.056819     0.847695   \n97526        0       0     -1.0          0.120922    0.894619    -0.090231   \n97527        0       0     -1.0          0.779105   -0.441831    -0.385912   \n...        ...     ...      ...               ...         ...          ...   \n97554        0       1      1.0         -0.771150    0.835409     0.541206   \n97555        1       0      1.0         -0.771150    0.835409     0.541206   \n97556        1       1     -1.0          0.970588   -0.168361     0.103084   \n97557        1       0      1.0         -0.970588    0.168361    -0.103084   \n97558        0       0     -1.0          0.191483    0.234689     0.468567   \n97559        0       0     -1.0          0.191483    0.234689     0.468567   \n97560        0       0      1.0         -0.134590    0.164402    -0.426393   \n97561        1       1     -1.0          0.618888   -0.123898    -0.095970   \n97562        1       1     -1.0          0.645872    0.110319     0.261398   \n97563        1       1     -1.0          0.796709    0.413485     0.363348   \n97564        0       1      1.0         -0.618888    0.123898     0.095970   \n97565        0       1     -1.0          0.134590   -0.164402     0.426393   \n97566        0       1      1.0         -0.645872   -0.110319    -0.261398   \n97567        1       0      1.0         -0.796709   -0.413485    -0.363348   \n97568        1       1     -1.0          0.618888   -0.123898    -0.095970   \n97569        1       1     -1.0          0.645872    0.110319     0.261398   \n97570        1       1      1.0         -0.046457    0.385664     0.602953   \n97571        1       1     -1.0          0.224278    0.151719    -0.143636   \n97572        0       0     -1.0          0.344176    0.125378    -0.190946   \n97573        0       1     -1.0          0.046457   -0.385664    -0.602953   \n97574        1       0      1.0         -0.224278   -0.151719     0.143636   \n97575        0       0     -1.0          0.167106    0.119458    -0.013971   \n97579        1       0      1.0         -0.344176   -0.125378     0.190946   \n97580        1       0     -1.0          0.046457   -0.385664    -0.602953   \n97581        0       1      1.0         -0.344176   -0.125378     0.190946   \n97582        1       0      1.0         -0.167106   -0.119458     0.013971   \n97586        1       0      1.0         -0.167106   -0.119458     0.013971   \n97591        1       0      1.0         -0.534889    0.288002     0.219408   \n97593        0       0     -1.0          0.534889   -0.288002    -0.219408   \n97595        0       1      1.0         -0.534889    0.288002     0.219408   \n\n       kills_per_rd_dif  deaths_per_rd_dif   adr_dif  kast_dif  ...  \\\n97490         -0.416823           0.214299 -0.376182 -0.254745  ...   \n97491          1.535165           1.321096  1.654750  2.972975  ...   \n97492          0.017783          -0.082043 -0.083886  0.005859  ...   \n97493          0.416823          -0.214299  0.376182  0.254745  ...   \n97494         -0.017783           0.082043  0.083886 -0.005859  ...   \n97495          0.014974          -0.523512  0.006196  0.153213  ...   \n97496         -0.014974           0.523512 -0.006196 -0.153213  ...   \n97497          0.029209           0.035314 -0.036594  0.041797  ...   \n97498         -0.387243           0.735595 -0.533368 -0.543970  ...   \n97501         -0.215865           0.624402 -0.281998 -0.114748  ...   \n97502          0.342473          -0.820158  0.171508  0.650176  ...   \n97503         -0.078390           0.612461  0.324453 -0.605446  ...   \n97504          0.387243          -0.735595  0.533368  0.543970  ...   \n97506          0.215865          -0.624402  0.281998  0.114748  ...   \n97507         -0.363300           0.135014 -0.211192 -0.516925  ...   \n97508          0.242682          -0.352651  0.110891  0.325350  ...   \n97510          0.077965          -0.207953  0.017104  0.436578  ...   \n97511          0.269131           0.387246  0.373988 -0.125272  ...   \n97513          0.387243          -0.735595  0.533368  0.543970  ...   \n97515          0.078390          -0.612461 -0.324453  0.605446  ...   \n97516         -0.370155           0.944889 -0.382549 -0.301429  ...   \n97517         -0.232206           0.380123 -0.433129 -0.319243  ...   \n97518         -0.305466           0.045970 -0.088524 -0.036318  ...   \n97520          0.305466          -0.045970  0.088524  0.036318  ...   \n97521          0.370155          -0.944889  0.382549  0.301429  ...   \n97522          0.232206          -0.380123  0.433129  0.319243  ...   \n97524          0.232206          -0.380123  0.433129  0.319243  ...   \n97525          1.564899           1.218667  1.730773  2.796954  ...   \n97526          0.711220          -0.989398  0.539852  0.653537  ...   \n97527         -0.302908           0.546595 -0.234576 -0.368883  ...   \n...                 ...                ...       ...       ...  ...   \n97554          0.432202          -0.742077  0.555418  0.552543  ...   \n97555          0.432202          -0.742077  0.555418  0.552543  ...   \n97556         -0.014974           0.523512 -0.006196 -0.153213  ...   \n97557          0.014974          -0.523512  0.006196  0.153213  ...   \n97558          0.267070           0.014484  0.221191  0.173091  ...   \n97559          0.267070           0.014484  0.221191  0.173091  ...   \n97560          0.054914          -0.184825  0.041766  0.182883  ...   \n97561         -0.335539          -0.224445 -0.237833  0.139508  ...   \n97562         -0.019481          -0.242438 -0.090697  0.074936  ...   \n97563          0.176396          -0.419405  0.241609  0.413611  ...   \n97564          0.335539           0.224445  0.237833 -0.139508  ...   \n97565         -0.054914           0.184825 -0.041766 -0.182883  ...   \n97566          0.019481           0.242438  0.090697 -0.074936  ...   \n97567         -0.176396           0.419405 -0.241609 -0.413611  ...   \n97568         -0.335539          -0.224445 -0.237833  0.139508  ...   \n97569         -0.019481          -0.242438 -0.090697  0.074936  ...   \n97570          0.395408          -0.271281  0.294637  0.149685  ...   \n97571          0.116527           0.076321  0.184806  0.124418  ...   \n97572         -0.005436          -0.271436 -0.050516  0.001336  ...   \n97573         -0.395408           0.271281 -0.294637 -0.149685  ...   \n97574         -0.116527          -0.076321 -0.184806 -0.124418  ...   \n97575          0.007506          -0.153676 -0.025360  0.244807  ...   \n97579          0.005436           0.271436  0.050516 -0.001336  ...   \n97580         -0.395408           0.271281 -0.294637 -0.149685  ...   \n97581          0.005436           0.271436  0.050516 -0.001336  ...   \n97582         -0.007506           0.153676  0.025360 -0.244807  ...   \n97586         -0.007506           0.153676  0.025360 -0.244807  ...   \n97591          0.146491           0.113026  0.237025  0.285987  ...   \n97593         -0.146491          -0.113026 -0.237025 -0.285987  ...   \n97595          0.146491           0.113026  0.237025  0.285987  ...   \n\n       scaled_score_dif_dif  win_rate_map_dif  kd_per_round_dif  \\\n97490             -1.124069         -0.126876         -0.541530   \n97491              0.053010         -0.002953          0.504626   \n97492             -0.770420          0.006047          0.074568   \n97493              1.124069         -0.379142          0.541530   \n97494              0.770420          0.101901         -0.074568   \n97495              0.527784         -0.224120          0.383600   \n97496             -0.527784          0.283226         -0.383600   \n97497              0.531486         -0.272841          0.002416   \n97498             -0.461861          0.172086         -0.881853   \n97501             -1.710859          1.203514         -0.642904   \n97502              0.236444         -0.210784          0.899636   \n97503              0.181880          0.119229         -0.505766   \n97504              0.461861         -0.665881          0.881853   \n97506              1.710859         -0.048629          0.642904   \n97507             -1.324828          0.532710         -0.435448   \n97508              0.072093         -0.012094          0.476165   \n97510              0.409916          0.236192          0.219801   \n97511             -0.000647         -0.046242         -0.021412   \n97513              0.461861         -1.788999          0.881853   \n97515             -0.181880         -0.748435          0.505766   \n97516             -1.488833         -0.259815         -1.013608   \n97517             -0.715729          0.660977         -0.485751   \n97518              1.546883          0.011189         -0.318440   \n97520             -1.546883         -1.528949          0.318440   \n97521              1.488833         -0.280894          1.013608   \n97522              0.715729         -0.363708          0.485751   \n97524              0.715729         -0.628649          0.485751   \n97525             -0.508277          0.308164          0.604775   \n97526              0.690210          0.269808          1.364345   \n97527             -0.848640          0.039446         -0.669468   \n...                     ...               ...               ...   \n97554              0.482713         -0.212935          0.928520   \n97555              0.482713         -0.597832          0.928520   \n97556             -0.527784          0.027652         -0.383600   \n97557              0.527784         -0.224120          0.383600   \n97558              0.302675         -0.038073          0.239814   \n97559              0.302675          0.094363          0.239814   \n97560             -0.503648          0.737667          0.181892   \n97561             -0.314967         -0.451348         -0.155692   \n97562              0.113908          0.376945          0.152913   \n97563              0.587881          5.078858          0.461233   \n97564              0.314967         -0.593199          0.155692   \n97565              0.503648         -0.367107         -0.181892   \n97566             -0.113908         -0.986500         -0.152913   \n97567             -0.587881         -5.561994         -0.461233   \n97568             -0.314967         -0.067071         -0.155692   \n97569              0.113908          3.995076          0.152913   \n97570              0.431114         -0.100313          0.561708   \n97571              0.471735         -0.170158          0.055217   \n97572             -0.385132          0.989780          0.186534   \n97573             -0.431114         -3.794668         -0.561708   \n97574             -0.471735         -0.222668         -0.055217   \n97575             -0.067229          0.582988          0.115517   \n97579              0.385132         -0.446045         -0.186534   \n97580             -0.431114         -5.307538         -0.561708   \n97581              0.385132         -0.274861         -0.186534   \n97582              0.067229         -3.729163         -0.115517   \n97586              0.067229         -0.723486         -0.115517   \n97591             -0.920322         -0.089541          0.057358   \n97593              0.920322          0.014856         -0.057358   \n97595             -0.920322         -0.606747          0.057358   \n\n       scaled_kd_dif  momentum_dif  map_rating_dif  tier_1  tier_2  fav_ind_2  \\\n97490      -0.874358     -1.353951       -1.084605     2.0     4.0        1.0   \n97491       0.025537      2.080160       -0.470765     6.0     5.0       -1.0   \n97492      -0.689170     -1.846296       -0.973705     2.0     2.0       -1.0   \n97493       0.874358      1.353951       -1.892290     4.0     2.0       -1.0   \n97494       0.689170      1.846296        0.354458     2.0     2.0        1.0   \n97495       0.272026     -1.200093       -1.079990     4.0     2.0       -1.0   \n97496      -0.272026      1.200093        1.199989     2.0     4.0        1.0   \n97497       0.576347     -1.144704        0.663215     3.0     2.0       -1.0   \n97498      -0.300319     -0.640049       -1.964203     1.0     2.0        1.0   \n97501      -1.274541     -1.200093       -3.026346     1.0     2.0        1.0   \n97502       0.084383      1.581074        0.117691     6.0     2.0       -1.0   \n97503       0.226516     -1.731043       -2.018751     1.0     3.0        1.0   \n97504       0.300319      0.640049        0.913838     2.0     1.0       -1.0   \n97506       1.274541      1.200093        0.384494     2.0     1.0       -1.0   \n97507      -0.698285     -1.680130       -0.201132     1.0     3.0        1.0   \n97508       0.079838      0.698236       -0.284053     4.0     2.0       -1.0   \n97510       0.201064     -0.480037        2.057520     2.0     3.0       -1.0   \n97511      -0.115335      0.160012       -0.258459     4.0     2.0       -1.0   \n97513       0.300319      0.640049        1.416910     2.0     1.0       -1.0   \n97515      -0.226516      1.731043        0.339029     3.0     1.0       -1.0   \n97516      -1.302067     -1.920148       -3.507660     1.0     2.0        1.0   \n97517      -0.290537      0.320025       -1.103848     1.0     2.0        1.0   \n97518       1.655063     -0.100008       -1.040430     1.0     3.0        1.0   \n97520      -1.655063      0.100008        0.173145     3.0     1.0       -1.0   \n97521       1.302067      1.920148       -0.489226     2.0     1.0       -1.0   \n97522       0.290537     -0.320025        1.198143     2.0     1.0       -1.0   \n97524       0.290537     -0.320025        0.750762     2.0     1.0       -1.0   \n97525      -0.230817      2.880222        2.418439     2.0     5.0       -1.0   \n97526       0.585934      1.753981        1.470844     4.0     5.0       -1.0   \n97527      -0.705324     -0.800062        0.969222     2.0     4.0        1.0   \n...              ...           ...             ...     ...     ...        ...   \n97554       0.304460      0.800062        2.148157     2.0     1.0       -1.0   \n97555       0.304460      0.800062        0.980970     2.0     1.0       -1.0   \n97556      -0.272026      1.200093        0.897223     2.0     4.0        1.0   \n97557       0.272026     -1.200093       -1.079990     4.0     2.0       -1.0   \n97558       0.415443      2.000154        0.778015     2.0     2.0       -1.0   \n97559       0.415443      2.000154       -0.238459     2.0     2.0       -1.0   \n97560      -0.156702      0.160012       -1.550755     1.0     2.0       -1.0   \n97561      -0.292459     -0.400031       -2.939050     1.0     2.0        1.0   \n97562       0.602475      1.040080        0.216262     1.0     3.0       -1.0   \n97563       1.641544      1.360105        0.060659     1.0     3.0       -1.0   \n97564       0.292459      0.400031       -0.592302     2.0     1.0       -1.0   \n97565       0.156702     -0.160012        0.290767     2.0     1.0        1.0   \n97566      -0.602475     -1.040080       -0.669225     3.0     1.0        1.0   \n97567      -1.641544     -1.360105       -0.772231     3.0     1.0        1.0   \n97568      -0.292459     -0.400031       -0.096503     1.0     2.0        1.0   \n97569       0.602475      1.040080        0.478102     1.0     3.0       -1.0   \n97570       1.337135      0.400031       -1.483503     1.0     1.0       -1.0   \n97571       0.596868      1.360105       -0.014266     2.0     3.0       -1.0   \n97572      -0.067549     -0.400031       -1.602183     1.0     3.0       -1.0   \n97573      -1.337135     -0.400031        0.037762     1.0     1.0        1.0   \n97574      -0.596868     -1.360105       -0.812037     3.0     2.0        1.0   \n97575       0.471349      1.040080       -0.103216     1.0     2.0       -1.0   \n97579       0.067549      0.400031       -0.486797     3.0     1.0        1.0   \n97580      -1.337135     -0.400031       -2.187672     1.0     1.0        1.0   \n97581       0.067549      0.400031       -0.276921     3.0     1.0        1.0   \n97582      -0.471349     -1.040080       -0.585793     2.0     1.0        1.0   \n97586      -0.471349     -1.040080        0.332305     2.0     1.0        1.0   \n97591      -0.920074     -1.120086        0.692301     1.0     1.0       -1.0   \n97593       0.920074      1.120086       -0.847025     1.0     1.0        1.0   \n97595      -0.920074     -1.120086        0.946926     1.0     1.0       -1.0   \n\n       fav_ind_3  \n97490        1.0  \n97491       -1.0  \n97492       -1.0  \n97493       -1.0  \n97494        1.0  \n97495       -1.0  \n97496        1.0  \n97497       -1.0  \n97498        1.0  \n97501        1.0  \n97502       -1.0  \n97503        1.0  \n97504       -1.0  \n97506       -1.0  \n97507        1.0  \n97508       -1.0  \n97510       -1.0  \n97511       -1.0  \n97513       -1.0  \n97515       -1.0  \n97516        1.0  \n97517        1.0  \n97518        1.0  \n97520       -1.0  \n97521       -1.0  \n97522       -1.0  \n97524       -1.0  \n97525       -1.0  \n97526       -1.0  \n97527        1.0  \n...          ...  \n97554       -1.0  \n97555       -1.0  \n97556        1.0  \n97557       -1.0  \n97558       -1.0  \n97559       -1.0  \n97560       -1.0  \n97561        1.0  \n97562        1.0  \n97563       -1.0  \n97564       -1.0  \n97565        1.0  \n97566       -1.0  \n97567        1.0  \n97568        1.0  \n97569        1.0  \n97570       -1.0  \n97571       -1.0  \n97572        1.0  \n97573        1.0  \n97574        1.0  \n97575       -1.0  \n97579       -1.0  \n97580        1.0  \n97581       -1.0  \n97582        1.0  \n97586        1.0  \n97591       -1.0  \n97593        1.0  \n97595       -1.0  \n\n[80 rows x 28 columns]"
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do some feature engineering\n",
    "df_all_feat = pd.DataFrame.from_dict(map_training_dict,orient='index')\n",
    "\n",
    "df_all_feat = df_all_feat.fillna(0)\n",
    "\n",
    "#df_train = pd.DataFrame()\n",
    "\n",
    "df_train = df_all_feat[['fav_win','t1_win','fav_ind']].copy()\n",
    "\n",
    "feat_names = ['prize_rating','rating','hs_perc','kills_per_rd','deaths_per_rd','adr',\n",
    "                'kast','assists_per_rd','flash_per_rd','first_kills_dif','team_rank','score_dif','win_rate',\n",
    "                'scaled_win','scaled_rating','scaled_score_dif','win_rate_map','kd_per_round','scaled_kd','momentum','map_rating']\n",
    "\n",
    "for feat in feat_names:\n",
    "\n",
    "    avg_0 = 0.0*df_all_feat['t_1_p_0_rating']\n",
    "    avg_1 = 0.0*df_all_feat['t_1_p_1_rating']\n",
    "\n",
    "    vec_both = []\n",
    "\n",
    "    for ind in range(0,5):\n",
    "        # df_train[feat+'_p_'+str(ind)+'_dif'] = df_all_feat['t_0_p_'+str(ind)+'_'+feat] - df_all_feat['t_1_p_'+str(ind)+'_'+feat]\n",
    "\n",
    "        avg_0 = avg_0 + df_all_feat['t_0_p_'+str(ind)+'_'+feat]\n",
    "        avg_1 = avg_1 + df_all_feat['t_1_p_'+str(ind)+'_'+feat]\n",
    "\n",
    "        # Normalized\n",
    "        # avg_0 = avg_0 + (df_all_feat['t_0_p_'+str(ind)+'_'+feat]-df_all_feat['t_0_p_'+str(ind)+'_'+feat].mean())/df_all_feat['t_0_p_'+str(ind)+'_'+feat].std()\n",
    "        # avg_1 = avg_1 + (df_all_feat['t_1_p_'+str(ind)+'_'+feat]-df_all_feat['t_1_p_'+str(ind)+'_'+feat].mean())/df_all_feat['t_1_p_'+str(ind)+'_'+feat].std()\n",
    "\n",
    "        vec_both.append( df_all_feat['t_0_p_'+str(ind)+'_'+feat].values ) \n",
    "        vec_both.append( df_all_feat['t_1_p_'+str(ind)+'_'+feat].values ) \n",
    "   \n",
    "    # Normalize\n",
    "    avg_0 = (avg_0 - np.mean(vec_both)) / np.std(vec_both)\n",
    "    avg_1 = (avg_1 - np.mean(vec_both)) / np.std(vec_both)\n",
    "\n",
    "    df_train[feat+'_dif'] = ( avg_0 - avg_1 ) / 5.0\n",
    "\n",
    "    # Take the first and last\n",
    "    # ind = 0\n",
    "    # df_train[feat+'_p_'+str(ind)+'_dif'] = df_all_feat['t_0_p_'+str(ind)+'_'+feat] - df_all_feat['t_1_p_'+str(ind)+'_'+feat]\n",
    "\n",
    "    # ind = 4\n",
    "    # df_train[feat+'_p_'+str(ind)+'_dif'] = df_all_feat['t_0_p_'+str(ind)+'_'+feat] - df_all_feat['t_1_p_'+str(ind)+'_'+feat]\n",
    "    \n",
    "# df_train['prize_rating_dif'] = np.sqrt( abs( df_train['prize_rating_dif'] )) *np.sign(df_train['prize_rating_dif']) \n",
    "\n",
    "# Does it help if we normalize everything?\n",
    "# df_train['norm_dif'] = 0.0\n",
    "\n",
    "\n",
    "# for column in df_train:\n",
    "#     if (column != 'fav_win') & (column !='t1_win') & (column !='fav_ind') & (column != 'norm_dif'):\n",
    "        \n",
    "#         df_train[column] = (df_train[column]-df_train[column].mean())/df_train[column].std()\n",
    "\n",
    "        # Try using the norm of the difference\n",
    "        # df_train['norm_dif'] = df_train['norm_dif'] + df_train[column]\n",
    "\n",
    "        # Try using a sigmoid function to exagerate the feature difference for small changes\n",
    "        # df_train[column] = np.arctan( 0.50*df_train[column] )\n",
    "\n",
    "# df_train['norm_dif'] = df_train['norm_dif'] * np.sign( df_train['rating_dif'] ) / df_train['norm_dif'].max()\n",
    "# df_train['norm_dif'] = np.sign( df_train['rating_dif'] )* (df_train['rating_dif']**2 + df_train['prize_rating_dif']**2 )\n",
    "\n",
    "df_train['tier_1'] = np.ceil( df_all_feat['t1_rank'] / 20.0 )\n",
    "df_train['tier_2'] = np.ceil( df_all_feat['t2_rank'] / 20.0 )\n",
    "\n",
    "df_train = df_train.fillna(0)\n",
    "\n",
    "\n",
    "df_train['fav_ind'] = -1.0*(df_train['prize_rating_dif']>=0.0) + 1.0*(df_train['prize_rating_dif']<0.0)\n",
    "\n",
    "df_train['fav_ind_2'] = -1.0*(df_train['rating_dif']>=0.0) + 1.0*(df_train['rating_dif']<0.0)\n",
    "\n",
    "df_train['fav_ind_3'] = -1.0*(df_train['kills_per_rd_dif']>=0.0) + 1.0*(df_train['kills_per_rd_dif']<0.0)\n",
    "\n",
    "# df_train['prize_rating_dif'] = np.sign(df_train['prize_rating_dif'])*(np.abs(df_train['prize_rating_dif'])**0.1)\n",
    "\n",
    "# df_train['scaled_win_dif'] = np.sign(df_train['scaled_win_dif'])*(np.abs(df_train['scaled_win_dif'])**2)\n",
    "\n",
    "#print(df_train.keys())\n",
    "\n",
    "print(df_train['t1_win'].mean())\n",
    "\n",
    "df_train.tail(n=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fav_win</th>\n      <th>t1_win</th>\n      <th>fav_ind</th>\n      <th>map_rating_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>92838</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.810772</td>\n    </tr>\n    <tr>\n      <th>92845</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>1.340639</td>\n    </tr>\n    <tr>\n      <th>92846</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>-0.055113</td>\n    </tr>\n    <tr>\n      <th>92855</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.047574</td>\n    </tr>\n    <tr>\n      <th>92856</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.462612</td>\n    </tr>\n    <tr>\n      <th>92860</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.712879</td>\n    </tr>\n    <tr>\n      <th>92868</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.634560</td>\n    </tr>\n    <tr>\n      <th>92873</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-2.254641</td>\n    </tr>\n    <tr>\n      <th>92875</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>2.075389</td>\n    </tr>\n    <tr>\n      <th>92878</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.613673</td>\n    </tr>\n    <tr>\n      <th>92880</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.471019</td>\n    </tr>\n    <tr>\n      <th>92881</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.040758</td>\n    </tr>\n    <tr>\n      <th>92883</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>1.133198</td>\n    </tr>\n    <tr>\n      <th>92884</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.376336</td>\n    </tr>\n    <tr>\n      <th>92885</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.985262</td>\n    </tr>\n    <tr>\n      <th>92887</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.529861</td>\n    </tr>\n    <tr>\n      <th>92888</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-1.585390</td>\n    </tr>\n    <tr>\n      <th>92889</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.738730</td>\n    </tr>\n    <tr>\n      <th>92892</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.190331</td>\n    </tr>\n    <tr>\n      <th>92899</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2.895303</td>\n    </tr>\n    <tr>\n      <th>92901</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.572040</td>\n    </tr>\n    <tr>\n      <th>92919</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.315261</td>\n    </tr>\n    <tr>\n      <th>92921</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.641090</td>\n    </tr>\n    <tr>\n      <th>92922</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.055270</td>\n    </tr>\n    <tr>\n      <th>92926</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>1.996390</td>\n    </tr>\n    <tr>\n      <th>92927</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.143308</td>\n    </tr>\n    <tr>\n      <th>92931</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-1.193533</td>\n    </tr>\n    <tr>\n      <th>92933</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.312082</td>\n    </tr>\n    <tr>\n      <th>92936</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.537423</td>\n    </tr>\n    <tr>\n      <th>92938</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.250564</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>93188</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.367379</td>\n    </tr>\n    <tr>\n      <th>93190</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.680072</td>\n    </tr>\n    <tr>\n      <th>93192</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.472651</td>\n    </tr>\n    <tr>\n      <th>93194</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.822891</td>\n    </tr>\n    <tr>\n      <th>93195</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>-0.861858</td>\n    </tr>\n    <tr>\n      <th>93197</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-3.343187</td>\n    </tr>\n    <tr>\n      <th>93199</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.783172</td>\n    </tr>\n    <tr>\n      <th>93202</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.741423</td>\n    </tr>\n    <tr>\n      <th>93205</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.894713</td>\n    </tr>\n    <tr>\n      <th>93219</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-1.907797</td>\n    </tr>\n    <tr>\n      <th>93221</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>-0.815719</td>\n    </tr>\n    <tr>\n      <th>93222</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.549974</td>\n    </tr>\n    <tr>\n      <th>93223</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>1.000610</td>\n    </tr>\n    <tr>\n      <th>93224</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>-2.931727</td>\n    </tr>\n    <tr>\n      <th>93227</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.558413</td>\n    </tr>\n    <tr>\n      <th>93231</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.128152</td>\n    </tr>\n    <tr>\n      <th>93232</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.959282</td>\n    </tr>\n    <tr>\n      <th>93233</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.403972</td>\n    </tr>\n    <tr>\n      <th>93234</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.064911</td>\n    </tr>\n    <tr>\n      <th>93236</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.337628</td>\n    </tr>\n    <tr>\n      <th>93237</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.144763</td>\n    </tr>\n    <tr>\n      <th>93239</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-1.508545</td>\n    </tr>\n    <tr>\n      <th>93240</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.153564</td>\n    </tr>\n    <tr>\n      <th>93241</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.249217</td>\n    </tr>\n    <tr>\n      <th>93242</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-2.870814</td>\n    </tr>\n    <tr>\n      <th>93243</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>2.378578</td>\n    </tr>\n    <tr>\n      <th>93246</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.573548</td>\n    </tr>\n    <tr>\n      <th>93247</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-2.106820</td>\n    </tr>\n    <tr>\n      <th>93251</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-2.233881</td>\n    </tr>\n    <tr>\n      <th>93262</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.464523</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 4 columns</p>\n</div>",
      "text/plain": "       fav_win  t1_win  fav_ind  map_rating_dif\n92838        1       0      1.0       -0.810772\n92845        0       0     -1.0        1.340639\n92846        1       1     -1.0       -0.055113\n92855        1       1     -1.0        0.047574\n92856        1       0      1.0        0.462612\n92860        0       1      1.0       -0.712879\n92868        1       1     -1.0        0.634560\n92873        0       0      1.0       -2.254641\n92875        0       0     -1.0        2.075389\n92878        1       0      1.0        0.613673\n92880        1       0     -1.0       -0.471019\n92881        1       0      1.0       -0.040758\n92883        1       1     -1.0        1.133198\n92884        0       0     -1.0       -0.376336\n92885        0       0      1.0        0.985262\n92887        0       0     -1.0       -0.529861\n92888        0       0     -1.0       -1.585390\n92889        1       0      1.0        1.738730\n92892        0       0     -1.0        0.190331\n92899        0       0      1.0        2.895303\n92901        0       0      1.0        1.572040\n92919        0       1      1.0        2.315261\n92921        1       0      1.0        1.641090\n92922        1       0      1.0       -0.055270\n92926        0       0     -1.0        1.996390\n92927        0       1      1.0        0.143308\n92931        0       1      1.0       -1.193533\n92933        1       1     -1.0        0.312082\n92936        0       1      1.0        0.537423\n92938        0       0     -1.0       -0.250564\n...        ...     ...      ...             ...\n93188        1       1      1.0        2.367379\n93190        1       0      1.0       -0.680072\n93192        1       1      1.0        0.472651\n93194        1       0     -1.0        0.822891\n93195        0       1     -1.0       -0.861858\n93197        0       0      1.0       -3.343187\n93199        1       0     -1.0       -0.783172\n93202        0       0     -1.0        0.741423\n93205        1       0     -1.0       -0.894713\n93219        1       0      1.0       -1.907797\n93221        0       1     -1.0       -0.815719\n93222        0       0     -1.0       -0.549974\n93223        1       1     -1.0        1.000610\n93224        1       1     -1.0       -2.931727\n93227        1       1      1.0        0.558413\n93231        0       1      1.0        0.128152\n93232        0       0      1.0        0.959282\n93233        1       0      1.0       -0.403972\n93234        1       0      1.0        1.064911\n93236        0       0     -1.0        0.337628\n93237        0       0     -1.0       -0.144763\n93239        0       1      1.0       -1.508545\n93240        0       0     -1.0       -0.153564\n93241        0       0     -1.0       -0.249217\n93242        1       0      1.0       -2.870814\n93243        1       1     -1.0        2.378578\n93246        0       1      1.0        1.573548\n93247        1       0      1.0       -2.106820\n93251        0       0     -1.0       -2.233881\n93262        1       0     -1.0       -0.464523\n\n[100 rows x 4 columns]"
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['fav_win', 't1_win', 'fav_ind', 'map_rating_dif']].head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fav_win</th>\n      <th>t1_win</th>\n      <th>fav_ind</th>\n      <th>prize_rating_dif</th>\n      <th>rating_dif</th>\n      <th>hs_perc_dif</th>\n      <th>kills_per_rd_dif</th>\n      <th>deaths_per_rd_dif</th>\n      <th>adr_dif</th>\n      <th>kast_dif</th>\n      <th>assists_per_rd_dif</th>\n      <th>flash_per_rd_dif</th>\n      <th>first_kills_dif_dif</th>\n      <th>team_rank_dif</th>\n      <th>score_dif_dif</th>\n      <th>win_rate_dif</th>\n      <th>scaled_win_dif</th>\n      <th>scaled_rating_dif</th>\n      <th>scaled_score_dif_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.605000</td>\n      <td>0.495000</td>\n      <td>1.506000</td>\n      <td>-5.329071e-18</td>\n      <td>3.552714e-18</td>\n      <td>1.776357e-18</td>\n      <td>1.065814e-17</td>\n      <td>3.552714e-18</td>\n      <td>7.105427e-18</td>\n      <td>-1.065814e-17</td>\n      <td>-4.440892e-18</td>\n      <td>1.332268e-17</td>\n      <td>1.065814e-17</td>\n      <td>-7.105427e-18</td>\n      <td>3.552714e-17</td>\n      <td>-3.197442e-17</td>\n      <td>3.552714e-18</td>\n      <td>-3.552714e-18</td>\n      <td>-2.309264e-17</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.489095</td>\n      <td>0.500225</td>\n      <td>0.500214</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-4.294535e+00</td>\n      <td>-7.087553e+00</td>\n      <td>-6.435974e+00</td>\n      <td>-7.735193e+00</td>\n      <td>-7.817805e+00</td>\n      <td>-8.118688e+00</td>\n      <td>-8.099462e+00</td>\n      <td>-5.048495e+00</td>\n      <td>-6.284287e+00</td>\n      <td>-4.760382e+00</td>\n      <td>-3.583056e+00</td>\n      <td>-4.217007e+00</td>\n      <td>-3.724805e+00</td>\n      <td>-1.128112e+01</td>\n      <td>-5.958851e+00</td>\n      <td>-7.928005e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-4.468085e-01</td>\n      <td>-3.924654e-01</td>\n      <td>-4.381664e-01</td>\n      <td>-3.243540e-01</td>\n      <td>-3.263356e-01</td>\n      <td>-2.815156e-01</td>\n      <td>-2.678898e-01</td>\n      <td>-5.383833e-01</td>\n      <td>-5.687058e-01</td>\n      <td>-5.859129e-01</td>\n      <td>-4.851744e-01</td>\n      <td>-5.956345e-01</td>\n      <td>-6.036420e-01</td>\n      <td>-1.568888e-01</td>\n      <td>-2.703883e-01</td>\n      <td>-1.762917e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.314647e-03</td>\n      <td>3.299395e-02</td>\n      <td>2.559003e-02</td>\n      <td>4.188134e-02</td>\n      <td>4.807448e-02</td>\n      <td>3.512834e-02</td>\n      <td>6.239436e-02</td>\n      <td>1.682523e-02</td>\n      <td>1.543661e-02</td>\n      <td>-2.020498e-02</td>\n      <td>1.221722e-02</td>\n      <td>2.430884e-02</td>\n      <td>3.373862e-03</td>\n      <td>7.291137e-02</td>\n      <td>-1.931123e-03</td>\n      <td>1.044269e-01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>4.447292e-01</td>\n      <td>4.738720e-01</td>\n      <td>5.452574e-01</td>\n      <td>4.213216e-01</td>\n      <td>4.289761e-01</td>\n      <td>3.676461e-01</td>\n      <td>3.678864e-01</td>\n      <td>5.776503e-01</td>\n      <td>5.976279e-01</td>\n      <td>6.196910e-01</td>\n      <td>4.602681e-01</td>\n      <td>5.966732e-01</td>\n      <td>6.204561e-01</td>\n      <td>2.741010e-01</td>\n      <td>2.377506e-01</td>\n      <td>3.057342e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>4.291147e+00</td>\n      <td>6.331305e+00</td>\n      <td>6.316330e+00</td>\n      <td>7.090528e+00</td>\n      <td>7.788433e+00</td>\n      <td>7.684703e+00</td>\n      <td>7.808019e+00</td>\n      <td>5.311427e+00</td>\n      <td>6.438761e+00</td>\n      <td>4.460250e+00</td>\n      <td>4.320149e+00</td>\n      <td>3.464001e+00</td>\n      <td>4.228936e+00</td>\n      <td>4.603757e+00</td>\n      <td>5.245539e+00</td>\n      <td>8.470654e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "           fav_win       t1_win      fav_ind  prize_rating_dif    rating_dif  \\\ncount  1000.000000  1000.000000  1000.000000      1.000000e+03  1.000000e+03   \nmean      0.605000     0.495000     1.506000     -5.329071e-18  3.552714e-18   \nstd       0.489095     0.500225     0.500214      1.000000e+00  1.000000e+00   \nmin       0.000000     0.000000     1.000000     -4.294535e+00 -7.087553e+00   \n25%       0.000000     0.000000     1.000000     -4.468085e-01 -3.924654e-01   \n50%       1.000000     0.000000     2.000000      1.314647e-03  3.299395e-02   \n75%       1.000000     1.000000     2.000000      4.447292e-01  4.738720e-01   \nmax       1.000000     1.000000     2.000000      4.291147e+00  6.331305e+00   \n\n        hs_perc_dif  kills_per_rd_dif  deaths_per_rd_dif       adr_dif  \\\ncount  1.000000e+03      1.000000e+03       1.000000e+03  1.000000e+03   \nmean   1.776357e-18      1.065814e-17       3.552714e-18  7.105427e-18   \nstd    1.000000e+00      1.000000e+00       1.000000e+00  1.000000e+00   \nmin   -6.435974e+00     -7.735193e+00      -7.817805e+00 -8.118688e+00   \n25%   -4.381664e-01     -3.243540e-01      -3.263356e-01 -2.815156e-01   \n50%    2.559003e-02      4.188134e-02       4.807448e-02  3.512834e-02   \n75%    5.452574e-01      4.213216e-01       4.289761e-01  3.676461e-01   \nmax    6.316330e+00      7.090528e+00       7.788433e+00  7.684703e+00   \n\n           kast_dif  assists_per_rd_dif  flash_per_rd_dif  \\\ncount  1.000000e+03        1.000000e+03      1.000000e+03   \nmean  -1.065814e-17       -4.440892e-18      1.332268e-17   \nstd    1.000000e+00        1.000000e+00      1.000000e+00   \nmin   -8.099462e+00       -5.048495e+00     -6.284287e+00   \n25%   -2.678898e-01       -5.383833e-01     -5.687058e-01   \n50%    6.239436e-02        1.682523e-02      1.543661e-02   \n75%    3.678864e-01        5.776503e-01      5.976279e-01   \nmax    7.808019e+00        5.311427e+00      6.438761e+00   \n\n       first_kills_dif_dif  team_rank_dif  score_dif_dif  win_rate_dif  \\\ncount         1.000000e+03   1.000000e+03   1.000000e+03  1.000000e+03   \nmean          1.065814e-17  -7.105427e-18   3.552714e-17 -3.197442e-17   \nstd           1.000000e+00   1.000000e+00   1.000000e+00  1.000000e+00   \nmin          -4.760382e+00  -3.583056e+00  -4.217007e+00 -3.724805e+00   \n25%          -5.859129e-01  -4.851744e-01  -5.956345e-01 -6.036420e-01   \n50%          -2.020498e-02   1.221722e-02   2.430884e-02  3.373862e-03   \n75%           6.196910e-01   4.602681e-01   5.966732e-01  6.204561e-01   \nmax           4.460250e+00   4.320149e+00   3.464001e+00  4.228936e+00   \n\n       scaled_win_dif  scaled_rating_dif  scaled_score_dif_dif  \ncount    1.000000e+03       1.000000e+03          1.000000e+03  \nmean     3.552714e-18      -3.552714e-18         -2.309264e-17  \nstd      1.000000e+00       1.000000e+00          1.000000e+00  \nmin     -1.128112e+01      -5.958851e+00         -7.928005e+00  \n25%     -1.568888e-01      -2.703883e-01         -1.762917e-01  \n50%      7.291137e-02      -1.931123e-03          1.044269e-01  \n75%      2.741010e-01       2.377506e-01          3.057342e-01  \nmax      4.603757e+00       5.245539e+00          8.470654e+00  "
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame.describe()\n",
    "df_train.describe()\n",
    "\n",
    "# %matplotlib qt\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "# sns.set(font_scale=3)\n",
    "\n",
    "# plt.figure(figsize=(30,20));\n",
    "# ax = plt.axes()\n",
    "\n",
    "# df_train.hist(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prize_rating_dif</th>\n      <th>scaled_win_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>495.000000</td>\n      <td>495.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.227814</td>\n      <td>0.156660</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.972410</td>\n      <td>0.921393</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-4.294535</td>\n      <td>-6.860687</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.252996</td>\n      <td>-0.037509</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.177084</td>\n      <td>0.134580</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.584404</td>\n      <td>0.352129</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.114808</td>\n      <td>4.603757</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       prize_rating_dif  scaled_win_dif\ncount        495.000000      495.000000\nmean           0.227814        0.156660\nstd            0.972410        0.921393\nmin           -4.294535       -6.860687\n25%           -0.252996       -0.037509\n50%            0.177084        0.134580\n75%            0.584404        0.352129\nmax            4.114808        4.603757"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['t1_win']==1][['prize_rating_dif','scaled_win_dif']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prize_rating_dif</th>\n      <th>scaled_win_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>505.000000</td>\n      <td>505.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-0.223303</td>\n      <td>-0.153558</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.976928</td>\n      <td>1.049901</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-4.294535</td>\n      <td>-11.281118</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.638892</td>\n      <td>-0.259403</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.178711</td>\n      <td>0.002556</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.245448</td>\n      <td>0.183122</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.291147</td>\n      <td>3.242652</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       prize_rating_dif  scaled_win_dif\ncount        505.000000      505.000000\nmean          -0.223303       -0.153558\nstd            0.976928        1.049901\nmin           -4.294535      -11.281118\n25%           -0.638892       -0.259403\n50%           -0.178711        0.002556\n75%            0.245448        0.183122\nmax            4.291147        3.242652"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['t1_win']==0][['prize_rating_dif','scaled_win_dif']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x7ff1cb782f60>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = df_train.drop(['fav_win'],axis=1).corr()\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=1)\n",
    "plt.figure(figsize=(20,16));\n",
    "ax = plt.axes()\n",
    "\n",
    "sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70}, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t1_win</th>\n      <th>fav_win</th>\n      <th>fav_ind</th>\n      <th>map</th>\n      <th>score_dif</th>\n      <th>t1_rank</th>\n      <th>t2_rank</th>\n      <th>t_0_p_0_prize_rating</th>\n      <th>t_0_p_1_prize_rating</th>\n      <th>t_0_p_2_prize_rating</th>\n      <th>...</th>\n      <th>t_1_p_0_scaled_rating</th>\n      <th>t_1_p_1_scaled_rating</th>\n      <th>t_1_p_2_scaled_rating</th>\n      <th>t_1_p_3_scaled_rating</th>\n      <th>t_1_p_4_scaled_rating</th>\n      <th>t_1_p_0_scaled_score_dif</th>\n      <th>t_1_p_1_scaled_score_dif</th>\n      <th>t_1_p_2_scaled_score_dif</th>\n      <th>t_1_p_3_scaled_score_dif</th>\n      <th>t_1_p_4_scaled_score_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>96641</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>trn</td>\n      <td>-10</td>\n      <td>97</td>\n      <td>41</td>\n      <td>0.663651</td>\n      <td>0.629862</td>\n      <td>0.569225</td>\n      <td>...</td>\n      <td>0.043875</td>\n      <td>0.039537</td>\n      <td>0.034187</td>\n      <td>0.032402</td>\n      <td>0.029116</td>\n      <td>-0.098653</td>\n      <td>-0.102610</td>\n      <td>-0.102610</td>\n      <td>-0.102906</td>\n      <td>-0.102906</td>\n    </tr>\n    <tr>\n      <th>96639</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>inf</td>\n      <td>8</td>\n      <td>41</td>\n      <td>97</td>\n      <td>0.721413</td>\n      <td>0.709766</td>\n      <td>0.709069</td>\n      <td>...</td>\n      <td>0.097023</td>\n      <td>0.008072</td>\n      <td>0.007778</td>\n      <td>0.007525</td>\n      <td>0.006096</td>\n      <td>-0.016848</td>\n      <td>-0.021450</td>\n      <td>-0.021450</td>\n      <td>-0.022641</td>\n      <td>-0.820086</td>\n    </tr>\n    <tr>\n      <th>96638</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>mrg</td>\n      <td>8</td>\n      <td>35</td>\n      <td>48</td>\n      <td>0.818040</td>\n      <td>0.799803</td>\n      <td>0.799803</td>\n      <td>...</td>\n      <td>0.026508</td>\n      <td>0.022743</td>\n      <td>0.022248</td>\n      <td>0.021855</td>\n      <td>0.019667</td>\n      <td>-0.024231</td>\n      <td>-0.024263</td>\n      <td>-0.024263</td>\n      <td>-0.025765</td>\n      <td>-0.033227</td>\n    </tr>\n    <tr>\n      <th>96634</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>trn</td>\n      <td>-2</td>\n      <td>48</td>\n      <td>35</td>\n      <td>0.726537</td>\n      <td>0.726537</td>\n      <td>0.720326</td>\n      <td>...</td>\n      <td>0.063923</td>\n      <td>0.055480</td>\n      <td>0.054788</td>\n      <td>0.052385</td>\n      <td>0.028128</td>\n      <td>-0.077340</td>\n      <td>-0.136681</td>\n      <td>-0.136681</td>\n      <td>-0.136681</td>\n      <td>-0.139161</td>\n    </tr>\n    <tr>\n      <th>96624</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>nuke</td>\n      <td>-3</td>\n      <td>84</td>\n      <td>35</td>\n      <td>0.699892</td>\n      <td>0.697520</td>\n      <td>0.697520</td>\n      <td>...</td>\n      <td>0.063923</td>\n      <td>0.055480</td>\n      <td>0.054788</td>\n      <td>0.052385</td>\n      <td>0.028128</td>\n      <td>-0.077340</td>\n      <td>-0.136681</td>\n      <td>-0.136681</td>\n      <td>-0.136681</td>\n      <td>-0.139161</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 167 columns</p>\n</div>",
      "text/plain": "       t1_win  fav_win  fav_ind   map  score_dif  t1_rank  t2_rank  \\\n96641       0        1        2   trn        -10       97       41   \n96639       1        1        1   inf          8       41       97   \n96638       1        1        1   mrg          8       35       48   \n96634       0        1        2   trn         -2       48       35   \n96624       0        1        2  nuke         -3       84       35   \n\n       t_0_p_0_prize_rating  t_0_p_1_prize_rating  t_0_p_2_prize_rating  ...  \\\n96641              0.663651              0.629862              0.569225  ...   \n96639              0.721413              0.709766              0.709069  ...   \n96638              0.818040              0.799803              0.799803  ...   \n96634              0.726537              0.726537              0.720326  ...   \n96624              0.699892              0.697520              0.697520  ...   \n\n       t_1_p_0_scaled_rating  t_1_p_1_scaled_rating  t_1_p_2_scaled_rating  \\\n96641               0.043875               0.039537               0.034187   \n96639               0.097023               0.008072               0.007778   \n96638               0.026508               0.022743               0.022248   \n96634               0.063923               0.055480               0.054788   \n96624               0.063923               0.055480               0.054788   \n\n       t_1_p_3_scaled_rating  t_1_p_4_scaled_rating  t_1_p_0_scaled_score_dif  \\\n96641               0.032402               0.029116                 -0.098653   \n96639               0.007525               0.006096                 -0.016848   \n96638               0.021855               0.019667                 -0.024231   \n96634               0.052385               0.028128                 -0.077340   \n96624               0.052385               0.028128                 -0.077340   \n\n       t_1_p_1_scaled_score_dif  t_1_p_2_scaled_score_dif  \\\n96641                 -0.102610                 -0.102610   \n96639                 -0.021450                 -0.021450   \n96638                 -0.024263                 -0.024263   \n96634                 -0.136681                 -0.136681   \n96624                 -0.136681                 -0.136681   \n\n       t_1_p_3_scaled_score_dif  t_1_p_4_scaled_score_dif  \n96641                 -0.102906                 -0.102906  \n96639                 -0.022641                 -0.820086  \n96638                 -0.025765                 -0.033227  \n96634                 -0.136681                 -0.139161  \n96624                 -0.136681                 -0.139161  \n\n[5 rows x 167 columns]"
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ct_start = pd.DataFrame.from_dict( map_training_dict,orient='index')\n",
    "len( df_ct_start )\n",
    "\n",
    "# type(data['roundHistory'][0]['ctTeam'])\n",
    "df_ct_start.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.49171270718232046\n0.574\n"
    }
   ],
   "source": [
    "df_ct_start.tail(n=20)\n",
    "print(df_ct_start[df_ct_start['map']=='inf']['t1_win'].mean())\n",
    "#df_ct_start['t1_rank'].max()\n",
    "print(df_ct_start['fav_win'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fav_win</th>\n      <th>tier_1</th>\n      <th>tier_2</th>\n      <th>team_rank_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>95444</th>\n      <td>1</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>-0.444945</td>\n    </tr>\n    <tr>\n      <th>95448</th>\n      <td>0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>0.582442</td>\n    </tr>\n    <tr>\n      <th>95451</th>\n      <td>0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>-0.336810</td>\n    </tr>\n    <tr>\n      <th>95454</th>\n      <td>1</td>\n      <td>9.0</td>\n      <td>12.0</td>\n      <td>0.041625</td>\n    </tr>\n    <tr>\n      <th>95455</th>\n      <td>1</td>\n      <td>12.0</td>\n      <td>9.0</td>\n      <td>0.209711</td>\n    </tr>\n    <tr>\n      <th>95457</th>\n      <td>0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>0.158154</td>\n    </tr>\n    <tr>\n      <th>95458</th>\n      <td>0</td>\n      <td>10.0</td>\n      <td>9.0</td>\n      <td>0.949587</td>\n    </tr>\n    <tr>\n      <th>95463</th>\n      <td>0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>-0.318071</td>\n    </tr>\n    <tr>\n      <th>95466</th>\n      <td>0</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>-0.996637</td>\n    </tr>\n    <tr>\n      <th>95488</th>\n      <td>0</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>-1.530299</td>\n    </tr>\n    <tr>\n      <th>95489</th>\n      <td>1</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>-0.785422</td>\n    </tr>\n    <tr>\n      <th>95490</th>\n      <td>1</td>\n      <td>8.0</td>\n      <td>6.0</td>\n      <td>1.384378</td>\n    </tr>\n    <tr>\n      <th>95491</th>\n      <td>1</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>-0.203632</td>\n    </tr>\n    <tr>\n      <th>95492</th>\n      <td>1</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>-1.184397</td>\n    </tr>\n    <tr>\n      <th>95493</th>\n      <td>1</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>-1.045390</td>\n    </tr>\n    <tr>\n      <th>95494</th>\n      <td>0</td>\n      <td>8.0</td>\n      <td>5.0</td>\n      <td>-0.277107</td>\n    </tr>\n    <tr>\n      <th>95495</th>\n      <td>0</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>-1.989885</td>\n    </tr>\n    <tr>\n      <th>95496</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>1.006644</td>\n    </tr>\n    <tr>\n      <th>95507</th>\n      <td>1</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>-0.333311</td>\n    </tr>\n    <tr>\n      <th>95509</th>\n      <td>0</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>0.569866</td>\n    </tr>\n    <tr>\n      <th>95512</th>\n      <td>1</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>-0.315236</td>\n    </tr>\n    <tr>\n      <th>95514</th>\n      <td>1</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>-1.777143</td>\n    </tr>\n    <tr>\n      <th>95516</th>\n      <td>0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>0.433163</td>\n    </tr>\n    <tr>\n      <th>95520</th>\n      <td>0</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>-0.087839</td>\n    </tr>\n    <tr>\n      <th>95521</th>\n      <td>1</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>-0.261706</td>\n    </tr>\n    <tr>\n      <th>95525</th>\n      <td>0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>0.152257</td>\n    </tr>\n    <tr>\n      <th>95529</th>\n      <td>1</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>0.086812</td>\n    </tr>\n    <tr>\n      <th>95531</th>\n      <td>1</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>0.122923</td>\n    </tr>\n    <tr>\n      <th>95534</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.053074</td>\n    </tr>\n    <tr>\n      <th>95535</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.037637</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       fav_win  tier_1  tier_2  team_rank_dif\n95444        1     6.0     7.0      -0.444945\n95448        0     5.0     3.0       0.582442\n95451        0     3.0     5.0      -0.336810\n95454        1     9.0    12.0       0.041625\n95455        1    12.0     9.0       0.209711\n95457        0     6.0     3.0       0.158154\n95458        0    10.0     9.0       0.949587\n95463        0     6.0     3.0      -0.318071\n95466        0     9.0    10.0      -0.996637\n95488        0     6.0     8.0      -1.530299\n95489        1     6.0     8.0      -0.785422\n95490        1     8.0     6.0       1.384378\n95491        1     5.0     8.0      -0.203632\n95492        1     9.0    15.0      -1.184397\n95493        1     3.0     2.0      -1.045390\n95494        0     8.0     5.0      -0.277107\n95495        0     9.0    15.0      -1.989885\n95496        1     2.0     3.0       1.006644\n95507        1     5.0     8.0      -0.333311\n95509        0    15.0     9.0       0.569866\n95512        1     6.0     5.0      -0.315236\n95514        1     9.0    15.0      -1.777143\n95516        0     5.0     6.0       0.433163\n95520        0     9.0    15.0      -0.087839\n95521        1     6.0     5.0      -0.261706\n95525        0     5.0     3.0       0.152257\n95529        1     3.0     5.0       0.086812\n95531        1     5.0     3.0       0.122923\n95534        1     2.0     1.0       0.053074\n95535        0     1.0     2.0       0.037637"
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['fav_win','tier_1','tier_2','team_rank_dif']].head(n=30)\n",
    "# X = df_train.iloc[:, 2:15]\n",
    "# np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array(['fav_win', 't1_win', 'fav_ind', 'prize_rating_dif', 'rating_dif',\n       'hs_perc_dif', 'kills_per_rd_dif', 'deaths_per_rd_dif', 'adr_dif',\n       'kast_dif', 'assists_per_rd_dif', 'flash_per_rd_dif',\n       'first_kills_dif_dif', 'team_rank_dif', 'score_dif_dif',\n       'win_rate_dif', 'scaled_win_dif', 'scaled_rating_dif',\n       'scaled_score_dif_dif', 'win_rate_map_dif', 'kd_per_round_dif',\n       'scaled_kd_dif', 'momentum_dif', 'map_rating_dif', 'tier_1',\n       'tier_2', 'fav_ind_2', 'fav_ind_3'], dtype=object)"
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.keys().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6846\nAUC Score (Train): 0.756250\n\n>              precision    recall  f1-score   support\n\n     T Start       0.62      0.63      0.62      1404\n    CT Start       0.62      0.62      0.62      1403\n\n   micro avg       0.62      0.62      0.62      2807\n   macro avg       0.62      0.62      0.62      2807\nweighted avg       0.62      0.62      0.62      2807\n\nAccuracy (Test): 0.622\nAUC Score (Test): 0.622014\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6827\nAUC Score (Train): 0.754151\n\n>              precision    recall  f1-score   support\n\n     T Start       0.61      0.63      0.62      1346\n    CT Start       0.65      0.63      0.64      1461\n\n   micro avg       0.63      0.63      0.63      2807\n   macro avg       0.63      0.63      0.63      2807\nweighted avg       0.63      0.63      0.63      2807\n\nAccuracy (Test): 0.6323\nAUC Score (Test): 0.632344\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6843\nAUC Score (Train): 0.753894\n\n>              precision    recall  f1-score   support\n\n     T Start       0.63      0.62      0.62      1391\n    CT Start       0.63      0.63      0.63      1416\n\n   micro avg       0.63      0.63      0.63      2807\n   macro avg       0.63      0.63      0.63      2807\nweighted avg       0.63      0.63      0.63      2807\n\nAccuracy (Test): 0.6274\nAUC Score (Test): 0.627293\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6848\nAUC Score (Train): 0.754643\n\n>              precision    recall  f1-score   support\n\n     T Start       0.64      0.64      0.64      1408\n    CT Start       0.64      0.63      0.64      1399\n\n   micro avg       0.64      0.64      0.64      2807\n   macro avg       0.64      0.64      0.64      2807\nweighted avg       0.64      0.64      0.64      2807\n\nAccuracy (Test): 0.6384\nAUC Score (Test): 0.638388\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6786\nAUC Score (Train): 0.753449\n\n>              precision    recall  f1-score   support\n\n     T Start       0.66      0.62      0.64      1439\n    CT Start       0.63      0.66      0.64      1368\n\n   micro avg       0.64      0.64      0.64      2807\n   macro avg       0.64      0.64      0.64      2807\nweighted avg       0.64      0.64      0.64      2807\n\nAccuracy (Test): 0.643\nAUC Score (Test): 0.643528\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6814\nAUC Score (Train): 0.754532\n\n>              precision    recall  f1-score   support\n\n     T Start       0.62      0.61      0.62      1395\n    CT Start       0.62      0.64      0.63      1412\n\n   micro avg       0.62      0.62      0.62      2807\n   macro avg       0.62      0.62      0.62      2807\nweighted avg       0.62      0.62      0.62      2807\n\nAccuracy (Test): 0.6245\nAUC Score (Test): 0.624440\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6813\nAUC Score (Train): 0.753987\n\n>              precision    recall  f1-score   support\n\n     T Start       0.63      0.63      0.63      1402\n    CT Start       0.63      0.64      0.63      1405\n\n   micro avg       0.63      0.63      0.63      2807\n   macro avg       0.63      0.63      0.63      2807\nweighted avg       0.63      0.63      0.63      2807\n\nAccuracy (Test): 0.6313\nAUC Score (Test): 0.631274\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6828\nAUC Score (Train): 0.755620\n\n>              precision    recall  f1-score   support\n\n     T Start       0.64      0.63      0.63      1410\n    CT Start       0.63      0.64      0.64      1397\n\n   micro avg       0.63      0.63      0.63      2807\n   macro avg       0.63      0.63      0.63      2807\nweighted avg       0.63      0.63      0.63      2807\n\nAccuracy (Test): 0.6345\nAUC Score (Test): 0.634517\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.685\nAUC Score (Train): 0.755330\n\n>              precision    recall  f1-score   support\n\n     T Start       0.63      0.62      0.63      1413\n    CT Start       0.62      0.62      0.62      1394\n\n   micro avg       0.62      0.62      0.62      2807\n   macro avg       0.62      0.62      0.62      2807\nweighted avg       0.62      0.62      0.62      2807\n\nAccuracy (Test): 0.6227\nAUC Score (Test): 0.622714\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6825\nAUC Score (Train): 0.754676\n\n>              precision    recall  f1-score   support\n\n     T Start       0.64      0.62      0.63      1416\n    CT Start       0.62      0.64      0.63      1390\n\n   micro avg       0.63      0.63      0.63      2806\n   macro avg       0.63      0.63      0.63      2806\nweighted avg       0.63      0.63      0.63      2806\n\nAccuracy (Test): 0.6304\nAUC Score (Test): 0.630505\n--------------------------------------------------------------\nAvg accuracy (Test) = 0.630660\n--------------------------------------------------------------\n"
    }
   ],
   "source": [
    "# Quick XGBoost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "#from sklearn.metrics import cross_validation   #Additional scklearn functions\n",
    "#from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import plot_roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# model = XGBClassifier(\n",
    "#     learning_rate =0.1,\n",
    "#     n_estimators=600,\n",
    "#     max_depth=5,\n",
    "#     min_child_weight=5,\n",
    "#     gamma=1,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     objective= 'binary:logistic',\n",
    "#     nthread=4,\n",
    "#     scale_pos_weight=1,\n",
    "#     seed=0)\n",
    "\n",
    "\n",
    "\n",
    "all_keys = ['fav_ind','prize_rating_dif', 'rating_dif', 'hs_perc_dif',\n",
    "       'kills_per_rd_dif', 'deaths_per_rd_dif', 'adr_dif', 'kast_dif',\n",
    "       'assists_per_rd_dif', 'flash_per_rd_dif', 'first_kills_dif_dif',\n",
    "       'team_rank_dif', 'score_dif_dif', 'win_rate_dif',\n",
    "       'scaled_win_dif','scaled_rating_dif','scaled_score_dif']\n",
    "\n",
    "features = ['t1_win','tier_1','tier_2','fav_ind','fav_ind_2','fav_ind_3','prize_rating_dif', 'rating_dif', 'deaths_per_rd_dif', \n",
    "       'assists_per_rd_dif', 'flash_per_rd_dif', 'first_kills_dif_dif',\n",
    "       'score_dif_dif', 'win_rate_dif','scaled_win_dif',\n",
    "       'scaled_rating_dif','scaled_score_dif_dif','win_rate_map_dif']\n",
    "\n",
    "\n",
    "features = ['t1_win', 'fav_ind', 'prize_rating_dif', 'rating_dif',\n",
    "       'hs_perc_dif', 'kills_per_rd_dif', 'deaths_per_rd_dif', 'adr_dif',\n",
    "       'kast_dif', 'assists_per_rd_dif', 'flash_per_rd_dif',\n",
    "       'first_kills_dif_dif', 'team_rank_dif', 'score_dif_dif',\n",
    "       'win_rate_dif', 'scaled_win_dif', 'scaled_rating_dif',\n",
    "       'scaled_score_dif_dif', 'win_rate_map_dif', 'kd_per_round_dif',\n",
    "       'scaled_kd_dif', 'tier_1', 'tier_2', 'fav_ind_2', 'fav_ind_3','momentum_dif','map_rating_dif']\n",
    "\n",
    "# These features are giving 63%\n",
    "features = ['t1_win', 'prize_rating_dif',  'score_dif_dif', \n",
    "        'win_rate_dif', 'scaled_win_dif', 'scaled_rating_dif','win_rate_map_dif', 'kd_per_round_dif',\n",
    "        'map_rating_dif', 'momentum_dif']\n",
    "\n",
    "\n",
    "\n",
    "# features = ['t1_win', 'fav_ind', 'prize_rating_dif', 'rating_dif',\n",
    "#        'hs_perc_dif', 'kills_per_rd_dif', 'deaths_per_rd_dif', 'adr_dif',\n",
    "#        'kast_dif', 'assists_per_rd_dif', 'flash_per_rd_dif',\n",
    "#        'first_kills_dif_dif', 'team_rank_dif', 'score_dif_dif',\n",
    "#        'win_rate_dif', 'scaled_win_dif', 'scaled_rating_dif',\n",
    "#        'scaled_score_dif_dif', 'win_rate_map_dif', 'kd_per_round_dif',\n",
    "#        'scaled_kd_dif', 'momentum_dif', 'map_rating_dif', 'tier_1',\n",
    "#        'tier_2', 'fav_ind_2', 'fav_ind_3']\n",
    "\n",
    "\n",
    "# features = ['t1_win','fav_ind','fav_ind_2']\n",
    "\n",
    "# features = ['t1_win','win_rate_dif','team_rank_dif']\n",
    "\n",
    "# df_tmp = df_train[ features ].head(n=10000).copy()\n",
    "\n",
    "df_tmp = df_train[ features ].copy()\n",
    "\n",
    "# Get rid of outliers\n",
    "df_tmp = df_tmp[ np.abs(df_tmp['win_rate_map_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['adr_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['assists_per_rd_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['deaths_per_rd_dif'])<3.0  ]\n",
    "df_tmp = df_tmp[ np.abs(df_tmp['kd_per_round_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['map_rating_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['scaled_win_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['scaled_score_dif_dif'])<3.0  ]\n",
    "\n",
    "\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['team_rank_dif'])<1.5]\n",
    "\n",
    "n = len(df_tmp.keys())\n",
    "X = df_tmp.iloc[:, 1:n].values\n",
    "y = df_tmp.iloc[:, 0].values\n",
    "\n",
    "# Scale all variables to [0,1]\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "\n",
    "\n",
    "model = XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=500,\n",
    "        max_depth=3,\n",
    "        min_child_weight=1,\n",
    "        gamma=10,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1 )\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200,max_depth=8, random_state=1, bootstrap=False,n_jobs=6)\n",
    "\n",
    "\n",
    "accuracy_vec = []\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "#for test_index, train_index in cv.split(X):\n",
    "    # print(\"Train Index: \", train_index, \"\\n\")\n",
    "    # print(\"Test Index: \", test_index)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X[train_index,:], X[test_index,:], y[train_index], y[test_index]\n",
    "    # best_svr.fit(X_train, y_train)\n",
    "    # scores.append(best_svr.score(X_test, y_test))\n",
    "    \n",
    "#     model.fit(X_train,y_train) \n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_train_pred = model.predict(X_train)\n",
    "#     y_train_proba = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_train_proba = clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('XGBoost')\n",
    "    print('')\n",
    "    print('Accuracy (Train): %.4g' % accuracy_score(y_train, y_train_pred) )\n",
    "    print('AUC Score (Train): %f' % roc_auc_score(y_train, y_train_proba) )\n",
    "    print('')\n",
    "    print('>'+classification_report(y_test,predictions,target_names=['T Start','CT Start']))\n",
    "\n",
    "    print('Accuracy (Test): %.4g' % accuracy_score(y_test, y_pred) )\n",
    "    print('AUC Score (Test): %f' % roc_auc_score(y_test, y_pred) )\n",
    "\n",
    "    accuracy_vec.append( accuracy_score(y_test, y_pred) )\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------')\n",
    "print('Avg accuracy (Test) = %f' % np.mean(accuracy_vec) )\n",
    "print('--------------------------------------------------------------')\n",
    "\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "#     sns.set(font_scale=3)\n",
    "\n",
    "#     plt.figure(figsize=(10,8));\n",
    "#     ax = plt.axes()\n",
    "\n",
    "#     metrics.plot_roc_curve(model, X_test, y_test, ax=ax, lw=6)  # doctest: +SKIP\n",
    "\n",
    "#     ax.plot([0, 1], [0, 1], linestyle='--', lw=6, color='r',\n",
    "#             label='Chance', alpha=.8 )\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "order = np.argsort( clf.feature_importances_ )[::-1][:n]\n",
    "feat_new = [features[order[ind]+1] for ind in range(0,n-1)]\n",
    "\n",
    "df = pd.DataFrame( {'importance':np.array(clf.feature_importances_[order]), \n",
    "    'features':feat_new, 'std':std[order]} )\n",
    "\n",
    "\n",
    "dic_aux = {}\n",
    "dic_aux['feature'] = []\n",
    "dic_aux['importance'] = []\n",
    "count = -1\n",
    "for tree in clf.estimators_:\n",
    "    count = count+1\n",
    "    \n",
    "    for ind in range(0,n-1):\n",
    "        dic_aux['feature'].append( features[ind+1] )\n",
    "        dic_aux['importance'].append( tree.feature_importances_[ind] )\n",
    "\n",
    "df = pd.DataFrame.from_dict(dic_aux,orient=\"columns\")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=3)\n",
    "\n",
    "plt.figure(figsize=(15,20));\n",
    "ax = plt.axes()\n",
    "ax = sns.barplot(x=\"importance\", y=\"feature\", data=df, order=feat_new, capsize=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/emmanuel/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3296: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared\n  exec(code_obj, self.user_global_ns, self.user_ns)\n-4.9992246385084815\n"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.figure(figsize=(20,20))\n",
    "ax = plt.axes()\n",
    "df_tmp.hist(ax=ax,bins=20)\n",
    "\n",
    "print(df_tmp['win_rate_map_dif'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x7ff1ce1f3438>"
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = df_tmp.drop(['t1_win'],axis=1).corr()\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=1)\n",
    "plt.figure(figsize=(20,16));\n",
    "ax = plt.axes()\n",
    "\n",
    "sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70}, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7ff1d21b9400>]"
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApsAAAJeCAYAAAD7t85jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd8VOeB7/+vegEJkBBC9P4ApldjjDEG2xg3XIjtOImTdXpys8lucvPb3+7d9e793b1319m7u9nEm+I48abYjrsNBtwwvdiYXh5AoAIINYR6n/P7Q3I8YGSNpJk5Z2Y+79dLr5w5OmeerzNm+PqZOc+JcxxHAAAAQCjEux0AAAAA0YuyCQAAgJChbAIAACBkKJsAAAAIGcomAAAAQoayCQAAgJChbAIAACBkKJsAAAAIGcomAAAAQoayCQAAgJChbAIAACBkEt0OAKVImi+pRFK7y1kAAAA+TYKkPEnvS2oO5ATKpvvmS9rqdggAAIAeWCJpWyAHUjbdVyJJVVX18vmckA2Snd1flZV1IXt+eBevfezitY9NvO6xKxyvfXx8nAYN6id19pdAUDbd1y5JPp8T0rL50RiITbz2sYvXPjbxuseuML72AX/1jwuEAAAAEDKUTQAAAIQMZRMAAAAhQ9kEAABAyFA2AQAAEDKUTQAAAIQMZRMAAAAhQ9kEAABAyFA2AQAAEDKUTQAAAIQMZRMAAAAhQ9kEAABAyFA2AQAAEDKUTQAAAIQMZRMAAAAhQ9kEAABAyFA2AQAAEDKUTQAAAIQMZRMAAAAhQ9kEAABAyCS6HQAAAAC95ziOzlXUq7HdUVpCnNtxPoGyCQAAEGGaWtp0rKBKB09X6tDpSl2sada4YQP0N1+Y63a0T6BsAgAARICKS43af6pCB05V6HjRJbX7HKUmJ2jqmCzdtThbNy0crbamVrdjfgJlEwAAwIN8jqMzJTXaf7KjYJ4tr5ckDc1K14p5IzRj/GBNHDFAiQkdl+AMykhVOWUTAAAAXWluadfRgosdM5j5laqpb1F8XJwmjhigzyyboFkTB2toVrrbMXuEsgkAAOCi+qZW7T9Zob22XEcKLqq1zae0lARNG5utWRMHa/q4bPVPS3I7Zq9RNgEAAMKspqFF+09W6IPjZTpWWKV2n6OszBTdMHOYZk0cLDNy4J8+Ho90lE0AAIAwuFTXrA9PlOuD42WyxZfkOFLOwFTdMn+k5pohGpuXobg47y1d1FeUTQAAgBC5VNes94+V6QNbplNnq+VIystO1+2LxmieydHIIf2jsmD6o2wCAAAEUX1Tq/bacu0+WqrjhVVyJI0c0l93LxmruWaIhg/u53bEsKJsAgAA9FFza7sOnKrQriOlOnS6Uu0+R7lZ6bpz8RgtnJqrvOzYKpj+KJsAAAC90O7z6fDpi9p9tFT7TlaoubVdgzJStGLeCC2cmqvRudH5HcyeomwCAAD0wNmyOm0/XKKdR0pVU9+ifqmJuvaaXF07NVcTRw5UPAXzMpRNAACAbtQ2tGj30VJtP3RBhaW1SoiP08wJg7V42lBNH58dNcsUhQJlEwAA4Cra2n06dLpS2w9d0IFTFWr3ORqdm6HPrpiohVNzlZGe7HbEiEDZBAAA8FNa1aAt+89r+6ES1TS0KjM9ScvnjtDi6XkaOaS/2/EiDmUTAADEvLZ2n/adrNB7+87pWGGV4uPiNHNCtpbMHKZpY7P4mLwPKJsAACBmlV1q1Ob957T9YMcsZnZmiu5ZMlbXzximQRkpbseLCpRNAAAQU9rafdp/skKb95/TkYIqxcVJsyYM1tJZwzRtbLbi47maPJgomwAAICbUNLRoy/7z2rTvnKpqm5WVmaLV14/VkpnMYoYSZRMAAES1otJavf3BWe06Wqq2dp+uGTNIn7tlkmaOH8wsZhhQNgEAQNRp9/m070SF3v6gWCfOVis5KV5LZuTpprkjYu7e5G6jbAIAgKjR2NymzfvP6+29xbpY06zBA1L1wE0TdP2MPPVLTXI7XkyibAIAgIhXVdustz8o1nv7z6mxuV2TRw3UwysmaeYEPip3G2UTAABErHMV9dq4u0g7j1yQz3E0f/IQrVw4SmOGZrodDZ0omwAAIKI4jqOTZ6u1flehDuRXKjkxXktnDdMtC0ZpyMA0t+PhCpRNAAAQERzH0ZEzF/Xa9gKdOlet/mlJuvv6sbppznDuU+5hlE0AAOBpjuPoYH6lXtteoDMlNcrKTNHDN0/S9TPylJKU4HY8dIOyCQAAPMlxHO0/WaHXtheosLRWgwek6pGVRoun53Gv8gji6bJpjEmWNFPSAkkLO/93kqSPLiv7e2vtYyEcP17SfZIe7syRJ6la0hlJr0j6tbW2NFTjAwAQi3yOow9tuV7fUaDisjoNGZimL62arEXXDKVkRiDPlk1jzBck/UKSK/ePMsaMkPR7STdc8ashnT8LJX3fGPOotfbVcOcDACDaOI6j/acq9PKW0zpbXq+hWen6yh1TtWDqECXEUzIjlWfLpqQsuVc0B0p6U9IUv907JR2TlC1puaT+ndsvGmNWWWvfDHtQAACixLGCi3pxy2mdPl+j3EFp+updU7Vgci5rZEYBL5fNjxRL2u3380+SFoV4zCf0cdGskHSPtXbbR780xgyS9AdJKyUlSHrOGDPeWnsxxLkAAIgq+eer9dLm0zpWWKWszBR98bbJWjx9KDOZUcTLZfMFSc9Za0v8dxpjWkI5qDFmhqQH/XY96F80JclaW2WMuVfSAUkTJQ2U9MPOHwAA0I1z5XV6actp7TtZoYz0JD20fKJunD1MSYlcXR5tPFs2rbVnXRr6G/r4AqQN1tp3rnaQtbbRGPO3kp7p3PUVY8xfW2vbwhESAIBIVF3XrJe3ntbWgyVKTU7QPUvGasW8kUpL8WwlQR/xyvoxxsRJustv12+6OeVlSbWSMiQNknSjpLdDkQ0AgEjW3NqujXuKtH5XkdrafVoxd6TuXDxG/dOS3I6GEKNsXm6SpGF+jzd/2sHW2mZjzE5Jt3TuWibKJgAAf+JzHO08fEEvbTmtqtpmzTU5uv/G8codlO52NIQJZfNy/lefl1hrLwRwzof6uGxO+bQDAQCIJccKq/TcuydVVFqnsXmZ+tpd12jSyIFux0KYUTYvN8lvuyjAc4r9tk0QswAAEJEqq5v03KZT+uB4mbIzUzuWMZqSq/g4ljGKRZTNy2X7bQd6ZyD/2c+sIGYBACCitLb5tHFPkdbuLJAc6Z4lY7Vy4SiuMI9xlM3L9ffbbgzwHP/j+nd5FAAAUexgfoX+8PZJlVU1aq7J0QM3TdDgAWlux4IHUDYvl+q3Heh6ns1+273+U5WdHfqempOTEfIx4E289rGL1z42hfN1L6mo15OvHtaeoxc0PKe//uGrizTbDAnb+LicF//MUzYv1+S3nRzgOf631Ax0NvQTKivr5PM5vT29Wzk5GSovrw3Z88O7eO1jF699bArX697W7tOG3UV6bXuBEhLitGbZeN08b6QSE+L5984l4Xjt4+PjejxBRtm8XJ3fdqCzlP7H1XV5FAAAUSL/XLV+s+G4zpXXa57J0UMrJmlQRkr3JyImUTYv539v89wAz/E/jnujAwCiVmNzm17cnK9NH57TwIwUfee+GZo1cbDbseBxlM3LWb/tUQGe43+c7fIoAAAi2L4T5frdWyd0qbZZy+eO0D03jOMWkwgI/5Zc7pjfdp4xJtda290SSLO7OB8AgIhXXdes3711QnttuUbk9Ne37pmuccMy3Y6FCELZvNwJSef18S0rl0r6Y1cHG2OSJS3y27UpdNEAAAgfx3G051iZfvemVXOrT/ctHadbF4xSYkK829EQYfg3xo+11pH0mt+uR7o5ZbWkj/7z7pKk90IQCwCAsKppaNETrxzWz187otysdP39n83X7YvGUDTRK8xsftLPJH1NUpyk24wxy6y1n5ixNMakSvoHv12/sNa2hSkjAAAh8cHxMv32TavG5jbdf+N43bpgpBLiKZnovZj5t8cY8xtjjNP5815Xx1lrD0h6tvNhnKTnjDGLr3iugZJe1Mf3Qr8k6Z+CnxoAgPCoa2zVz149rCdeOayszFT93Rfna9W1oyma6DNPz2waY/ZfZfcEv+2vG2NWX/H789baVX0c+pvquPBnsqQcSduMMTskHVfH/c9X6ONbU7ZLetBay7JHAICIdKzgop5cd0w19S1avWSsVl07mo/METSeLpuSZnbz+1x9cj3MgX0d1Fp7yRhzi6TfS1rSufu6zh9/lZIetdZu7OuYAACEW1u7Ty9vOa0Nu4s0JCtdf/2FuRozlCvNEVxeL5uusdYWG2NulHS/pIfVUXyHSqqRVCDpZUlPBbA0EgAAnlNSWa9fvHZUhaW1WjprmB68aaJSkhPcjoUo5Omyaa2NC+JzfVHSF3t4jk8dSx91ufwRAACRxHEcbT5wXs++fVLJSQn69r3TNWdSjtuxEMU8XTYBAEDwNDS16Tfrj+kDW66pYwbp0dunck9zhBxlEwCAGFB4oVb/+cphVVQ3ac2N43XrwlGKjwvaB4hAlyibAABEMcdx9N7+83rm7ZPKSE/SDx+erYkj+nwtLRAwyiYAAFGqsblN/7XRavfRUk0bm6Uv3zlVmenJbsdCjKFsAgAQhc6W1+mnLx9WWVWD7r1hnFYtGs3H5nAFZRMAgCjz/vEyPbXumFKTE/SDB2dr8uhBbkdCDKNsAgAQJXw+R0+vO6oX3j2p8cMz9c3V07naHK6jbAIAEAXqm1r189eO6PDpi1o6a5g+u2KSkhK55STcR9kEACDCnS2v009ePKTKmiZ96/6Zmjsh2+1IwJ9QNgEAiGB7bZmeXNvx/cwffnaOFs0eofLyWrdjAX9C2QQAIAI5jqM3dhXqxc2nNW5Ypr51D9/PhDdRNgEAiDBt7T49veG4th+6oAVThujR26coKTHB7VjAVVE2AQCIIHWNrfrpS4dkiy/prsVjdPf1YxXH+pnwMMomAAARovRig/7t+QOqrGnSV+6cqkXXDHU7EtAtyiYAABHAFlXpJy8dUlxcnL7/4GxNGsn9zREZKJsAAHjc+8fL9MvXjyhnYJr+fM1MDRmY5nYkIGCUTQAAPOydvWf1h7dOaPyIAfrOfTPUPy3J7UhAj1A2AQDwIMdx9NKW01q3s1CzJgzW1+++RslJXHGOyEPZBADAY9p9Pj293mrboRLdMHOYPn/rJCXEc+tJRCbKJgAAHtLc2q7/fOWwDuZXsrQRogJlEwAAj2hoatW/Pn9Ap8/V6PO3Gi2bPdztSECfUTYBAPCA2oYW/ctz+3WuvF7fWD1N8yYPcTsSEBSUTQAAXHaprlk/ena/yi816jv3z9D0cdluRwKChrIJAICLKqub9Piz+1Rd16LvrZmpyaMHuR0JCCrKJgAALimtatCPntmnhuZ2ff/BWRo/fIDbkYCgo2wCAOCCcxX1+tGz+9Te7ui/PzRbo4dmuB0JCAnKJgAAYXa2vE6PP7NP8fFx+uHDczR8cD+3IwEhQ9kEACCMzlfU60fP7FNCfJx++Nk5ys1KdzsSEFLcjgAAgDApqazX48/sU1xcnH7w0GyKJmICZRMAgDAovdigf35mnxzH0Q8emq28bD46R2ygbAIAEGJlVR1Fs729o2gO4zuaiCGUTQAAQqj8UqP++Zl9amlt1w8emq3hOf3djgSEFWUTAIAQuVjTpMef2afmlnZ9/8HZGjmEoonYQ9kEACAEPrrXeV1jq/7igVmso4mYRdkEACDIGpvb9G/PH1D5pSb9+f0zNDYv0+1IgGsomwAABFFrW7t+8tIhFV6o0zdWXyMzinudI7ZRNgEACJJ2n08/f+2ojhVW6c9un6zZE3PcjgS4jrIJAEAQOI6jpzdYfXiiXA8tn6jrpuW5HQnwBMomAAB95DiOnt+Ur20HS3TX4jG6ef5ItyMBnkHZBACgj958v1gb9hTppjnDdff1Y92OA3gKZRMAgD54/3iZnnv3lOaaHH325kmKi4tzOxLgKZRNAAB66UTxJf3y9aOaMHyAvnLHVMVTNIFPoGwCANALJZX1+o8XDyo7M0XfuX+GkpMS3I4EeBJlEwCAHqqub9G//vGA4uPj9L3PzFT/tCS3IwGeRdkEAKAHmlva9e/PH1BNfYv+/P6ZGjIo3e1IgKdRNgEACJDPcfTk2qMqLK3V1+6+RuOGcRtKoDuUTQAAAvTK1jPae6Jcn1k2gbsDAQGibAIAEIBdRy5o7Y4CLZmRp1tYtB0IGGUTAIBu5J+v1lNvHNekkQP1+VsNa2kCPUDZBADgU1ysadJ/vHhIgzKS9a17pikxgb86gZ7gTwwAAF1obmnXj184qNa2dn3n/pnKSE92OxIQcSibAABcheM4euqNYyour9PX7pqm4YP7uR0JiEiUTQAArmLjnmK9f7xM9y0drxnjs92OA0QsyiYAAFc4UnBRz793SvNMjm5bOMrtOEBEo2wCAOCn4lKjfv7qEQ3L7qc/u30KV54DfUTZBACgU0tru37y8iG1+xx9+97pSk1OdDsSEPEomwAAqOOCoKc3HFdxaZ2+eudU5WZxz3MgGCibAABIevfDc9p5pFR3LxmrmRMGux0HiBqUTQBAzDtTUqNn3zmpGeOzdcd1Y9yOA0QVyiYAIKbVN7XqP185rIH9k/XlO6YqnguCgKCibAIAYpbjOPrV2mOqqm3W11dPU/+0JLcjAVGHsgkAiFkb9xRr/6kKfWbZBI0fNsDtOEBUomwCAGLSybOX9MJ7+Zo7KUcr5o1wOw4QtSibAICYU9PQop+9ekTZA1L0pVUs3A6EEmUTABBTHMfRU+uOqbahRd9cPV3pqSzcDoQSZRMAEFPe3ntWB/Mr9ZllEzR6aIbbcYCoR9kEAMSM4rI6Pb/plGaOz9byuXxPEwgHyiYAICY0t7brZ68eVr+0JH3pdr6nCYQLZRMAEBOee+ekLlQ26Mt3TFVmerLbcYCYQdkEAES9vbZM7+0/r5ULR+maMVluxwFiCmUTABDVLtY06Tfrj2vM0Azdc8M4t+MAMYeyCQCIWj7H0ZNrj6rN5+hrd1+jxAT+2gPCjT91AICo9fYHZ3W86JIeWj5RuYPS3Y4DxCTKJgAgKp2rqNcL7+Vr1oTBWjIjz+04QMyibAIAok5bu09Prj2q1OQEPXLbZJY5AlxE2QQARJ21OwpUeKFWj6w0GtCPZY4AN1E2AQBR5fT5Gq3dUajrpg3VXDPE7ThAzKNsAgCiRnNru3659qgGZiTrsysmuR0HgKREtwMEwhiTKekRSfdLmigpS1KZpGOSnpX0B2ttc5DGGiPpTA9P+5W19svBGB8A0Hsvvpev0osN+sGDs5SeGhF/xQFRz/Mzm8aY6yUdkvRjSTdIypOUImmkpFskPSVpjzFmimshAQCuO1F8SW/vPavlc0doCncJAjzD0//ZZ4yZI2mDpH6du1okvSPpvKRxkpaqozDPkPSOMWaBtfZsECPUSvqvAI7bEcQxAQA91NLarl+/cUyDB6Tq/qXj3Y4DwI9ny6YxJlnSi/q4aO6VtNq/TBpjpkp6XR3FM0/S79VRQIPlorX220F8PgBACLyy9YxKqxr1/QdnKSU5we04APx4+WP0r0ka07ldKWnVlbOW1tqjku6U9NH3NW8wxtwWtoQAANfln6/WxveLtHTWME3l43PAc7xcNr/pt/24tbbsagd1Fs7fdHEeACCKtbb59Os3jmtg/xStuXGC23EAXIUny6YxZpKkyX67nu7mFP/frzDG9A9+KgCA17y+o0DnK+r1yMrJXH0OeJQny6akZX7b1lp7oZvj35dU37mdKmlRSFIBADyj8EKt3thZqMXThmrG+Gy34wDoglf/M9B/GaMPuzvYWttmjDkk6Vq/898KQo5EY8wtkuZJGiypQVKppN2S9lpr24MwBgCgh9raffr1G8eUkZ6kB5ZPdDsOgE/h1bLpf9uHogDPKdbHZdMEKcdwSRu7+N05Y8zjkn5C6QSA8Fq/u0hFZXX69r3T1T8tye04AD6FVz9G9/88pDTAc/w/ag/H5YjDJf2bpDeNMQPCMB4AQNKFiw16fXuB5k8eojmTctyOA6AbXi2b/hf4NAZ4jv9xfb1AqEbSryStkTRBHWt9pqpjPc8/k3TQ79ibJD1njGFhNwAIMcdx9NuNVkmJ8frsCj4+ByKBVz9GT/XbbgnwHP97o6f1YewSScOttXVX+d0ZSWeMMb+V9B+Svt65/1ZJn9flSzD1SHZ26C+gz8nJCPkY8CZe+9gVba/9ux8U61hhlb553wxNGDvY7TieFW2vOwLnxdfeq2WzyW87OcBzUvy2A50N/QRrbbMuL65XO6bNGPMtdXw39KMr53+gPpTNyso6+XxOb0/vVk5OhsrLa0P2/PAuXvvYFW2vfV1jq375yiGNH5apOROyo+qfLZii7XVH4MLx2sfHx/V4gsyrH6P7zyoGOkvpf9zVZiWDylrrk/Q//XZNNcaMCvW4ABCrnt90So3NbXpk5WTFx8W5HQdAgLxaNi/6becGeI7/cRe7PCq4tunyj/knd3UgAKD3bFGVth4s0S3zR2rEEO7bAUQSr5ZN67cd6Gyh/3G2y6OCyFrbqo77tn+EVYUBIMha23z6r41Wgwek6q7FY92OA6CHvFo2j/ltz+7u4M4rwad3cX6opftt13d5FACgVzbsLlRJZYM+d4tRSjILfwCRxqtlc5PftjHGdPdR+jx1LE8kdVzcszMkqa5gjBkjyX+NzZJwjAsAsaK0qkGv7yjU/MlDuCUlEKE8WTattSckHe98GCfpC92c8ojf9ttdLFsUCl/y266VtC9M4wJA1HMcR7/baJWUGKeHWFMTiFieLJudnvDb/oEx5qoLqhljJuvy0vfT3g5ojEk2xgS01JIxZr46ljv6yHPW2rbejg0AuNxeW64jBVW694bxGtg/pfsTAHiSl8vmzyUVdG7nSHrDGDPc/wBjzBRJa/XxIvBbrbXrr/ZkxpjHjDFO50/B1Y6RNEzSKWPMX1w5lt/zJBljvirpXX283FKtpH8I6J8KANCt5pZ2PfvuSY0c0l83zh7mdhwAfeDVRd1lrW0xxtwnaas6LsKZLynfGPOOOr4bOUYdC6p/VJgvSHo4CEOPlPQvkn5kjMmXdEQdV5z7JOVJuk7SIL/jWyTdb60tDsLYAABJ63YV6GJNs7565zVKiPfyvAiA7ni2bEqStfZDY8xKSb9Tx9JGKZJWXeXQQ5IeDHLhi1PHfdEnfMoxByU9Yq3dH8RxASCmlVY1aMPuIi26JleTRg50Ow6APvJ02ZQka+1WY8x0SV+UtEYd5S9LUrmko5KelfT7zttM9lWhpBnqmL28TtJUSYM7f1IkVUsqlrRL0svW2reCMCYAoJPjOHrm7ZNKTIjXmmWf9t/6ACKF58umJFlrayT9uPOnt8/xmKTHujnGUccs6SF1fGcUABBGB05V6mB+pR64aQIXBQFRgi/CAAA8obWtXX94+4TystO1fO4It+MACBLKJgDAE9bvKlJFdZMevnmSEhP46wmIFvxpBgC4ruJSo9btKtS8yUM0dUyW23EABBFlEwDgumffPaW4OOnBm7goCIg2lE0AgKuOFVzUhyfKdfuiMcrKTO3+BAARhbIJAHCNz+fomXdOKTszVSsXjHQ7DoAQoGwCAFyz9eB5nS2v05pl45WUmOB2HAAhQNkEALiioalNL205rQkjBmj+5CFuxwEQIpRNAIAr1u0sUG1Dqx5aPlFxcXFuxwEQIpRNAEDYlV1q1FsfFGvxtKEam5fpdhwAIUTZBACE3fObTik+Pk73Lh3vdhQAIUbZBACElS2q0l5brlXXjtagDO5/DkQ7yiYAIGw6ljo6qazMFN26YJTbcQCEAWUTABA22w+XqKi0TvffOF4pSSx1BMQCyiYAICyaWtr00ubTGj8sUwun5LodB0CYUDYBAGGxcU+xqutb9ABLHQExhbIJAAi56rpmbdhdpHkmRxOGD3A7DoAwomwCAELu1e0Famv36T6WOgJiDmUTABBSJZX12rL/vG6cNVy5WeluxwEQZpRNAEBIvfBevpKT4nXn9WPcjgLABZRNAEDInCi+pH0nK7Tq2tHKTE92Ow4AF1A2AQAh4TiOnt90SgP7J+vm+SPdjgPAJZRNAEBI7LXlyj9fo3uWjGMBdyCGUTYBAEHX1u7TC5vzNXxwPy2enud2HAAuomwCAIJu8/7zKqtq1Jpl4xUfzwLuQCyjbAIAgqqxuU2vbT+jyaMGavq4bLfjAHAZZRMAEFQb9xSptqFVa5ZN4LaUACibAIDgqWlo0cb3izXX5GhsXqbbcQB4AGUTABA0b+wsVEtru+5ZMs7tKAA8grIJAAiKizVNevfDc1o8LU/DBvdzOw4Aj6BsAgCC4tVtZyQ5uvv6sW5HAeAhlE0AQJ+VVNZr26ES3Th7uLIHpLodB4CHUDYBAH328tYzSk5M0B2LxrgdBYDHUDYBAH1SeKFWHxwv0y3zRyqzX7LbcQB4DGUTANAnL27OV7/URN26YJTbUQB4EGUTANBrtqhKh89c1O2Lxig9NdHtOAA8iLIJAOgVx3H0wuZ8DcpI0U1zhrsdB4BHUTYBAL1y4FSl8s/V6M7FY5SclOB2HAAeRdkEAPSYz3H00pZ8DRmUpuun57kdB4CHUTYBAD2251ipzpbX654l45SYwF8lALrGOwQAoEfafT69uq1AI3L6af6UIW7HAeBxlE0AQI/sOlKq0osNuvv6cYqPi3M7DgCPo2wCAALW1u7T69sLNCq3v+ZMGux2HAARgLIJAAjYjsMXVHapUauXjFMcs5oAAkDZBAAE5KNZzbF5mZo5PtvtOAAiBGUTABCQrQdLVFnTpHuWjGVWE0DAKJsAgG61trVr7Y4CTRg+QNeMzXI7DoAIQtkEAHRr8/7zqqpt1mpmNQH0EGUTAPCpWlrbtW5noczIgZoyepDbcQBEGMomAOBTvbfvnKrrW5jVBNArlE0AQJeaW9r1xq5CTR0zSGYUs5oAeo6yCQDo0rsfnlVNQ6tWLxnndhQAEYqyCQC4qsbmNq3fXaTp47I1YfgAt+MAiFCUTQDAVb2996zqGlu1eslYt6MAiGCUTQCRR8KlAAAgAElEQVTAJzQ0tWrj7iLNmjBYY/My3Y4DIIJRNgEAn/D2B2fV0Nymu69nVhNA31A2AQCXaWxu05vvF2v2xMEaPTTD7TgAIhxlEwBwmXf2dsxq3rWYWU0AfUfZBAD8SWNzmzbuKdLM8dnMagIICsomAOBP3tt3TvVNbbqTWU0AQULZBABI6rhb0IY9RZo2LkvjhnEFOoDgoGwCACRJ7+0/p9qGVr6rCSCoKJsAALW0tmv97iJNHTOIuwUBCCrKJgBAmw+cV019C7OaAIKOsgkAMa61rV3rdxVq8qiBmjRyoNtxAEQZyiYAxLitB0t0qa6FK9ABhARlEwBiWGubT+t2FmriiAGaPIpZTQDBR9kEgBi2/XCJqmqbddfisYqLi3M7DoAoRNkEgBjV1u7Tuh2FGj8sU1PHDHI7DoAoRdkEgBi18/AFVdY06U5mNQGEEGUTAGJQu8+ntTsLNGZohqaPy3I7DoAoRtkEgBi060ipyi818V1NACFH2QSAGOPzOVq7o0CjhvTXzAnZbscBEOUomwAQY/YcK1VpVSPf1QQQFpRNAIghPp+j13cUaEROP82eNNjtOABiAGUTAGLIB7ZMJZUNunPxWMUzqwkgDCibABAjfE7HrOawwf001+S4HQdAjKBsAkCM2HeiXOfK63XHdaOZ1QQQNoluBwiEMSZT0iOS7pc0UVKWpDJJxyQ9K+kP1trmEI19a+fY8yUNl9QgqUjSOkm/stYWhGJcAAgmx3H02vYC5Wala8HkXLfjAIghnp/ZNMZcL+mQpB9LukFSnqQUSSMl3SLpKUl7jDFTgjzuQGPMi5I2SHpI0gRJaZKyJc2W9DeSjhhjvhLMcQEgFPafqlBxWZ3uvG604uOZ1QQQPp4um8aYOeooe6M6d7VIWi/pV5I2SfJ17p8h6R1jzIggjZss6VVJ9/rtPiDpaUkvSKrs3Jcu6RfGmEeDMS4AhMJHs5pDBqZp4VRmNQGEl2c/Ru8sfC9K6te5a6+k1dbas37HTJX0uqRx6pjx/L2kpUEY/jF1zKJKHR+bf95a+5LfuOmSfiLpS527/tMYs81aa4MwNgAE1aHTlSq8UKsvrZqshHhPzzEAiEJeftf5mqQxnduVklb5F01JstYelXSnpI++r3mDMea2vgxqjMmV9Od+u/6bf9HsHLdB0qOStnTuSpL0D30ZFwBC4aNZzcEDUrXomqFuxwEQg7xcNr/pt/24tbbsagd1Fs7fdHFeb3xRHR+PS9Ixa+1TXYzrSPqh3677jTFD+jg2AATVkYKLOn2+RrcvGq3EBC+/5QOIVp585zHGTJI02W/X092c4v/7FcaY/n0Y/u5Ax7XW7pL00Ufn8ZLu6sO4ABBUjuPotW0FyspM0eLpeW7HARCjPFk2JS3z27bW2gvdHP++pPrO7VRJi3ozqDEmTdJCv12bAzjN/5hlXR4FAGF28FSFTp2r1u3XMqsJwD1efffxX8bow+4Otta2qWN5pKud3xNGH/9/4kjaH8A5/vmCuvwSAPTFs29ZDcpI0fUzhrkdBUAM82rZnOS3XRTgOcV+2yYI45ZZa5vCNC4ABJUtqtLh/ErdtnCUkhK9+lYPIBZ49R0o22+7NMBz/D9qz3Jp3HRjTEovxwaAoHlte4EGZaTohpnMagJwl1fLpv8FPo0BnuN/XG8vEOrruH0ZGwCC4uTZSzpWWKV7l01UclKC23EAxDivLuqe6rfdEuA5/vdGT3Np3F6PnZ0d+o6ak5MR8jHgTbz2seUnLx/WwP4pWrlotFKTvfo2j1Diz3zs8uJr79V3If/vSiYHeI7/x9eBzkoGe9xej11ZWSefz+nNqQHJyclQeXltyJ4f3sVrH1vyz1frQ1umNcvGKzU5kdc+BvFnPnaF47WPj4/r8QSZVz9Gr/PbDnSm0P+4ui6PCu24fRkbAPrs9e0F6p+WpGWzh7sdBQAkebdsXvTbzg3wHP/jLnZ5VGjHbbDWXvmxOgCERcGFGh3Mr9StC0by8TkAz/Bq2bR+26MCPMf/ONvlUYGPOyTAK8uDMS4A9Nnr2wvULzVRN80Z4XYUAPgTr5bNY37bs7s72BiTIGl6F+f3hJXk69yOkzQrgHP88/V2XADok6LSWu07WaGb549UWgqzmgC8w6tlc5PftjHGdPeR9jxJ/Tq3myXt7M2g1tpGSbv9di0N4DT/YzZ1eRQAhNDrOwqUlpKoFXOZ1QTgLZ4sm9baE5KOdz6Mk/SFbk55xG/7bWttXy7SebWL5/0EY8wCSZM7H/okvdaHcQGgV86W12mvLdfN80YoPTXJ7TgAcBlPls1OT/ht/8AYM/hqBxljJkv6kt+un/Zx3KclNXRuTzXGXLVwGmPiJP0fv10vWGvL+jg2APTY2h0FSk1O0Ip5I92OAgCf4OWy+XNJBZ3bOZLeMMZctpaHMWaKpLX6eDH2rdba9Vd7MmPMY8YYp/On4GrHSJK19oKkf/fb9VNjzOornitN0pOSlnXuapX0twH8MwFAUJ2vqNf7x8q0fO4I9U9jVhOA93j2W+TW2hZjzH2StkpKlzRfUr4x5h1JJZLGqKPsfVSYL0h6OEjDPyZpsaQb1PFd0JeNMfsl7e98vEyS/0zrN621XIkOIOzW7ixQclKCbpnPrCYAb/LyzKastR9KWimpqHNXiqRVkh6VtFwf5z8kabm1tjhI47ZIWi3pZb/dsyR9UdIafVw0GyR93Vr7ZDDGBYCeuHCxQbuPlmrZnOHKSA/0pmcAEF6eLpuSZK3dqo5ljf5c0jZ1zGC2SDon6S11FM/51tqjQR63ylp7r6TbJD0r6bQ6bmdZJemApH+UNM1a+/NgjgsAgXp9e4GSEuO1ckGgyxEDQPh59mN0f9baGkk/7vzp7XM8po6Px3t63gZJG3o7LgCEQmlVg3YdvaBb5o9UZj9mNQF4l+dnNgEAn7R2R4ESE5jVBOB9lE0AiDBlVQ3aebhUN84argH9A7mrLgC4h7IJABFm3c5CxcfHaeVCZjUBeB9lEwAiSMWlRu04fEFLZw3ToAxmNQF4H2UTACLIul2FiouTVl072u0oABAQyiYARIjK6iZtO1iiJTOZ1QQQOSibABAh1u0qlCTdzqwmgAhC2QSACHCxpklbD5zXkhl5yspMdTsOAASMsgkAEeCNzlnNVYuY1QQQWSibAOBxVbXN2nLgvBZPz9PgAWluxwGAHqFsAoDHrd9VKMeRbmdWE0AEomwCgIddqmvW5gPntWjaUOUMZFYTQOShbAKAh23YXaT2dkd3MKsJIEJRNgHAo6rrW/TevnNadE2uhgxKdzsOAPQKZRMAPGrj7iK1tvt0x3Vj3I4CAL1G2QQAD6qpb9G7+87q2qm5ys1iVhNA5KJsAoAHbdxTpNY2ZjUBRD7KJgB4TE1Di9798JwWTMlVXnY/t+MAQJ9QNgHAYzbsLlJLW7vuWjzG7SgA0GeUTQDwkOr6Fr2796yunTqUWU0AUYGyCQAesn5XodraHWY1AUQNyiYAeERVbbM27Tun66YN5Qp0AFGDsgkAHvHGrkL5fI7uYFYTQBShbAKAB1ysadLm/ee0ePpQDeEe6ACiCGUTADxg3c5COY50x6IxbkcBgKCibAKAyyqqG7XlwHktmTlMg5nVBBBlKJsA4LK1OwoVFyfdsWi021EAIOgomwDgorJLjdp+qERLZw5XVmaq23EAIOgomwDgorXbCxQXF6dVzGoCiFKUTQBwSWlVg3YcvqBls4drUEaK23EAICQomwDgkte3FygxIU6rrh3ldhQACBnKJgC4oKSyXjuPXNBNc0ZoQH9mNQFEL8omALjg9e0FSk5M0EpmNQFEOcomAITZ2fI67T5aqpvmDldmerLbcQAgpCibABBmL285rdSUBN22kCvQAUQ/yiYAhFH++WrtO1mhlQtGqX9akttxACDkKJsAEEYvbT6tjPQkrZg30u0oABAWlE0ACJOjBRd1rLBKty8ao7SURLfjAEBYUDYBIAwcx9GLm08rKzNFy2YPczsOAIQNZRMAwmD/yQqdKanRXYvHKikxwe04ABA2lE0ACDGfz9FLW04rNytdi6cPdTsOAIQVZRMAQmz30VKdq6jXPUvGKiGet10AsYV3PQAIobZ2n17ZdlqjhvTXvMlD3I4DAGFH2QSAENp64LzKLzXp3qXjFB8X53YcAAg7yiYAhEhza7te21GgiSMGaPq4bLfjAIArKJsAECLvfnhW1XUtum/peMUxqwkgRlE2ASAEGpra9MbOQk0bl6VJIwe6HQcAXEPZBIAQWL+7UPVNbbrvhvFuRwEAV1E2ASDILtY06c33i3Xt1FyNHprhdhwAcBVlEwCC7JVtZ+Q4ju65YZzbUQDAdZRNAAiic+V12n6oRDfNGaGcgWluxwEA11E2ASCIXngvX6nJibrjujFuRwEAT6BsAkCQ2KIqHciv1KprR6l/WpLbcQDAEyibABAEjuPoj5vyNSgjRTfPG+l2HADwDMomAATBB7ZcZ0pqtHrJWCUnJbgdBwA8g7IJAH3U1u7Ti5vzNTynnxZPy3M7DgB4CmUTAPpo8/7zKqtq1P1Lxys+nttSAoA/yiYA9EFjc5te235GZuRAzRif7XYcAPAcyiYA9MH63YWqbWjVmmUTFBfHrCYAXImyCQC9VFndpI17Om5LOW5YpttxAMCTKJsA0EsvbM6XJN23dLzLSQDAuyibANAL+eeqtftoqW5dMErZA1LdjgMAnkXZBIAechxHz757UgP6JWvVtaPcjgMAnkbZBIAe2nOsTPnnanTvDeOUmpzodhwA8DTKJgD0QEtru15475RGDemvxdNZwB0AukPZBIAeeOuDYlXWNOuB5RNZwB0AAkDZBIAAVdc1a+3OQs2eOFhTRg9yOw4ARATKJgAE6OWtZ9TW5tNnlk1wOwoARAzKJgAEoLisTlsPntfyuSOUm5XudhwAiBiUTQDohuM4+v1bJ9QvNUl3Lh7jdhwAiCiUTQDoxp5jZTpRfEn3Lh2nfqlJbscBgIhC2QSAT9HU0qY/bjql0bkZumHGMLfjAEDEoWwCwKdYu6NQVbXNeviWSSx1BAC9QNkEgC6UXmzQxj1Fum7aUE0YPsDtOAAQkSibANCFZ945qaTEeK25cbzbUQAgYlE2AeAq9p+q0MH8St21eKwG9E9xOw4ARCzKJgBcobWtXc++fVJ52elaMW+E23EAIKJRNgHgChv2FKvsUqM+u2KSEhN4mwSAvkh0O0AgjDHTJD0q6WZJIyQlSDonaYukX1trdwZ5vMck/V0PT1tird0WzBwAwu9iTZPW7SzQ3Ek5umZslttxACDiebpsGmPi1FH6/kYdBfOyX3f+fMUY84Sk71lrW8IcEUCUeeadk3Ic6YGbuP85AASDp8umpH+U9P/4PT4raZukNkkLJU3s3P9NSRmSvhCCDO9L2hPAcedDMDaAMDpwqkJ7bbnuvWGcBg9MczsOAEQFz5ZNY8wturxo/n+S/t5a29b5+zhJ35D0Y3XMen7eGLPJWvvrIEd5w1r7WJCfE4DHNLe263dvnlBedrpWLhzldhwAiBpe/ub7P/pt/9Za+z8+KpqSZK11rLVPSPrffsc9ZoxhjRIAPfba9jOqrGnSF241XBQEAEHkyXdUY8xCSXM7H7ZL+qtPOfwfJV3q3B4l6Y4QRgMQhc6W1+nNPcW6fnqezKhBbscBgKjiybIp6W6/7bestee6OtBa2yjpj3677glZKgBRx+c4+u1Gq7SURK1Zxp2CACDYvFo2l/ltbw7g+Pe6OBcAPtW2gyU6ebZaa5aNV0Z6sttxACDqePUCocl+2x8GcLz/McOMMQOstdVBypJrjPmCOq587yepStIZSVustUVBGgOAC2oaWvT8plOaNGKAFk/PczsOAEQlz5VNY8wQSQP9dgVS6IqveDxJHUsWBcPXO38+wRjzjqT/EexF5QGEx/ObTqmppV2fv9UoPi7O7TgAEJW8+DF69hWPS7s7wVrbIKnWb1e4bvuxXNJWY8x3wzQegCCxRVXafuiCVi4cpeE5/d2OAwBRy3Mzm5KufNdvDPC8RnUs7H615+iNo5KeV8f3QY+q4+PzdHXMmt4j6VuSMtWxxue/GmMqrbW/7e1g2dmh/8suJyej+4MQlXjtL9fc2q7fPrlbuVnp+uJd05Sa7MW3wuDgtY9NvO6xy4uvvRffYVOveBzoLSib/bb7euuPH3exkHu1Oj6ef98Y86Sk9eoon5L0Y2PMWmttVW8GrKysk8/n9CpsIHJyMlReXtv9gYg6vPaf9MJ7+TpfUa+/fHCWaqsbFa3/7/DaxyZe99gVjtc+Pj6uxxNkPSqbxphvS/p2j0bo3m3W2jN+j5uu+H3yVfZdjf9i7oHOhl6VtfZiAMecNsbcKemQOjIOlPRlSY/3ZWwAoVV4oVYbdhfp+hl5umZMuL5xAwCxq6czm4MlmSBnuPKOP3VXPE5TYGXTfzbzyucICWvtCWPMs/r4nuwrRdkEPKut3adfv3FMGelJeuCmCW7HAYCY4MULhK6cVczt7gRjTJo+/r7m1Z4jlN7x254SxnEB9NDGPUUqKqvT524x6pea5HYcAIgJPZrZ7Pwe42MhSfLxGKXGmEv6ePmjUZKOd3PaqCsenwh6sK5d8Nu+8kp6AB5RUlmvV7cVaJ7J0VyT43YcAIgZXpzZlC4vl7MDON7/mPNBXNA9EOl+2/VhHBdAgHyOo9+sP66UpHg9fPOk7k8AAASNV8vmJr/tpQEc73/Mpi6PCg3/olsS5rEBBGDTh+d08my1Hlw+UQP6X/k1cQBAKHm1bL7qt32zMabL+8gZY1IlPeC365WQpfrk2ImSHvbbtSVcYwMITFlVg55/75SuGZul66YNdTsOAMQcT5ZNa+1uSXs7HyZK+sdPOfyvJA3q3C6W9HpfxjbG9GTxqP8jabzf49/3ZWwAweVzHD217pgS4uP1pdsmK45bUgJA2HlxUfeP/L+SNnZuf9EYUyTpf1pr2yTJGBMn6WuS/trvnL+z1jarC8aYAkmjOx/+fRcLt/+lMeY6SU9I2nC15zPGjJD0vyV9zm/3K9babQH8cwEIk7ffL9aJs9V69PYpysq88n4RAIBw8GzZtNa+aYz5J0k/7Nz1t5K+ZIzZJqlN0kJ9fPceSfqdtfbXQRg6TtItnT+NxpiDkk5LqlHHWp6TJM3T5f/fHZD0SBDGBhAkJZX1emHzac2aMJiPzwHARZ4tm53+Sh23ofxrddyDfKSkh65y3M8kfTcE46epo9Qu7OL37ZKekvQ9ay1XogMe0e7z6cm1x5SSFK9HVho+PgcAF3m6bFprHUl/Z4x5QdKjkm6WNEIdxfOcpK2SfmWt3RnEYR/vfN7rJF2rjjU8B0vKUseMapWko5K2SXraWlsYxLEBBMH6XUU6U1Kjr999DVefA4DLPF02P2KtPaQgzFxaa8cEcEy9Ou4K9E53xwLwnuKyOr267YzmTx6iBVO6vQEZACDEPHk1OgD0RmubT0+uPap+qYn63C0s3g4AXkDZBBA1XtqSr+KyOn1p1RRlpCe7HQcAIMomgChxpOCiNu4p1rI5wzVzwmC34wAAOlE2AUS8usZW/WrtUeVlp+szyya4HQcA4IeyCSCiOY6jp9cfV21Dq7565zVKSUpwOxIAwA9lE0BE23awRHtPlOvepeM0emiG23EAAFegbAKIWKUXG/SHt09q8qiBunXBKLfjAACugrIJICK1tfv0i9ePKDEhTl++Y6riuUsQAHgSZRNARHrhvXydKanVIysnKysz1e04AIAuUDYBRJz9Jyv05vvFumnOcM2bPMTtOACAT0HZBBBRLtY06VfrjmpUbn89cBPLHAGA11E2AUSMtnaffvbqEbX5HH3j7mlKSmSZIwDwOsomgIjx6rYzOnWuWo+sNMrNSnc7DgAgAJRNABHh8OlKrdtZqBtm5unaqUPdjgMACBBlE4DnXaxp0i/XHtXwnH56aMUkt+MAAHqAsgnA01rbfHrilcNqafPpm6uncTtKAIgwlE0Anvbsuyd1+nyNHl01RXnZ/dyOAwDoIcomAM/acbhEmz48p5ULRrGeJgBEKMomAE8qKq3Vf22wmjxqoO67cZzbcQAAvUTZBOA5DU2teuLlw0pPTdTX7p6mhHjeqgAgUvEODsBTfI6jJ9ceU2VNk765eroG9Et2OxIAoA8omwA85ZWtZ7T/VIUeXD5RE0YMcDsOAKCPKJsAPGPPsVKt3VGgJTPydNOc4W7HAQAEAWUTgCcUXqjVU+uOacLwAfrcLUZxcXFuRwIABAFlE4Drqutb9B8vHVS/tCR9697pSkrkrQkAogXv6ABc1drm009fPqS6hlZ9574ZXBAEAFGGsgnANY7j6PdvWZ06W60/u32KRg/NcDsSACDIKJsAXLNxT7G2HCjR7YtGa8GUXLfjAABCgLIJwBUfHC/THzed0rzJQ3TPDdwhCACiFWUTQNjln6/WL9ce1fhhmfry7VMUz5XnABC1KJsAwqriUqP+44WDGtAvWf/tvhlKTkpwOxIAIIQomwDCpqGpVf/2wkG1tTv67pqZyuTKcwCIepRNAGHR1u7TE68cVunFBn3r3ukaNrif25EAAGFA2QQQcj7H0VNvHNPRgio9snKypowe5HYkAECYUDYBhNzzm05p15FS3XvDOF0/I8/tOACAMKJsAgipDbuLtHFPsZbPGaHbF412Ow4AIMwomwBCZsfhkj+tpfnQiomKY4kjAIg5lE0AIXHodKV+/cZxTRk9SF+5Y6ri4ymaABCLKJsAgi7/XLV++vIhDR/cT9++d7qSEnmrAYBYxd8AAIKqqLRW//ePBzSwX4q+95mZSktJdDsSAMBFlE0AQVNSWa9/eW6/0lIS9P2HZmlA/xS3IwEAXEbZBBAUZZca9fgz+xQXF6fvPzhbgwekuR0JAOABlE0AfVZV26wfPbNPrW0+ff+BWRqale52JACAR1A2AfRJTX2LfvTsPtU1tuovHpilEUP6ux0JAOAhlE0AvVZT36LHn9mnyuomfXfNTI3Ny3Q7EgDAY7hMFECvVHcWzYrqRn13zUxNGjnQ7UgAAA+ibALoMf+i+b01M2VGDXI7EgDAoyibAHqkuq5Z//zMPlXWNFE0AQDdomwCCNilumY9/sw+XaxppmgCAAJC2QQQkMuK5mf4jiYAIDCUTQDdKr/UqB89u0819a0UTQBAj1A2AXyq8xUdt6BsaW3X9x+apfHDBrgdCQAQQSibALpUcKFG//e5A0qIj9MPPzuHBdsBAD1G2QRwVbaoSv/+wkH1S03S9x+apdxB3IISANBzlE0An3Awv0I/ffmwBg9I1V8+MEtZmaluRwIARCjKJoDL7Dp6Qb9ae0wjcvrrew/MVGZ6stuRAAARjLIJQJLkOI427inWHzed0qSRA/Wd+2YoPZW3CABA3/A3CQD5fI6eeeek3tl7VvMnD9GX75iqpMR4t2MBAKIAZROIcS2t7frF60f14Yly3bpgpNYsm6D4uDi3YwEAogRlE4hhtQ0t+vGLB3X6XI0eWj5RN88f6XYkAECUoWwCMarsUqP+9Y8HVFndpG+snqZ5k4e4HQkAEIUom0AMOlF8ST956ZAcx9H3H5zF7ScBACFD2QRizNYD5/VfG60GD0zTd++fodwsFmsHAIQOZROIET6foz9uOqU33y/W1DGD9I3V09QvNcntWACAKEfZBGJAY3Obfv7aER3Mr9TyOSP04IoJSohnaSMAQOhRNoEod6GyXv/rt3t1obJBn7/VaNns4W5HAgDEEMomEMUOn67UL14/Ksdx9JcPzNSUMVluRwIAxBjKJhCFfI6jdTsK9MrWMxqdl6mv3TVVuYO4EAgAEH6UTSDKNDS16sm1x7T/VIWunZqrv/zcPNXWNLodCwAQoyibQBQ5W16nn7x0SJXVTfrsiolaPneEUlMSVet2MABAzKJsAlFi19EL+s3640pLTtQPHprNQu0AAE+gbAIRrrm1Xc+8fVJbDpzXxBED9I3V0zSwf8r/3969h0lVnfke/3Y3yNXm0tAggiKIbyMKXlFRgiRqkGQUjXG8JZrRZHJydOIxl5nMcybhmJlnJsdnzplMTjJO1IwxmmjG65goqHgBFCMGAUV4US7Knaa5yR266/yxdqU3TRdd3VW7qrv693meety1e+1aS3atvd9ae12KXSwRERFAwaZIh7Zuy27ufeY91tXuZur5JzJt4kl0qdD8mSIi0n4o2BTpgFKpFHPf3cAjLy6nW9cK7rp2HKeNqCp2sURERI6gYFOkg9m7/xC/esF5c8kmak7oy9euGKPH5iIi0m4p2BTpQFau38l9zy5h8/a9TLvoJD4/YTjl5WXFLpaIiEhGCjZFOoD6hgZ+98ZHPPv6avoeewzfvf5M7IR+xS6WiIhIixRsirRzm7bu4b7fvc/K9Ts5f8wgbrr0FHp271rsYomIiGSlXQebZlYFjAfOi/47HoiPgjjJ3VcnXIZBwFeAq4DhQB9gA7AQeAR40t0bkiyDdE6pVIrXFq3n0Vkf0KW8nK9fOYbxowcVu1giIiKt0m6DTTN7HphS5DJMAx4A+jf50/DoNQ14zcxudPd1hS2dlLLtu/bz0Axn4YdbGH1iP2793Gj6V3YvdrFERERard0Gm8BxxczczKYCjwMV0a5dwCygDhgNXBDtnwTMNLMJ7r6z4AWVkpJKpXj93Y08OusDDtY3cN1nRnHJOUMpL9MgIBER6Zjac7AJcAhYDPwBeAtYD8xMOlMzGwD8hsZAcwZwg7tvi6WZCDwJDADGAD8Bbk66bFK6tu7cx4MzlvHeyq2MGtqHr0wdzeD+PYtdLBERkZy052DzRmClu+9N7zCz4QXK+3tAZbS9HLjK3ffFE7j7HDO7Hngx2nWTmf1vd19SoDJKiUj3zfztyx/SkEpxwyWj+PTZas0UEZHS0G6DzWIFbWbWFbg1tuv7TQPNNHd/ydMMV48AABsDSURBVMxmAp8FyoGvA3ckX0opFbXb9/Lg88tY+tE2Rp/Yj5svr6G6b49iF0tERCRv2m2wWUSTCSPOAXYCT7WQ/peEYBPCgCEFm9KiQ/UNvPT2Wp6Zu4qyMvjyFGPSuCGUqTVTRERKjILNI02Obc9z9wMtpH81tj3UzEa5+wf5L5aUig/X7uChmctYW7ubcSOruOkyo6qPRpqLiEhpUrB5pNGx7QUtJXb3DWa2ERgcO17Bphxh196DPP7qCmYvWk+/Y7tx+9Wnc+aoAWrNFBGRkqZg80inxLY/zvKYNTQGm5bf4khHl0qleOO9jTz28ofs2XeIKeNP4IqLhtP9GFU/EREpfbrbHSm+QtGmLI/ZGNtuOgG8dGJrN+/ikReX42u2M/L4Sr782RqGVfcudrFEREQKRsHmkeKRwN6MqQ4XT9emSKKqKvkAZODAYxPPQ4Kduw/wyIylzJi3mp7du3L7F8dx6fgTKS8vziNznfvOS+e+c9J577za47lXsHmk+EiNlgYHpe2Pbbdp3pq6ul00NKTacmhWBg48ltraTxL7fAkO1Tfw6jvreGbuKvbur2fymUO5cuJJ9O7Rlbq6XUUpk85956Vz3znpvHdehTj35eVlrW4ga1WwaWa3A7e3KoeWXe7uq/L8mbnYB6SXbTkmy2O6xbazbQ2VErNk1VZ+M+sD1m/ZzegT+3H9JaMYOlCPzEVEpHNrbcvmAPI/AKZby0kKaheNwWa2rZTxdMVpvpKi2bh1D799+UMWfriFgX27c8fVp3OGRpmLiIgAeozenK1AdbQ9KMtj4um25rc40l7t2LWfZ15fzeyF6+natZxrLh7JpecMo2uX8mIXTUREpN1oVbDp7tOB6YmUpP1woCbaPiHLY+LpPL/FkfZm7/5DzHzrY2a+tYZD9Q1cfOYQrrjwJCp7ZdvrQkREpPNQy+aRlgJXRttntpTYzAbTOMdm+ngpQYfqG3ht4XqefX0VO/cc5Jyaar7wqREM6t+z5YNFREQ6KQWbR3oF+Jto+wIz6+ruB4+SflJse62Wqiw9DakU85du5uk5K9m0bS82rC9/dc3JjBhSWeyiiYiItHsKNo/0CrATqAT6ANOA/zxK+ptj288kWC4psFQqxYLltTw9dxXrandz/IBefPOasYwdWaXBPyIiIllSsNmEux80s/uBu6Jdd5vZs+6+r2laM5sMTIneNgD3FqiYkqBUKsWiD+t4eu5KPt60i8H9e/KXV4zh3NHVlCvIFBERaZVOFWyaWXzW9K+4+4MZkv4jcBuhdbMGeMLMbnT37bHPuhB4DEhHHw+7+3v5L7UUSiqVYsmqrTw1ZxWrNuxkYN/u3Pq50Zw/ZhAV5RphLiIi0hbtNtg0syuAu5vsbjrc9zkza7rKz73unlMLo7tvMbPrgf8CKoCpwBoze4kwtVENMCF2yPvAHbnkKcWTSqVYvKKO381bzYp1O6mq7MYtl9cw4bTBdKlQkCkiIpKLdhtsAv2BcS2kGd3MvsHN7Gs1d3/OzK4F7ovK0pvQf7Op2cAN7r4zH/lK4TQ0pHjbN/P7eR+xZvMuqiq786XLTmHiuCEKMkVERPKkPQebRefuT5rZ68BfAFcBwwmP1jcCi4CHgSfcvaFohZRWO1TfwLwlG3nuzY/ZtHUPg/v35NbPjea8UwcpyBQREcmzslQq1XIqSdJwYFVd3S4aGpI7FwMHHktt7SeJfX5HcOBgPXMWb2DGHz6ibud+TqjuzecnDOesUwZSXl66A3907jsvnfvOSee98yrEuS8vL6OqqjfAScDqbI5Ry6aUvJ27D/DKO+t4ecFaPtlzkJOP78OXPmucPkJTGImIiCRNwaaUrPVbdvPC/DW88d5GDtU3MG5kFVPOO4FThvVVkCkiIlIgCjalpKRSKZZ9tI2Z89eweEUdXbuUc+Hpg7ns3GEcV9Wr2MUTERHpdBRsSkk4eKiB+cs28cJba/h48y4qe3Zl2kUncfFZx1PZs+mMWSIiIlIoCjalQ6vbsY9XF65j9qL1fLLnIMdV9eSWy2u4YMwgunapKHbxREREOj0Fm9LhpFIp3v9oGy//cS0LP9wCwBknD+DTZw/l1BP7qT+miIhIO6JgUzqMvfsP8fq7G3jlnXVsqNtD7x5dmXr+iVx8xvFU9ele7OKJiIhIMxRsSruWSqVYuX4nry1az/ylm9l/sJ4RQyq57fOjObemWo/KRURE2jkFm9Iu7dp7kDfe28icRetZt2U33bpWcN6p1Uw643hOOq6y2MUTERGRLCnYlHajIZq2aPai9SxYXsuh+hQjhlRyy+U1nFtTTY9u+rqKiIh0NLp7S9HV7djHG0s2Mnfxemq376NX9y5cfMbxfGrcEIZW9y528URERCQHCjalKPbuP8Tbvpl5723EP95OCqg5oS9XTRzB2TZQfTFFRERKhIJNKZj6hgbeX72Nee9tZMHyWg4caqC6Xw+unHgSF4wZzMC+PYpdRBEREckzBZuSqFQqxZrNu5i3ZCNvLtnEjt0H6NW9CxeefhwXnDaYkUMqNS+miIhICVOwKYnYULebt5Zu5q2lm9hQt4eK8jLGjqxiwmmDGTtyAF27lBe7iCIiIlIACjYlbzZv38v8pZv4w/ubWVu7izJg1LC+fObsoZxbU82xWqNcRESk01GwKTmp27GP+ctCC+bqjZ8AMPL4Sq7/zCjOqamm37HdilxCERERKSYFm9Jqtdv3smB5LW/7Zlas2wnA8MHHcu3kkzm3plpLR4qIiMifKNiUFqVSKdbW7mbB8loWLK9lzeZdAAyr7s0XJo3g3Jpqqvv1LHIpRUREpD1SsCnNamhIsWL9jj8FmLXb91EGnDy0D3/+6ZM585SBVGuqIhEREWmBgk35kwMH61n60TYWfriFdz7Yws7dB+hSUcboE/sz9fwTOWPUQPr00iAfERERyZ6CzU5uy469LF5Rx+IVdSz9aBsHDzXQ7ZgKxo6o4qxTBjJ2ZJXWJBcREZE2UxTRydQ3NLBi3U4WrdjC4g/rWLdlNwDVfXswadwQxp5chQ3rp3kwRUREJC8UbHYCe/cf4pU/rmHuO2tZsmoru/cdoqK8jFFR/8uxI6sY3L+nVvIRERGRvFOw2Qk8+Pwy5i/bTGXPrpwxagDjRg7g1OH96dldp19ERESSpWijE/ji5JFcP6WGym4VlKv1UkRERApIHfM6gQF9ejBqWD8FmiIiIlJwCjZFREREJDEKNkVEREQkMQo2RURERCQxCjZFREREJDEKNkVEREQkMQo2RURERCQxCjZFREREJDEKNkVEREQkMQo2RURERCQxCjZFREREJDEKNkVEREQkMQo2RURERCQxCjZFREREJDEKNkVEREQkMQo2RURERCQxCjZFREREJDEKNkVEREQkMQo2RURERCQxCjZFREREJDEKNkVEREQkMQo2RURERCQxXYpdAKECoLy8LPGMCpGHtE86952Xzn3npPPeeSV97mOfX5HtMWWpVCqZ0ki2LgLmFLsQIiIiIq0wEZibTUIFm8XXDTgX2ADUF7ksIiIiIkdTARwHzAf2Z3OAgk0RERERSYwGCImIiIhIYhRsioiIiEhiFGyKiIiISGIUbIqIiIhIYhRsioiIiEhiFGyKiIiISGIUbIqIiIhIYhRsioiIiEhitDZ6CTCz1cCJrThkhbufnEA5BgFfAa4ChgN9CCsjLQQeAZ5094Z859tZmdkxwCTgEuAcoAaoAhqALcAiYAbwkLt/kue8HwRubuVhw9x9bT7LUWrMrJLw73oNMAroD2wGlgKPAr9296xW7GhD3p+N8j4XOB7YA3wM/B54wN1XJ5FvZ2dmpwKXEZYuPg0YAnQHdgArgdnAL9x9aR7zvBh4pZWH/Z27/32+ytDZmdl04AetPGyiu2e1PGSWZSgHvgDcCIwjrAq0A1gFPA38h7tvykdeCjYlL8xsGvAA4eYYNzx6TQNeM7Mb3X1dYUtXeszsHuA2oG+GJMOi1+eBH5rZN9z90UKVT1rPzC4i/Cg7ocmf0ufyMuBOM7suz4FHX0LdvbrJn3oQfrycCdxlZne6+335yrezM7NLgX8BTs2QZED0Gg9828zuB+50990FKqKUMDMbSrjefKrJn6qj13mE792t7v5Mrvkp2Cw9DwEttWLV5jNDM5sKPE5YLxVgFzALqANGAxdE+ycBM81sgrvvzGcZOqE/4/BA8xPgD8A6oJ7Qynk+oatMP+A3ZjbQ3X+SQFlmAcuySLcrgbxLgpmdRWiF7hXtOkD4d10PjCDUnXJgLDDLzMbno5U4ah1/hsNvOIsITyN6AZMJAWdP4Odm1uDuD+SarwBwNocHmg2Ef/sPgO3AYMJ5Sdfz24AaM7vM3ffmsRzrgaeySDc/j3nK4eYDb2WRbn0+Mot+YL5AuD+nzSM8QakCPgP0jrafMLOp7v5CLnkq2Cw9Pyjk4y4zGwD8hsZAcwZwg7tvi6WZCDxJ+JU+BvgJrX8EK0c6SAjy7wdec/f6+B/NrAZ4mHBTA/gXM5vt7ovyXI6H3f3BPH9mpxEFfE/QGGj+EZgWDyajR63PEgLP4wgtEpPykP10GgPNPcCX3P3JWL49gf9H6B4D8G9mNtfdPQ95S/AO8HPgsfh1E8DMehEetX4n2nUR8EPg23nM/wN3vz2Pnyet95y7Ty9gfj+jMdDcAlwVfzxvZv2AXwNTCPf2x8xspLtvbWuGGiAkufoeUBltLyd8aQ+7YLr7HOD62K6bzGxMgcpXqh4Fatz9Bnd/uWmgCeDuywj9OVdHu8qBvy1cESVLf0noagLhacDUpq2W7v4+oTU73V/zU2Z2eS6ZRn2svxnbdUc80Izy3QPcSug3CNAVuDuXfOVPlhN+VJzl7vc2vW4CuPtud/8u4XF72h1m1qdgpZSSYmZjgetiu65r2g80+i5eTWhlh9C6/te55KtgU9rMzLoSbkRp33f3fc2ldfeXgJnR23Lg6wkXr6S5+3R3X5lFuu3Aj2K7piRXKmmjb8S273H3zc0ligLOBzMc1xa3EB6PAyx1919kyDfF4Teaa8ysOse8Oz13f7IVfeGmE7pWABxDeMwp0hb/DSiLtme4+6zmEkVdNb4f2/VVM2vz03AFm5KLyYQR5wA7abnfzy9j29MSKZE0543YdqWZNR3EJUViZqcQ+tem/TJT2mb+fomZ9c4h+yuzzdfd3wTSj87LgStyyFdayd13AEtiu4YXqSjSgZlZGYfX3QdbOOQpGseA9AMubmveCjYlF5Nj2/Pc/UDGlMGrse2hZjYq/0WSZqSavK9oNpUUQ7wOubtvbCH9fCA9Grk7jYPvWsXMehBGm6a9lsVh8TSTM6aSpMTrseqwtMUphKm10o5a76Np1ubFdrW53muAUOk528yuInyh6gl9wBYCbyQwZUZ8JNuClhK7+wYz20gYZZk+/oOjHCL5cXpsey+hQ3g+DTezrwInAd0I37kPgNn5mqOthLW2Dh0ys3cJMw2kj3+xDfkajY0NKcI1oiXx8o3OmEryzsy6EeZdTVuTx4/vYWZXEOZZ7EdoydpAeCLybtSNQpI1yMy+TDjHvYBthLkuZ7v7x3nMJ15vN2Tx4xZCvb+smeNbRcFm6Xk8w/7dZnYfcHdzHdHb6JTYdrYVYg2NwablqRxydLfEtl9O4OaRaWLilJk9RZgM+v0851kq2lqH0sFmW+tQPN/NmfpaN5NvmupuYX0BODbaTgEv5/GzxxOmv2qOm9k/uPuv8pifHOnrZBjHYGazCNfQec39vZXaer35U3HamrEeo3cevYA7gQVmdnpLibNUFdvOtgUr/ktKfQcTFo1YvjS262cFzL6MMKJxvpldW8B8O5Ji1aFc8+0ZtbZJwqIuD/8Q2/V4pkFkSWQPPGRmv4kGhErhfQaYY2Z35uGzinbPVrBZGg4S5uD7GuFRSCVhxOJxhM7Az8bSDgeeM7PB5C4+OCHbSYbj6XIZ3CAtiEYM/zy2a5a7P5enj08RJiH+G2AC4SLWlXAx+hTwUxqn6ekJPGxm6ud3pGLVoVzzzSVvaZ0f0zggaB/wP/P0uZsJc6heQVjuuDuhUaIG+CvCY9y064B/y1O+0uh94H8R+kIOIty3+xJam/+RMPAWQh/d/2tmX8oxv6Lds/UYvTSc7+51zezfSAg0n436g/wH4QfGUOCfOPzxalt0j223NDgoLb6uc48c85cMoikqHiOca4Ct5H6+476d4Tu3DZhD+CX+C+B5wtJnXYH7zKzG3Q/lsRwdXbHqUK755pK3ZMnMbgO+Gtv1LXdfnoePfhsYlmFQpxMen/+CsHhAetaCW83sYXd/NQ/5C/xrhoncdxAGAs6Plih9nsbH3/9qZr/LoStc0e7ZCjYTYma3A/leleFyd1/VdGeGm37TNA+Z2Qga+9fdZGbfc/cNOZRnH43z9B2T5THxR2/5XHKtXSjkeW/BfTROU3EIuCkfyxumZfmdW2Bmfw68Eu0aCVwF/Ge+ylEC4n0lC1mHcs03l7wlC2Z2GYd3e3nE3fPSDcbdW1w61t13m9l1hMFj6b563+HwWUWkjbJZjcfdV5rZnwHv0tjqeRtwTxuzLdb1RsFmggaQ/070ufaRugf4LuHXSQVhdZlcOn7vojHYzPYXTzxdKa6VXfTzbmb30NiKmQK+6u7P57lMWXH3V81sNo1LIk5BwWZcvA4Usg7lmm8ueUsLzOw8whK/6X6SLwB/UehyuPs+M/sRkJ7wf7KZdYumxJECcPflZvYo8OVo1xTaHmwW63qjPpudSTT10ZuxXblOXxL/ZTYoy2Pi6dq8zqo0z8z+lsPXTf52O1i3PL5ChabMOVyx6lCu+e5RwJEMMzsNeI7QfxLCNfvqLOYxTkq8/vYg9O+UwsrXNbRo92y1bCYk6osxvcjFaE58ZFlVxlTZcRpXPzkhy2Pi6Txjqg6qmOfdzP47h49a/Xt3/z/FKEsT+fzOlRqncQnRQtah+HHVWbZWlXTdbQ/M7GTCvKnpUb/vAlMTmCO5NZrOxag6XHj5uobG621B79lq2ex8esa2c72ALY1tn9lS4mgEfHwU/NJMaaV1zOxm4CexXT9x978rVnmayOd3rtS0tg5VcPgk/W2tQw40RNtlwBlZHBMvn+punpnZMOAlGq+RHwCX5nFe5Lbq2eS96nDh5esaGq+3x5lZNq2bean3CjY7n/hNJZfBQdA48APggizmYZsU217r7lo9KA/M7AvAA4SgAcI6198sXomOEL9Y5fqdKzXxOmRZXPzPofHxatOl5LLm7nuBP8R2TcqUNkOaVzKmklaLpil7icZH1GuAS9rJClxNfwSpDhdevq6hy4H1sfdHrfdmdgyHL4nb5nqvYLMTieY5jPe3mZ3jR75C4zxgfYBpLaS/ObadacUKaQUzmwL8msa1kh8Hbm0vS8yZWT8ap06B3L9zJSWaxmZZ9LaMxkEAmcTr0EvZjCo+ingdvDljKsDMxtPYZaYB+K8c8pWYqI68SOP0NpsJgWY+lynMxS2x7WXuXlusgnRG0TR2N8Z2tfkaGt0X4nX3qPWecE+vjLa3k8NMBAo2Ozgzy2qSVTPrD9wb27WcMJdXm7n7QeD+2K67zax7c2mjQDfdN62hSVmkDcxsImHEanoKixnAje5en3C+2X7nyoB/J/wQgbD4wG+TKlcHFp/O5jtmNqC5RGZWA3wltuunOeb7S2BPtH1q1BWjuXzLCPPyphVyBZuSZma9CIOBxka7thEenedjLs1MefY0s6zu/dGa6TfFdj2cTKk6l2yvoZF/Ikwbl/ZIjtnfS5ilBODyTIttRPfyu2O7fp7LHMkKNju+N83sn81sXKYEZnYJ4ZFZfF3Uv3b3hgyHYGap2OuWo+QfX+WgBnjCzPo2+awLCROMpx/zPuzu7x3lM6UFZnYW8Dsap6WYTR5GrGZ53n9sZo+Z2SVRH8LmPscIkxF/Mbb7p22YL7Qz+HdgdbQ9kLDC1/HxBGY2mnC+0z/m5mSazsrMpsfO4erm0gC4+0bC6jRpPzWzw55OREsl3k9Y4QTCD4bvZ/H/JC2Ilvt8hsZ17ncR5tRdnMNnPhg7969mSDYeeM/MvmZmzQ42MbNeZvY9wpOSdJywlsO/L9J23zKzmWZ2ZaZlX81sqJn9CvhWbPfT7j43Q/pszj3uvgh4NHpbBjwW3aPjn9UXeILGafy2Az/K5n8sE41G7/h6A3cBd5nZRmARYc3TvYQRjeM5cqqK6e7+dD4yd/ctZnY9oWm+ApgKrDGzlwjTJNQQljNMex+4Ix95d3IzaHy8ASFYuSfEeC36cY79ZSuAa6PXJ2a2EPiYcLPsDYwhLJtaFjvmRcIcr9KEux+I+t3OIQwEOBdYYWazCP2zhhOCvfRNfyOHP1bLxXTgQsI8qL2Ap6LzuTB6P5kwd2zaN9xdI9Hz44eEda/T3ge+lOWShG+6ey6tjKMJP3J+ZmZOGPixjfAdG0bopxdvfdsBfD7HbhvSqAy4LHrtNbPFwEpCw00PQsPQORweoy2i5cfe2foGoR9oDeEH7lwze4PQpac/YQ7u9PmvB67LZhL6o1GwWVqajvZuqha4w90fy2em7v6cmV1LWLWmP+FL2lz/zdnADe6+s5m/SesMbPK+pb5+cY8TRrrmw7HAxKP8fT/wz8APtExlZtFqS1MIjylPIEzkP7WZpO8SLvxr8pTvgag18wHC6k4QBhE2HZ2+B7jL3e9H8qW6yfvx0SsbvcnPI+0K4NTolcls4BY9lUhMD+C86NWcesKk+v8jX1Ngufv2aIWqR2i8fk/g8IYhgDrCGICZueapYLPjm0homZgAnEW4gA0gBAG7CJ3N3wZmAr91930ZPicn7v6kmb1OWOXiKkJrTCWhFWYR4cL4xNEe3UuHcSeh7+UEws1xKGHut36E4HIrsBh4DXhI/fuy4+5zzOx0woCMLwInE3681RJavR4lLFmY18nUo6l1ro6C3ZsJ53QI4enIx8DvgfsVbJSMOYTW8wmEFkwj3DOqCMHnDmAV8AbhnvFmhs+RtruHcB4mELpRnEA4B/0JywtvI9T5ucAv3f2jfBfA3deY2cXANYQnJeMIjVU7CU/KngJ+ka8ZEcpSqXYxaFVERERESpAGCImIiIhIYhRsioiIiEhiFGyKiIiISGIUbIqIiIhIYhRsioiIiEhiFGyKiIiISGIUbIqIiIhIYhRsioiIiEhiFGyKiIiISGIUbIqIiIhIYhRsioiIiEhiFGyKiIiISGIUbIqIiIhIYv4/WeqAc3Wd8uwAAAAASUVORK5CYII=\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"605.437812pt\" version=\"1.1\" viewBox=\"0 0 668.033281 605.437812\" width=\"668.033281pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 605.437812 \nL 668.033281 605.437812 \nL 668.033281 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 96.833281 556.8 \nL 654.833281 556.8 \nL 654.833281 13.2 \nL 96.833281 13.2 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 122.196918 556.8 \nL 122.196918 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- âˆ’5.0 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(82.130277 591.374844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 249.015099 556.8 \nL 249.015099 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- âˆ’2.5 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(208.948459 591.374844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 375.833281 556.8 \nL 375.833281 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(349.593125 591.374844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 502.651463 556.8 \nL 502.651463 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2.5 -->\n      <g style=\"fill:#262626;\" transform=\"translate(476.411307 591.374844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 629.469645 556.8 \nL 629.469645 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 5.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(603.229489 591.374844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 96.833281 492.588839 \nL 654.833281 492.588839 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- âˆ’1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 505.126261)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 96.833281 388.79442 \nL 654.833281 388.79442 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- âˆ’0.5 -->\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 401.331842)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 96.833281 285 \nL 654.833281 285 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.852969 297.537422)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 96.833281 181.20558 \nL 654.833281 181.20558 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.5 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.852969 193.743002)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p8288a9778a)\" d=\"M 96.833281 77.411161 \nL 654.833281 77.411161 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.852969 89.948583)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#p8288a9778a)\" d=\"M 122.196918 532.090909 \nL 133.379282 528.81056 \nL 143.545068 525.596693 \nL 153.710855 522.137621 \nL 162.860062 518.79255 \nL 172.00927 515.20478 \nL 180.141898 511.791752 \nL 188.274527 508.147613 \nL 196.407156 504.250559 \nL 203.523207 500.61419 \nL 210.639257 496.747786 \nL 217.755307 492.631762 \nL 224.871357 488.244708 \nL 231.987408 483.563277 \nL 238.086879 479.297137 \nL 244.186351 474.778896 \nL 250.285823 469.99021 \nL 256.385294 464.911631 \nL 262.484766 459.522675 \nL 268.584238 453.801961 \nL 274.683709 447.727416 \nL 279.766602 442.378812 \nL 284.849495 436.755906 \nL 289.932389 430.845954 \nL 295.015282 424.636742 \nL 300.098175 418.116933 \nL 305.181068 411.27648 \nL 310.263961 404.107114 \nL 315.346854 396.60289 \nL 320.429747 388.760794 \nL 326.529219 378.905368 \nL 332.62869 368.57357 \nL 338.728162 357.784835 \nL 344.827634 346.570169 \nL 351.943684 333.006623 \nL 360.076313 316.985209 \nL 370.242099 296.428706 \nL 392.606828 250.98688 \nL 400.739457 235.027056 \nL 407.855508 221.532826 \nL 413.954979 210.386476 \nL 420.054451 199.67207 \nL 426.153923 189.418613 \nL 432.253394 179.643727 \nL 438.352866 170.354971 \nL 443.435759 162.985231 \nL 448.518652 155.948287 \nL 453.601545 149.237195 \nL 458.684438 142.842912 \nL 463.767331 136.754822 \nL 468.850224 130.961191 \nL 474.949696 124.379938 \nL 481.049168 118.182468 \nL 487.148639 112.34629 \nL 493.248111 106.849163 \nL 499.347583 101.669448 \nL 505.447054 96.786345 \nL 511.546526 92.18005 \nL 517.645998 87.831844 \nL 523.745469 83.724141 \nL 530.86152 79.213901 \nL 537.97757 74.984454 \nL 545.09362 71.013571 \nL 552.20967 67.280902 \nL 560.342299 63.282898 \nL 568.474928 59.546596 \nL 576.607557 56.049313 \nL 585.756765 52.375232 \nL 594.905972 48.951863 \nL 604.05518 45.756075 \nL 614.220966 42.447152 \nL 624.386752 39.368801 \nL 629.469645 37.909091 \nL 629.469645 37.909091 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 96.833281 556.8 \nL 96.833281 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 654.833281 556.8 \nL 654.833281 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 96.833281 556.8 \nL 654.833281 556.8 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 96.833281 13.2 \nL 654.833281 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p8288a9778a\">\n   <rect height=\"543.6\" width=\"558\" x=\"96.833281\" y=\"13.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": "<Figure size 720x720 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "alpha = 0.1\n",
    "\n",
    "# In units of STD\n",
    "n = 500\n",
    "x_max = 5.0\n",
    "x = np.array( [ -x_max + (2.0*x_max)*ind/(n-1) for ind in range(0,n) ] )\n",
    "\n",
    "# y = alpha + (1.0-alpha)*(420-x)/420.0\n",
    "y = np.arctan((x)*0.50)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.0"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(11/10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}