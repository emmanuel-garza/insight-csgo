{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bittfconda04cc3002c149454bb73c3f879bac7c68",
   "display_name": "Python 3.7.6 64-bit ('tf': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is exclusively to create the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modues to be used\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original data\n",
    "\n",
    "# -> Player dictionary with the dataFrames\n",
    "f = open('../data/dict_player.pickle', 'rb')\n",
    "dict_player = pickle.load(f)\n",
    "\n",
    "# -> Map dataFrames\n",
    "f = open('../data/df_map.pickle','rb')\n",
    "df_map = pickle.load(f)\n",
    "\n",
    "# -> Map dictionary\n",
    "f = open('../data/dict_map.pickle','rb')\n",
    "dict_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "420"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_map.loc[ df_map['team_rank_1']==0, 'team_rank_1'] = 420\n",
    "\n",
    "df_map.loc[ df_map['team_rank_2']==0, 'team_rank_2'] = 420\n",
    "\n",
    "df_map['team_rank_1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>map_id</th>\n      <th>map</th>\n      <th>date</th>\n      <th>event_id</th>\n      <th>event_name</th>\n      <th>team_id_1</th>\n      <th>team_name_1</th>\n      <th>team_score_1</th>\n      <th>team_kills_1</th>\n      <th>team_deaths_1</th>\n      <th>team_assists_1</th>\n      <th>team_rank_1</th>\n      <th>team_id_2</th>\n      <th>team_name_2</th>\n      <th>team_score_2</th>\n      <th>team_kills_2</th>\n      <th>team_deaths_2</th>\n      <th>team_assists_2</th>\n      <th>team_rank_2</th>\n      <th>winner</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97594</th>\n      <td>97594</td>\n      <td>inf</td>\n      <td>2020-01-10 01:00:00</td>\n      <td>5164</td>\n      <td>Aorus League 2019 #4 Northern Cone</td>\n      <td>10330</td>\n      <td>Supremacy</td>\n      <td>16</td>\n      <td>99.0</td>\n      <td>93.0</td>\n      <td>16.0</td>\n      <td>420</td>\n      <td>10578</td>\n      <td>Infamous</td>\n      <td>12</td>\n      <td>93.0</td>\n      <td>100.0</td>\n      <td>17.0</td>\n      <td>420</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97592</th>\n      <td>97592</td>\n      <td>nuke</td>\n      <td>2020-01-10 01:00:00</td>\n      <td>5164</td>\n      <td>Aorus League 2019 #4 Northern Cone</td>\n      <td>10330</td>\n      <td>Supremacy</td>\n      <td>16</td>\n      <td>98.0</td>\n      <td>69.0</td>\n      <td>17.0</td>\n      <td>420</td>\n      <td>10578</td>\n      <td>Infamous</td>\n      <td>8</td>\n      <td>69.0</td>\n      <td>98.0</td>\n      <td>13.0</td>\n      <td>420</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97595</th>\n      <td>97595</td>\n      <td>inf</td>\n      <td>2020-01-10 00:35:00</td>\n      <td>5104</td>\n      <td>IEM Katowice 2020 North America Closed Qualifier</td>\n      <td>5752</td>\n      <td>Cloud9</td>\n      <td>16</td>\n      <td>100.0</td>\n      <td>91.0</td>\n      <td>14.0</td>\n      <td>18</td>\n      <td>9215</td>\n      <td>MIBR</td>\n      <td>12</td>\n      <td>90.0</td>\n      <td>100.0</td>\n      <td>16.0</td>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97593</th>\n      <td>97593</td>\n      <td>ovp</td>\n      <td>2020-01-10 00:35:00</td>\n      <td>5104</td>\n      <td>IEM Katowice 2020 North America Closed Qualifier</td>\n      <td>5752</td>\n      <td>Cloud9</td>\n      <td>16</td>\n      <td>77.0</td>\n      <td>46.0</td>\n      <td>11.0</td>\n      <td>18</td>\n      <td>9215</td>\n      <td>MIBR</td>\n      <td>4</td>\n      <td>46.0</td>\n      <td>77.0</td>\n      <td>10.0</td>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97591</th>\n      <td>97591</td>\n      <td>trn</td>\n      <td>2020-01-10 00:35:00</td>\n      <td>5104</td>\n      <td>IEM Katowice 2020 North America Closed Qualifier</td>\n      <td>5752</td>\n      <td>Cloud9</td>\n      <td>13</td>\n      <td>90.0</td>\n      <td>105.0</td>\n      <td>12.0</td>\n      <td>18</td>\n      <td>9215</td>\n      <td>MIBR</td>\n      <td>16</td>\n      <td>105.0</td>\n      <td>90.0</td>\n      <td>20.0</td>\n      <td>14</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       map_id   map                date  event_id  \\\n97594   97594   inf 2020-01-10 01:00:00      5164   \n97592   97592  nuke 2020-01-10 01:00:00      5164   \n97595   97595   inf 2020-01-10 00:35:00      5104   \n97593   97593   ovp 2020-01-10 00:35:00      5104   \n97591   97591   trn 2020-01-10 00:35:00      5104   \n\n                                             event_name  team_id_1  \\\n97594                Aorus League 2019 #4 Northern Cone      10330   \n97592                Aorus League 2019 #4 Northern Cone      10330   \n97595  IEM Katowice 2020 North America Closed Qualifier       5752   \n97593  IEM Katowice 2020 North America Closed Qualifier       5752   \n97591  IEM Katowice 2020 North America Closed Qualifier       5752   \n\n      team_name_1  team_score_1  team_kills_1  team_deaths_1  team_assists_1  \\\n97594   Supremacy            16          99.0           93.0            16.0   \n97592   Supremacy            16          98.0           69.0            17.0   \n97595      Cloud9            16         100.0           91.0            14.0   \n97593      Cloud9            16          77.0           46.0            11.0   \n97591      Cloud9            13          90.0          105.0            12.0   \n\n       team_rank_1  team_id_2 team_name_2  team_score_2  team_kills_2  \\\n97594          420      10578    Infamous            12          93.0   \n97592          420      10578    Infamous             8          69.0   \n97595           18       9215        MIBR            12          90.0   \n97593           18       9215        MIBR             4          46.0   \n97591           18       9215        MIBR            16         105.0   \n\n       team_deaths_2  team_assists_2  team_rank_2  winner  \n97594          100.0            17.0          420       1  \n97592           98.0            13.0          420       1  \n97595          100.0            16.0           14       1  \n97593           77.0            10.0           14       1  \n97591           90.0            20.0           14       2  "
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map.sort_values(['date'],ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "29013"
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(( df_map[ (df_map['date']>datetime(2016,6,6)) & (df_map['team_rank_1']<150) &  (df_map['team_rank_2']<150) ] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Converting data to DataFrame\n0:58:55.156098\n"
    }
   ],
   "source": [
    "# All unranked teams make them 420\n",
    "df_map.loc[ df_map['team_rank_1']==0, 'team_rank_1'] = 420\n",
    "df_map.loc[ df_map['team_rank_2']==0, 'team_rank_2'] = 420\n",
    "\n",
    "df_map['team_rank_1'].max()\n",
    "\n",
    "# Loop throughout all of the players and put it in the right map\n",
    "DAYS_WEIGHT = 90\n",
    "\n",
    "MAX_RANK = 150\n",
    "time_1 = datetime.now()\n",
    "\n",
    "\n",
    "# df_map_ranked = ( df_map[ (df_map['date']>datetime(2016,6,6)) & (df_map['team_rank_1']>0) &  (df_map['team_rank_2']>0) ] )\n",
    "df_map_ranked = ( df_map[ (df_map['date']>datetime(2016,6,6)) & (df_map['team_rank_1']<MAX_RANK) &  (df_map['team_rank_2']<MAX_RANK) ] )\n",
    "\n",
    "df_map_ranked = df_map_ranked.sort_values(['date'],ascending=False)\n",
    "\n",
    "metric_vec = ['KAST', 'ADR', 'first_kills_diff', 'rating',\n",
    "       'kills_per_round', 'deaths_per_round', 'impact', \n",
    "       'team_score',\n",
    "       'op_score', 'win', 'team_rank', 'prize']\n",
    "\n",
    "map_training_dict = {}\n",
    "\n",
    "\n",
    "# Function to append values of all the players\n",
    "def append_val(team_01,stat_vec,stat_name,dict_inout):\n",
    "\n",
    "    n = len(stat_vec)\n",
    "    order = np.argsort( stat_vec )[::-1][:n]\n",
    "\n",
    "    count_p = -1\n",
    "    for ind in order:\n",
    "        count_p = count_p + 1\n",
    "        dict_inout['t_'+team_01+'_p_'+str(count_p)+'_'+stat_name] = stat_vec[ind]\n",
    "    \n",
    "    return dict_inout\n",
    "\n",
    "for map_id in df_map_ranked['map_id'][:]:\n",
    "    \n",
    "    map_training_dict[map_id] = {}\n",
    "\n",
    "    # Keys for the dictionary\n",
    "    map_training_dict[map_id]['t1_win']    = 0\n",
    "    map_training_dict[map_id]['fav_win']   = 0\n",
    "    map_training_dict[map_id]['fav_ind']   = 0\n",
    "    map_training_dict[map_id]['map']       = df_map_ranked.at[map_id,'map']\n",
    "    map_training_dict[map_id]['score_dif'] = 0\n",
    "    map_training_dict[map_id]['t1_rank']   = 0\n",
    "    map_training_dict[map_id]['t2_rank']   = 0\n",
    "\n",
    "    # Get who started CT\n",
    "    # for t in range(0,2):\n",
    "    #     for p in range(0,5):\n",
    "    #         for metric in metric_vec:\n",
    "    #             map_training_dict[map_id][\n",
    "    #                 't_'+str(t)+'_p_'+str(p)+'_'+metric] = {}\n",
    "\n",
    "    with open('/home/emmanuel/Desktop/csgo-csv/json_maps/hltv_map_'+str(map_id)+'.json') as f:\n",
    "\n",
    "        data = json.load(f)\n",
    "        id_ct_start = data['roundHistory'][0]['ctTeam']\n",
    "\n",
    "    team_1  = df_map.at[map_id,'team_id_1']\n",
    "    rank_1  = df_map.at[map_id,'team_rank_1']\n",
    "    score_1 = df_map.at[map_id,'team_score_1']\n",
    "\n",
    "    team_2  = df_map.at[map_id,'team_id_2']\n",
    "    rank_2  = df_map.at[map_id,'team_rank_2']\n",
    "    score_2 = df_map.at[map_id,'team_score_2']\n",
    "\n",
    "    if team_1 != id_ct_start:\n",
    "        # We swap them\n",
    "        team_aux  = team_1\n",
    "        rank_aux  = rank_1\n",
    "        score_aux = score_1\n",
    "\n",
    "        team_1  = team_2\n",
    "        rank_1  = rank_2\n",
    "        score_1 = score_2\n",
    "\n",
    "        team_2  = team_aux\n",
    "        rank_2  = rank_aux\n",
    "        score_2 = score_aux\n",
    "\n",
    "    map_training_dict[map_id]['score_dif'] = score_1-score_2\n",
    "    map_training_dict[map_id]['t1_rank']   = rank_1\n",
    "    map_training_dict[map_id]['t2_rank']   = rank_2\n",
    "    \n",
    "\n",
    "    if score_1 > score_2:\n",
    "        map_training_dict[map_id]['t1_win'] = 1\n",
    "    else:\n",
    "        map_training_dict[map_id]['t1_win'] = 0\n",
    "\n",
    "\n",
    "    if map_training_dict[map_id]['t1_rank'] < map_training_dict[map_id]['t2_rank']:\n",
    "        map_training_dict[map_id]['fav_ind'] = 1\n",
    "    else:\n",
    "        map_training_dict[map_id]['fav_ind'] = 2\n",
    "\n",
    "\n",
    "    if ( (map_training_dict[map_id]['score_dif'] > 0) & (map_training_dict[map_id]['t1_rank']<map_training_dict[map_id]['t2_rank']) )|( (map_training_dict[map_id]['score_dif'] < 0) & (map_training_dict[map_id]['t2_rank']<map_training_dict[map_id]['t1_rank']) ):\n",
    "        map_training_dict[map_id]['fav_win'] = 1\n",
    "\n",
    "\n",
    "    # Now we fill the player statistics -----------------------------------------#\n",
    "    map_date = df_map_ranked.loc[map_id]['date']\n",
    "\n",
    "    # Here we are taking the rankings to be non-zero\n",
    "    team_vec = [team_1, team_2]\n",
    "    \n",
    "    for ind in range(0,2):\n",
    "\n",
    "        team_id = team_vec[ind]\n",
    "\n",
    "        rating_vec    = []\n",
    "        prize_rtg_vec = []\n",
    "        hs_vec        = []\n",
    "        kills_per_rd_vec  = []\n",
    "        deaths_per_rd_vec = []\n",
    "        adr_vec = []\n",
    "        kast_vec = []\n",
    "        assists_per_rd_vec = []\n",
    "        flash_per_rd_vec   = []\n",
    "        first_kills_dif_vec = []\n",
    "        team_rank_vec = []\n",
    "        score_dif_vec = []\n",
    "        win_rate_vec = []\n",
    "        win_rate_map_vec = []\n",
    "\n",
    "        kd_per_round_vec  = []\n",
    "\n",
    "        scaled_win_vec = []\n",
    "        scaled_rating_vec = [] \n",
    "        scaled_score_dif_vec = []\n",
    "        scaled_kd_vec = []\n",
    "\n",
    "        momentum_vec = []\n",
    "        map_rating_vec = []\n",
    "\n",
    "        for player_id in dict_map[map_id][team_id]['players_id']:            \n",
    "            \n",
    "            # Get the data for this player\n",
    "            df_aux   = dict_player[player_id]\n",
    "            date_vec = (map_date-df_aux['date']).astype('timedelta64[D]')\n",
    "\n",
    "            # Note the prize rating we take a whole year back\n",
    "            prize = df_aux[ (date_vec>1) & (date_vec<365) ]['prize'].sum()\n",
    "            if prize > 0.0:\n",
    "                prize_rtg_vec.append( np.log( prize )/12.0 )\n",
    "            else:\n",
    "                prize_rtg_vec.append( 0.0 )\n",
    "\n",
    "\n",
    "            df_aux   = df_aux[ (date_vec>1) & (date_vec<DAYS_WEIGHT) ]\n",
    "\n",
    "            # What if we use only the historical data for this map in particular\n",
    "            # df_aux = df_aux[ (date_vec>1) & (date_vec<DAYS_WEIGHT) & (df_aux['map']==df_map_ranked.at[map_id,'map']) ]\n",
    "\n",
    "            # Append the average values of this player \n",
    "            rating_vec.append         ( df_aux['rating'].mean() )\n",
    "            hs_vec.append             ( (df_aux['hs_kills']/(df_aux['team_score']+df_aux['op_score'])).mean() )        \n",
    "            kills_per_rd_vec.append   ( df_aux['kills_per_round'].mean() )\n",
    "            deaths_per_rd_vec.append  ( df_aux['deaths_per_round'].mean() )\n",
    "            adr_vec.append            ( df_aux['ADR'].mean() )\n",
    "            kast_vec.append           ( df_aux['KAST'].mean() )\n",
    "            assists_per_rd_vec.append ( (df_aux['assists']/(df_aux['team_score']+df_aux['op_score'])).mean() )\n",
    "            flash_per_rd_vec.append   ( (df_aux['flash_assists']/(df_aux['team_score']+df_aux['op_score'])).mean() )\n",
    "            first_kills_dif_vec.append( (df_aux['first_kills_diff']/(df_aux['team_score']+df_aux['op_score'])).mean() )\n",
    "            team_rank_vec.append      ( df_aux['team_rank'].mean() )\n",
    "            score_dif_vec.append      ( (df_aux['team_score']-df_aux['op_score']).mean() )\n",
    "            win_rate_vec.append       ( df_aux['win'].mean() )\n",
    "\n",
    "            \n",
    "\n",
    "            kd_per_round_vec.append  ( (df_aux['kills_per_round']-df_aux['deaths_per_round']).mean() )\n",
    "\n",
    "            # Get the opponent rank\n",
    "            df_player_maps = df_map.loc[df_aux.index.values]\n",
    "            op_rank_vec    = (df_aux['team_id']!=df_player_maps['team_id_1'])*df_player_maps['team_rank_1'] + (df_aux['team_id']!=df_player_maps['team_id_2'])*df_player_maps['team_rank_2']\n",
    "\n",
    "            # Scaled Variables\n",
    "            \n",
    "            # Linear scaling\n",
    "            # alpha = 0.1\n",
    "            # op_rank_vec = alpha + (1.0-alpha)*(420.0-op_rank_vec)/420.0\n",
    "            \n",
    "            # scaled_win_vec.append       ( (df_aux['win']*op_rank_vec).mean() )\n",
    "            # scaled_rating_vec.append    ( (df_aux['rating']*op_rank_vec).mean() )\n",
    "            # scaled_score_dif_vec.append ( ((df_aux['team_score']-df_aux['op_score'])*op_rank_vec).mean() )\n",
    "\n",
    "            scaled_win_vec.append       ( (df_aux['win']/op_rank_vec).mean() )\n",
    "            scaled_rating_vec.append    ( (df_aux['rating']/op_rank_vec).mean() )\n",
    "            scaled_score_dif_vec.append ( ((df_aux['team_score']-df_aux['op_score'])/op_rank_vec).mean() )\n",
    "\n",
    "            scaled_kd_vec.append ( ((df_aux['kills_per_round']-df_aux['deaths_per_round'])/op_rank_vec).mean() )\n",
    "\n",
    "            momentum_vec.append ( df_aux['win'].head(n=15).mean() )\n",
    "\n",
    "            df_aux_map = df_aux[ df_aux['map']==df_map_ranked.at[map_id,'map'] ]\n",
    "            map_rating_vec.append( (df_aux_map['team_score']-df_aux_map['op_score']).mean() )\n",
    "            win_rate_map_vec.append ( (df_aux_map['win']/op_rank_vec).mean() )\n",
    "\n",
    "\n",
    "        t = str(ind)\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,prize_rtg_vec,'prize_rating',map_training_dict[map_id])\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,rating_vec,'rating',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,hs_vec,'hs_perc',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,kills_per_rd_vec,'kills_per_rd',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,deaths_per_rd_vec,'deaths_per_rd',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,adr_vec,'adr',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,kast_vec,'kast',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,assists_per_rd_vec,'assists_per_rd',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,flash_per_rd_vec,'flash_per_rd',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,first_kills_dif_vec,'first_kills_dif',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,team_rank_vec,'team_rank',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,score_dif_vec,'score_dif',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,win_rate_vec,'win_rate',map_training_dict[map_id])\n",
    "\n",
    "        map_training_dict[map_id] = append_val(t,win_rate_map_vec,'win_rate_map',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,kd_per_round_vec,'kd_per_round',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,scaled_win_vec,'scaled_win',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,scaled_rating_vec,'scaled_rating',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,scaled_score_dif_vec,'scaled_score_dif',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,scaled_kd_vec,'scaled_kd',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,momentum_vec,'momentum',map_training_dict[map_id])\n",
    "        map_training_dict[map_id] = append_val(t,map_rating_vec,'map_rating',map_training_dict[map_id])\n",
    "    \n",
    "print('Converting data to DataFrame')\n",
    "df_ct_start = pd.DataFrame.from_dict( map_training_dict,orient='index')\n",
    "time_2 = datetime.now()\n",
    "\n",
    "print( time_2-time_1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final_train_df.sav'\n",
    "pickle.dump( df_ct_start, open(filename,'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t1_win</th>\n      <th>fav_win</th>\n      <th>fav_ind</th>\n      <th>map</th>\n      <th>score_dif</th>\n      <th>t1_rank</th>\n      <th>t2_rank</th>\n      <th>t_0_p_0_prize_rating</th>\n      <th>t_0_p_1_prize_rating</th>\n      <th>t_0_p_2_prize_rating</th>\n      <th>...</th>\n      <th>t_1_p_0_scaled_score_dif</th>\n      <th>t_1_p_1_scaled_score_dif</th>\n      <th>t_1_p_2_scaled_score_dif</th>\n      <th>t_1_p_3_scaled_score_dif</th>\n      <th>t_1_p_4_scaled_score_dif</th>\n      <th>t_1_p_0_scaled_kd</th>\n      <th>t_1_p_1_scaled_kd</th>\n      <th>t_1_p_2_scaled_kd</th>\n      <th>t_1_p_3_scaled_kd</th>\n      <th>t_1_p_4_scaled_kd</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>92838</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>mrg</td>\n      <td>-6</td>\n      <td>19</td>\n      <td>8</td>\n      <td>0.854439</td>\n      <td>0.852957</td>\n      <td>0.834175</td>\n      <td>...</td>\n      <td>-0.104174</td>\n      <td>-0.104174</td>\n      <td>-0.104174</td>\n      <td>-0.104174</td>\n      <td>-0.104174</td>\n      <td>0.004472</td>\n      <td>0.001304</td>\n      <td>-0.002880</td>\n      <td>-0.007629</td>\n      <td>-0.010502</td>\n    </tr>\n    <tr>\n      <th>92845</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>nuke</td>\n      <td>-6</td>\n      <td>1</td>\n      <td>28</td>\n      <td>1.032929</td>\n      <td>1.032929</td>\n      <td>1.032929</td>\n      <td>...</td>\n      <td>0.026481</td>\n      <td>-0.162239</td>\n      <td>-0.162239</td>\n      <td>-0.162239</td>\n      <td>-0.162239</td>\n      <td>0.005567</td>\n      <td>0.001263</td>\n      <td>-0.005819</td>\n      <td>-0.005870</td>\n      <td>-0.016589</td>\n    </tr>\n    <tr>\n      <th>92846</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>nuke</td>\n      <td>3</td>\n      <td>8</td>\n      <td>19</td>\n      <td>0.930477</td>\n      <td>0.898823</td>\n      <td>0.898823</td>\n      <td>...</td>\n      <td>-0.573629</td>\n      <td>-0.586402</td>\n      <td>-0.608132</td>\n      <td>-0.608132</td>\n      <td>-0.608132</td>\n      <td>0.001080</td>\n      <td>-0.022336</td>\n      <td>-0.027359</td>\n      <td>-0.028091</td>\n      <td>-0.028422</td>\n    </tr>\n    <tr>\n      <th>92855</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>d2</td>\n      <td>10</td>\n      <td>25</td>\n      <td>35</td>\n      <td>0.810288</td>\n      <td>0.810288</td>\n      <td>0.810288</td>\n      <td>...</td>\n      <td>0.009982</td>\n      <td>0.009182</td>\n      <td>0.008159</td>\n      <td>0.008159</td>\n      <td>0.008159</td>\n      <td>0.002185</td>\n      <td>0.000116</td>\n      <td>-0.000172</td>\n      <td>-0.000673</td>\n      <td>-0.001232</td>\n    </tr>\n    <tr>\n      <th>92856</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>vertigo</td>\n      <td>-11</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0.962391</td>\n      <td>0.960731</td>\n      <td>0.960731</td>\n      <td>...</td>\n      <td>0.236011</td>\n      <td>0.236011</td>\n      <td>0.236011</td>\n      <td>0.236011</td>\n      <td>0.236011</td>\n      <td>0.024258</td>\n      <td>0.014221</td>\n      <td>0.004867</td>\n      <td>-0.001036</td>\n      <td>-0.017150</td>\n    </tr>\n    <tr>\n      <th>92860</th>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>ovp</td>\n      <td>2</td>\n      <td>23</td>\n      <td>13</td>\n      <td>0.910906</td>\n      <td>0.910906</td>\n      <td>0.910906</td>\n      <td>...</td>\n      <td>0.226649</td>\n      <td>0.226649</td>\n      <td>0.226649</td>\n      <td>0.226649</td>\n      <td>-0.174910</td>\n      <td>0.017286</td>\n      <td>0.015257</td>\n      <td>0.001157</td>\n      <td>0.000660</td>\n      <td>-0.003709</td>\n    </tr>\n    <tr>\n      <th>92868</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>d2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1.063768</td>\n      <td>1.063768</td>\n      <td>1.063768</td>\n      <td>...</td>\n      <td>-0.160592</td>\n      <td>-0.747932</td>\n      <td>-0.747932</td>\n      <td>-0.747932</td>\n      <td>-0.747932</td>\n      <td>0.017874</td>\n      <td>0.011226</td>\n      <td>-0.021122</td>\n      <td>-0.045754</td>\n      <td>-0.049187</td>\n    </tr>\n    <tr>\n      <th>92873</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>trn</td>\n      <td>-2</td>\n      <td>30</td>\n      <td>49</td>\n      <td>0.894170</td>\n      <td>0.813129</td>\n      <td>0.774684</td>\n      <td>...</td>\n      <td>0.027949</td>\n      <td>0.027949</td>\n      <td>0.026007</td>\n      <td>0.026007</td>\n      <td>-0.040171</td>\n      <td>0.002331</td>\n      <td>0.000786</td>\n      <td>0.000098</td>\n      <td>-0.002182</td>\n      <td>-0.003897</td>\n    </tr>\n    <tr>\n      <th>92875</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>inf</td>\n      <td>-11</td>\n      <td>13</td>\n      <td>23</td>\n      <td>0.925763</td>\n      <td>0.924979</td>\n      <td>0.924979</td>\n      <td>...</td>\n      <td>0.062518</td>\n      <td>0.062518</td>\n      <td>0.062518</td>\n      <td>-0.057540</td>\n      <td>-0.287111</td>\n      <td>0.009369</td>\n      <td>0.007487</td>\n      <td>-0.002798</td>\n      <td>-0.003668</td>\n      <td>-0.021964</td>\n    </tr>\n    <tr>\n      <th>92878</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>ovp</td>\n      <td>-8</td>\n      <td>17</td>\n      <td>10</td>\n      <td>0.921959</td>\n      <td>0.921959</td>\n      <td>0.921959</td>\n      <td>...</td>\n      <td>-0.376298</td>\n      <td>-0.376298</td>\n      <td>-0.376298</td>\n      <td>-0.376298</td>\n      <td>-1.467185</td>\n      <td>0.015798</td>\n      <td>0.013924</td>\n      <td>0.002607</td>\n      <td>-0.004257</td>\n      <td>-0.041581</td>\n    </tr>\n    <tr>\n      <th>92880</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>inf</td>\n      <td>-6</td>\n      <td>49</td>\n      <td>30</td>\n      <td>0.965904</td>\n      <td>0.964151</td>\n      <td>0.793351</td>\n      <td>...</td>\n      <td>0.008224</td>\n      <td>-0.012128</td>\n      <td>-0.044622</td>\n      <td>-0.044818</td>\n      <td>-0.044818</td>\n      <td>0.002289</td>\n      <td>0.000578</td>\n      <td>0.000152</td>\n      <td>-0.001813</td>\n      <td>-0.003450</td>\n    </tr>\n    <tr>\n      <th>92881</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>d2</td>\n      <td>-14</td>\n      <td>28</td>\n      <td>8</td>\n      <td>0.810495</td>\n      <td>0.810495</td>\n      <td>0.810495</td>\n      <td>...</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>0.003818</td>\n      <td>0.000995</td>\n      <td>-0.002446</td>\n      <td>-0.007852</td>\n      <td>-0.009963</td>\n    </tr>\n    <tr>\n      <th>92883</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>inf</td>\n      <td>3</td>\n      <td>10</td>\n      <td>17</td>\n      <td>0.974488</td>\n      <td>0.973790</td>\n      <td>0.973790</td>\n      <td>...</td>\n      <td>0.254121</td>\n      <td>0.005015</td>\n      <td>-0.073845</td>\n      <td>-0.073845</td>\n      <td>-0.073845</td>\n      <td>0.012696</td>\n      <td>0.004571</td>\n      <td>0.001422</td>\n      <td>-0.000607</td>\n      <td>-0.006693</td>\n    </tr>\n    <tr>\n      <th>92884</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>trn</td>\n      <td>-2</td>\n      <td>24</td>\n      <td>35</td>\n      <td>0.811009</td>\n      <td>0.788964</td>\n      <td>0.788344</td>\n      <td>...</td>\n      <td>0.001799</td>\n      <td>0.001107</td>\n      <td>0.000282</td>\n      <td>0.000282</td>\n      <td>0.000282</td>\n      <td>0.002005</td>\n      <td>-0.000053</td>\n      <td>-0.000413</td>\n      <td>-0.000917</td>\n      <td>-0.001395</td>\n    </tr>\n    <tr>\n      <th>92885</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>d2</td>\n      <td>-4</td>\n      <td>30</td>\n      <td>49</td>\n      <td>0.894170</td>\n      <td>0.813129</td>\n      <td>0.774684</td>\n      <td>...</td>\n      <td>0.027949</td>\n      <td>0.027949</td>\n      <td>0.026007</td>\n      <td>0.026007</td>\n      <td>-0.040171</td>\n      <td>0.002331</td>\n      <td>0.000786</td>\n      <td>0.000098</td>\n      <td>-0.002182</td>\n      <td>-0.003897</td>\n    </tr>\n    <tr>\n      <th>92887</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>inf</td>\n      <td>-5</td>\n      <td>8</td>\n      <td>28</td>\n      <td>0.930477</td>\n      <td>0.898823</td>\n      <td>0.898823</td>\n      <td>...</td>\n      <td>0.099574</td>\n      <td>-0.098644</td>\n      <td>-0.098644</td>\n      <td>-0.098644</td>\n      <td>-0.098644</td>\n      <td>0.009913</td>\n      <td>0.004549</td>\n      <td>-0.000354</td>\n      <td>-0.008737</td>\n      <td>-0.016101</td>\n    </tr>\n    <tr>\n      <th>92888</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>d2</td>\n      <td>-2</td>\n      <td>19</td>\n      <td>21</td>\n      <td>0.920716</td>\n      <td>0.841084</td>\n      <td>0.831741</td>\n      <td>...</td>\n      <td>0.051476</td>\n      <td>0.005999</td>\n      <td>-0.002113</td>\n      <td>-0.070051</td>\n      <td>-0.070051</td>\n      <td>0.009136</td>\n      <td>0.001202</td>\n      <td>0.000671</td>\n      <td>-0.000732</td>\n      <td>-0.012684</td>\n    </tr>\n    <tr>\n      <th>92889</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>mrg</td>\n      <td>-11</td>\n      <td>28</td>\n      <td>8</td>\n      <td>0.810495</td>\n      <td>0.810495</td>\n      <td>0.810495</td>\n      <td>...</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>-0.110326</td>\n      <td>0.003818</td>\n      <td>0.000995</td>\n      <td>-0.002446</td>\n      <td>-0.007852</td>\n      <td>-0.009963</td>\n    </tr>\n    <tr>\n      <th>92892</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>mrg</td>\n      <td>-2</td>\n      <td>19</td>\n      <td>21</td>\n      <td>0.920716</td>\n      <td>0.841084</td>\n      <td>0.831741</td>\n      <td>...</td>\n      <td>0.051476</td>\n      <td>0.005999</td>\n      <td>-0.002113</td>\n      <td>-0.070051</td>\n      <td>-0.070051</td>\n      <td>0.009136</td>\n      <td>0.001202</td>\n      <td>0.000671</td>\n      <td>-0.000732</td>\n      <td>-0.012684</td>\n    </tr>\n    <tr>\n      <th>92899</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>trn</td>\n      <td>-4</td>\n      <td>26</td>\n      <td>37</td>\n      <td>0.751052</td>\n      <td>0.751052</td>\n      <td>0.751052</td>\n      <td>...</td>\n      <td>0.012881</td>\n      <td>0.012142</td>\n      <td>0.012142</td>\n      <td>-0.056612</td>\n      <td>-0.511936</td>\n      <td>0.003121</td>\n      <td>0.000714</td>\n      <td>0.000556</td>\n      <td>-0.000020</td>\n      <td>-0.016166</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows Ã— 197 columns</p>\n</div>",
      "text/plain": "       t1_win  fav_win  fav_ind      map  score_dif  t1_rank  t2_rank  \\\n92838       0        1        2      mrg         -6       19        8   \n92845       0        0        1     nuke         -6        1       28   \n92846       1        1        1     nuke          3        8       19   \n92855       1        1        1       d2         10       25       35   \n92856       0        1        2  vertigo        -11        5        2   \n92860       1        0        2      ovp          2       23       13   \n92868       1        1        1       d2          2        2        5   \n92873       0        0        1      trn         -2       30       49   \n92875       0        0        1      inf        -11       13       23   \n92878       0        1        2      ovp         -8       17       10   \n92880       0        1        2      inf         -6       49       30   \n92881       0        1        2       d2        -14       28        8   \n92883       1        1        1      inf          3       10       17   \n92884       0        0        1      trn         -2       24       35   \n92885       0        0        1       d2         -4       30       49   \n92887       0        0        1      inf         -5        8       28   \n92888       0        0        1       d2         -2       19       21   \n92889       0        1        2      mrg        -11       28        8   \n92892       0        0        1      mrg         -2       19       21   \n92899       0        0        1      trn         -4       26       37   \n\n       t_0_p_0_prize_rating  t_0_p_1_prize_rating  t_0_p_2_prize_rating  ...  \\\n92838              0.854439              0.852957              0.834175  ...   \n92845              1.032929              1.032929              1.032929  ...   \n92846              0.930477              0.898823              0.898823  ...   \n92855              0.810288              0.810288              0.810288  ...   \n92856              0.962391              0.960731              0.960731  ...   \n92860              0.910906              0.910906              0.910906  ...   \n92868              1.063768              1.063768              1.063768  ...   \n92873              0.894170              0.813129              0.774684  ...   \n92875              0.925763              0.924979              0.924979  ...   \n92878              0.921959              0.921959              0.921959  ...   \n92880              0.965904              0.964151              0.793351  ...   \n92881              0.810495              0.810495              0.810495  ...   \n92883              0.974488              0.973790              0.973790  ...   \n92884              0.811009              0.788964              0.788344  ...   \n92885              0.894170              0.813129              0.774684  ...   \n92887              0.930477              0.898823              0.898823  ...   \n92888              0.920716              0.841084              0.831741  ...   \n92889              0.810495              0.810495              0.810495  ...   \n92892              0.920716              0.841084              0.831741  ...   \n92899              0.751052              0.751052              0.751052  ...   \n\n       t_1_p_0_scaled_score_dif  t_1_p_1_scaled_score_dif  \\\n92838                 -0.104174                 -0.104174   \n92845                  0.026481                 -0.162239   \n92846                 -0.573629                 -0.586402   \n92855                  0.009982                  0.009182   \n92856                  0.236011                  0.236011   \n92860                  0.226649                  0.226649   \n92868                 -0.160592                 -0.747932   \n92873                  0.027949                  0.027949   \n92875                  0.062518                  0.062518   \n92878                 -0.376298                 -0.376298   \n92880                  0.008224                 -0.012128   \n92881                 -0.110326                 -0.110326   \n92883                  0.254121                  0.005015   \n92884                  0.001799                  0.001107   \n92885                  0.027949                  0.027949   \n92887                  0.099574                 -0.098644   \n92888                  0.051476                  0.005999   \n92889                 -0.110326                 -0.110326   \n92892                  0.051476                  0.005999   \n92899                  0.012881                  0.012142   \n\n       t_1_p_2_scaled_score_dif  t_1_p_3_scaled_score_dif  \\\n92838                 -0.104174                 -0.104174   \n92845                 -0.162239                 -0.162239   \n92846                 -0.608132                 -0.608132   \n92855                  0.008159                  0.008159   \n92856                  0.236011                  0.236011   \n92860                  0.226649                  0.226649   \n92868                 -0.747932                 -0.747932   \n92873                  0.026007                  0.026007   \n92875                  0.062518                 -0.057540   \n92878                 -0.376298                 -0.376298   \n92880                 -0.044622                 -0.044818   \n92881                 -0.110326                 -0.110326   \n92883                 -0.073845                 -0.073845   \n92884                  0.000282                  0.000282   \n92885                  0.026007                  0.026007   \n92887                 -0.098644                 -0.098644   \n92888                 -0.002113                 -0.070051   \n92889                 -0.110326                 -0.110326   \n92892                 -0.002113                 -0.070051   \n92899                  0.012142                 -0.056612   \n\n       t_1_p_4_scaled_score_dif  t_1_p_0_scaled_kd  t_1_p_1_scaled_kd  \\\n92838                 -0.104174           0.004472           0.001304   \n92845                 -0.162239           0.005567           0.001263   \n92846                 -0.608132           0.001080          -0.022336   \n92855                  0.008159           0.002185           0.000116   \n92856                  0.236011           0.024258           0.014221   \n92860                 -0.174910           0.017286           0.015257   \n92868                 -0.747932           0.017874           0.011226   \n92873                 -0.040171           0.002331           0.000786   \n92875                 -0.287111           0.009369           0.007487   \n92878                 -1.467185           0.015798           0.013924   \n92880                 -0.044818           0.002289           0.000578   \n92881                 -0.110326           0.003818           0.000995   \n92883                 -0.073845           0.012696           0.004571   \n92884                  0.000282           0.002005          -0.000053   \n92885                 -0.040171           0.002331           0.000786   \n92887                 -0.098644           0.009913           0.004549   \n92888                 -0.070051           0.009136           0.001202   \n92889                 -0.110326           0.003818           0.000995   \n92892                 -0.070051           0.009136           0.001202   \n92899                 -0.511936           0.003121           0.000714   \n\n       t_1_p_2_scaled_kd  t_1_p_3_scaled_kd  t_1_p_4_scaled_kd  \n92838          -0.002880          -0.007629          -0.010502  \n92845          -0.005819          -0.005870          -0.016589  \n92846          -0.027359          -0.028091          -0.028422  \n92855          -0.000172          -0.000673          -0.001232  \n92856           0.004867          -0.001036          -0.017150  \n92860           0.001157           0.000660          -0.003709  \n92868          -0.021122          -0.045754          -0.049187  \n92873           0.000098          -0.002182          -0.003897  \n92875          -0.002798          -0.003668          -0.021964  \n92878           0.002607          -0.004257          -0.041581  \n92880           0.000152          -0.001813          -0.003450  \n92881          -0.002446          -0.007852          -0.009963  \n92883           0.001422          -0.000607          -0.006693  \n92884          -0.000413          -0.000917          -0.001395  \n92885           0.000098          -0.002182          -0.003897  \n92887          -0.000354          -0.008737          -0.016101  \n92888           0.000671          -0.000732          -0.012684  \n92889          -0.002446          -0.007852          -0.009963  \n92892           0.000671          -0.000732          -0.012684  \n92899           0.000556          -0.000020          -0.016166  \n\n[20 rows x 197 columns]"
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feat = pd.DataFrame.from_dict(map_training_dict,orient='index')\n",
    "\n",
    "df_all_feat = df_all_feat.fillna(0)\n",
    "\n",
    "# filename = 'df_all_feat_40000.sav'\n",
    "# pickle.dump( df_all_feat, open(filename,'wb') )\n",
    "\n",
    "df_all_feat.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.5008789163478441\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fav_win</th>\n      <th>t1_win</th>\n      <th>fav_ind</th>\n      <th>prize_rating_dif</th>\n      <th>rating_dif</th>\n      <th>hs_perc_dif</th>\n      <th>kills_per_rd_dif</th>\n      <th>deaths_per_rd_dif</th>\n      <th>adr_dif</th>\n      <th>kast_dif</th>\n      <th>...</th>\n      <th>scaled_score_dif_dif</th>\n      <th>win_rate_map_dif</th>\n      <th>kd_per_round_dif</th>\n      <th>scaled_kd_dif</th>\n      <th>momentum_dif</th>\n      <th>map_rating_dif</th>\n      <th>tier_1</th>\n      <th>tier_2</th>\n      <th>fav_ind_2</th>\n      <th>fav_ind_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97490</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.703403</td>\n      <td>-0.348008</td>\n      <td>-0.425264</td>\n      <td>-0.416823</td>\n      <td>0.214299</td>\n      <td>-0.376182</td>\n      <td>-0.254745</td>\n      <td>...</td>\n      <td>-1.124069</td>\n      <td>-0.126876</td>\n      <td>-0.541530</td>\n      <td>-0.874358</td>\n      <td>-1.353951</td>\n      <td>-1.084605</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97491</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>1.343076</td>\n      <td>2.005799</td>\n      <td>0.646436</td>\n      <td>1.535165</td>\n      <td>1.321096</td>\n      <td>1.654750</td>\n      <td>2.972975</td>\n      <td>...</td>\n      <td>0.053010</td>\n      <td>-0.002953</td>\n      <td>0.504626</td>\n      <td>0.025537</td>\n      <td>2.080160</td>\n      <td>-0.470765</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97492</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.255373</td>\n      <td>0.010410</td>\n      <td>-0.169967</td>\n      <td>0.017783</td>\n      <td>-0.082043</td>\n      <td>-0.083886</td>\n      <td>0.005859</td>\n      <td>...</td>\n      <td>-0.770420</td>\n      <td>0.006047</td>\n      <td>0.074568</td>\n      <td>-0.689170</td>\n      <td>-1.846296</td>\n      <td>-0.973705</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97493</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.703403</td>\n      <td>0.348008</td>\n      <td>0.425264</td>\n      <td>0.416823</td>\n      <td>-0.214299</td>\n      <td>0.376182</td>\n      <td>0.254745</td>\n      <td>...</td>\n      <td>1.124069</td>\n      <td>-0.379142</td>\n      <td>0.541530</td>\n      <td>0.874358</td>\n      <td>1.353951</td>\n      <td>-1.892290</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97494</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.255373</td>\n      <td>-0.010410</td>\n      <td>0.169967</td>\n      <td>-0.017783</td>\n      <td>0.082043</td>\n      <td>0.083886</td>\n      <td>-0.005859</td>\n      <td>...</td>\n      <td>0.770420</td>\n      <td>0.101901</td>\n      <td>-0.074568</td>\n      <td>0.689170</td>\n      <td>1.846296</td>\n      <td>0.354458</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97495</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.970588</td>\n      <td>0.168361</td>\n      <td>-0.103084</td>\n      <td>0.014974</td>\n      <td>-0.523512</td>\n      <td>0.006196</td>\n      <td>0.153213</td>\n      <td>...</td>\n      <td>0.527784</td>\n      <td>-0.224120</td>\n      <td>0.383600</td>\n      <td>0.272026</td>\n      <td>-1.200093</td>\n      <td>-1.079990</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97496</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.970588</td>\n      <td>-0.168361</td>\n      <td>0.103084</td>\n      <td>-0.014974</td>\n      <td>0.523512</td>\n      <td>-0.006196</td>\n      <td>-0.153213</td>\n      <td>...</td>\n      <td>-0.527784</td>\n      <td>0.283226</td>\n      <td>-0.383600</td>\n      <td>-0.272026</td>\n      <td>1.200093</td>\n      <td>1.199989</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97497</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.693781</td>\n      <td>0.018406</td>\n      <td>-0.026862</td>\n      <td>0.029209</td>\n      <td>0.035314</td>\n      <td>-0.036594</td>\n      <td>0.041797</td>\n      <td>...</td>\n      <td>0.531486</td>\n      <td>-0.272841</td>\n      <td>0.002416</td>\n      <td>0.576347</td>\n      <td>-1.144704</td>\n      <td>0.663215</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97498</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.771150</td>\n      <td>-0.803193</td>\n      <td>-0.494025</td>\n      <td>-0.387243</td>\n      <td>0.735595</td>\n      <td>-0.533368</td>\n      <td>-0.543970</td>\n      <td>...</td>\n      <td>-0.461861</td>\n      <td>0.172086</td>\n      <td>-0.881853</td>\n      <td>-0.300319</td>\n      <td>-0.640049</td>\n      <td>-1.964203</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97501</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.264992</td>\n      <td>-0.410980</td>\n      <td>0.347706</td>\n      <td>-0.215865</td>\n      <td>0.624402</td>\n      <td>-0.281998</td>\n      <td>-0.114748</td>\n      <td>...</td>\n      <td>-1.710859</td>\n      <td>1.203514</td>\n      <td>-0.642904</td>\n      <td>-1.274541</td>\n      <td>-1.200093</td>\n      <td>-3.026346</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97502</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.642939</td>\n      <td>0.724167</td>\n      <td>0.588465</td>\n      <td>0.342473</td>\n      <td>-0.820158</td>\n      <td>0.171508</td>\n      <td>0.650176</td>\n      <td>...</td>\n      <td>0.236444</td>\n      <td>-0.210784</td>\n      <td>0.899636</td>\n      <td>0.084383</td>\n      <td>1.581074</td>\n      <td>0.117691</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97503</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>1.021586</td>\n      <td>-0.288187</td>\n      <td>0.188016</td>\n      <td>-0.078390</td>\n      <td>0.612461</td>\n      <td>0.324453</td>\n      <td>-0.605446</td>\n      <td>...</td>\n      <td>0.181880</td>\n      <td>0.119229</td>\n      <td>-0.505766</td>\n      <td>0.226516</td>\n      <td>-1.731043</td>\n      <td>-2.018751</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97504</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.771150</td>\n      <td>0.803193</td>\n      <td>0.494025</td>\n      <td>0.387243</td>\n      <td>-0.735595</td>\n      <td>0.533368</td>\n      <td>0.543970</td>\n      <td>...</td>\n      <td>0.461861</td>\n      <td>-0.665881</td>\n      <td>0.881853</td>\n      <td>0.300319</td>\n      <td>0.640049</td>\n      <td>0.913838</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97506</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.264992</td>\n      <td>0.410980</td>\n      <td>-0.347706</td>\n      <td>0.215865</td>\n      <td>-0.624402</td>\n      <td>0.281998</td>\n      <td>0.114748</td>\n      <td>...</td>\n      <td>1.710859</td>\n      <td>-0.048629</td>\n      <td>0.642904</td>\n      <td>1.274541</td>\n      <td>1.200093</td>\n      <td>0.384494</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97507</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.534258</td>\n      <td>-0.487020</td>\n      <td>-0.229087</td>\n      <td>-0.363300</td>\n      <td>0.135014</td>\n      <td>-0.211192</td>\n      <td>-0.516925</td>\n      <td>...</td>\n      <td>-1.324828</td>\n      <td>0.532710</td>\n      <td>-0.435448</td>\n      <td>-0.698285</td>\n      <td>-1.680130</td>\n      <td>-0.201132</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97508</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-1.990059</td>\n      <td>0.339997</td>\n      <td>0.140693</td>\n      <td>0.242682</td>\n      <td>-0.352651</td>\n      <td>0.110891</td>\n      <td>0.325350</td>\n      <td>...</td>\n      <td>0.072093</td>\n      <td>-0.012094</td>\n      <td>0.476165</td>\n      <td>0.079838</td>\n      <td>0.698236</td>\n      <td>-0.284053</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97510</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.965587</td>\n      <td>0.224792</td>\n      <td>0.418576</td>\n      <td>0.077965</td>\n      <td>-0.207953</td>\n      <td>0.017104</td>\n      <td>0.436578</td>\n      <td>...</td>\n      <td>0.409916</td>\n      <td>0.236192</td>\n      <td>0.219801</td>\n      <td>0.201064</td>\n      <td>-0.480037</td>\n      <td>2.057520</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97511</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.220462</td>\n      <td>0.111585</td>\n      <td>0.293026</td>\n      <td>0.269131</td>\n      <td>0.387246</td>\n      <td>0.373988</td>\n      <td>-0.125272</td>\n      <td>...</td>\n      <td>-0.000647</td>\n      <td>-0.046242</td>\n      <td>-0.021412</td>\n      <td>-0.115335</td>\n      <td>0.160012</td>\n      <td>-0.258459</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97513</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.771150</td>\n      <td>0.803193</td>\n      <td>0.494025</td>\n      <td>0.387243</td>\n      <td>-0.735595</td>\n      <td>0.533368</td>\n      <td>0.543970</td>\n      <td>...</td>\n      <td>0.461861</td>\n      <td>-1.788999</td>\n      <td>0.881853</td>\n      <td>0.300319</td>\n      <td>0.640049</td>\n      <td>1.416910</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97515</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-1.021586</td>\n      <td>0.288187</td>\n      <td>-0.188016</td>\n      <td>0.078390</td>\n      <td>-0.612461</td>\n      <td>-0.324453</td>\n      <td>0.605446</td>\n      <td>...</td>\n      <td>-0.181880</td>\n      <td>-0.748435</td>\n      <td>0.505766</td>\n      <td>-0.226516</td>\n      <td>1.731043</td>\n      <td>0.339029</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97516</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.236261</td>\n      <td>-0.637298</td>\n      <td>-0.328874</td>\n      <td>-0.370155</td>\n      <td>0.944889</td>\n      <td>-0.382549</td>\n      <td>-0.301429</td>\n      <td>...</td>\n      <td>-1.488833</td>\n      <td>-0.259815</td>\n      <td>-1.013608</td>\n      <td>-1.302067</td>\n      <td>-1.920148</td>\n      <td>-3.507660</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97517</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.799881</td>\n      <td>-0.556012</td>\n      <td>0.156693</td>\n      <td>-0.232206</td>\n      <td>0.380123</td>\n      <td>-0.433129</td>\n      <td>-0.319243</td>\n      <td>...</td>\n      <td>-0.715729</td>\n      <td>0.660977</td>\n      <td>-0.485751</td>\n      <td>-0.290537</td>\n      <td>0.320025</td>\n      <td>-1.103848</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97518</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.721858</td>\n      <td>-0.150643</td>\n      <td>-0.186307</td>\n      <td>-0.305466</td>\n      <td>0.045970</td>\n      <td>-0.088524</td>\n      <td>-0.036318</td>\n      <td>...</td>\n      <td>1.546883</td>\n      <td>0.011189</td>\n      <td>-0.318440</td>\n      <td>1.655063</td>\n      <td>-0.100008</td>\n      <td>-1.040430</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97520</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.721858</td>\n      <td>0.150643</td>\n      <td>0.186307</td>\n      <td>0.305466</td>\n      <td>-0.045970</td>\n      <td>0.088524</td>\n      <td>0.036318</td>\n      <td>...</td>\n      <td>-1.546883</td>\n      <td>-1.528949</td>\n      <td>0.318440</td>\n      <td>-1.655063</td>\n      <td>0.100008</td>\n      <td>0.173145</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97521</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.236261</td>\n      <td>0.637298</td>\n      <td>0.328874</td>\n      <td>0.370155</td>\n      <td>-0.944889</td>\n      <td>0.382549</td>\n      <td>0.301429</td>\n      <td>...</td>\n      <td>1.488833</td>\n      <td>-0.280894</td>\n      <td>1.013608</td>\n      <td>1.302067</td>\n      <td>1.920148</td>\n      <td>-0.489226</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97522</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.799881</td>\n      <td>0.556012</td>\n      <td>-0.156693</td>\n      <td>0.232206</td>\n      <td>-0.380123</td>\n      <td>0.433129</td>\n      <td>0.319243</td>\n      <td>...</td>\n      <td>0.715729</td>\n      <td>-0.363708</td>\n      <td>0.485751</td>\n      <td>0.290537</td>\n      <td>-0.320025</td>\n      <td>1.198143</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97524</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.799881</td>\n      <td>0.556012</td>\n      <td>-0.156693</td>\n      <td>0.232206</td>\n      <td>-0.380123</td>\n      <td>0.433129</td>\n      <td>0.319243</td>\n      <td>...</td>\n      <td>0.715729</td>\n      <td>-0.628649</td>\n      <td>0.485751</td>\n      <td>0.290537</td>\n      <td>-0.320025</td>\n      <td>0.750762</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97525</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>1.878984</td>\n      <td>2.056819</td>\n      <td>0.847695</td>\n      <td>1.564899</td>\n      <td>1.218667</td>\n      <td>1.730773</td>\n      <td>2.796954</td>\n      <td>...</td>\n      <td>-0.508277</td>\n      <td>0.308164</td>\n      <td>0.604775</td>\n      <td>-0.230817</td>\n      <td>2.880222</td>\n      <td>2.418439</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97526</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.120922</td>\n      <td>0.894619</td>\n      <td>-0.090231</td>\n      <td>0.711220</td>\n      <td>-0.989398</td>\n      <td>0.539852</td>\n      <td>0.653537</td>\n      <td>...</td>\n      <td>0.690210</td>\n      <td>0.269808</td>\n      <td>1.364345</td>\n      <td>0.585934</td>\n      <td>1.753981</td>\n      <td>1.470844</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97527</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.779105</td>\n      <td>-0.441831</td>\n      <td>-0.385912</td>\n      <td>-0.302908</td>\n      <td>0.546595</td>\n      <td>-0.234576</td>\n      <td>-0.368883</td>\n      <td>...</td>\n      <td>-0.848640</td>\n      <td>0.039446</td>\n      <td>-0.669468</td>\n      <td>-0.705324</td>\n      <td>-0.800062</td>\n      <td>0.969222</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>97554</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.771150</td>\n      <td>0.835409</td>\n      <td>0.541206</td>\n      <td>0.432202</td>\n      <td>-0.742077</td>\n      <td>0.555418</td>\n      <td>0.552543</td>\n      <td>...</td>\n      <td>0.482713</td>\n      <td>-0.212935</td>\n      <td>0.928520</td>\n      <td>0.304460</td>\n      <td>0.800062</td>\n      <td>2.148157</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97555</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.771150</td>\n      <td>0.835409</td>\n      <td>0.541206</td>\n      <td>0.432202</td>\n      <td>-0.742077</td>\n      <td>0.555418</td>\n      <td>0.552543</td>\n      <td>...</td>\n      <td>0.482713</td>\n      <td>-0.597832</td>\n      <td>0.928520</td>\n      <td>0.304460</td>\n      <td>0.800062</td>\n      <td>0.980970</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97556</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.970588</td>\n      <td>-0.168361</td>\n      <td>0.103084</td>\n      <td>-0.014974</td>\n      <td>0.523512</td>\n      <td>-0.006196</td>\n      <td>-0.153213</td>\n      <td>...</td>\n      <td>-0.527784</td>\n      <td>0.027652</td>\n      <td>-0.383600</td>\n      <td>-0.272026</td>\n      <td>1.200093</td>\n      <td>0.897223</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97557</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.970588</td>\n      <td>0.168361</td>\n      <td>-0.103084</td>\n      <td>0.014974</td>\n      <td>-0.523512</td>\n      <td>0.006196</td>\n      <td>0.153213</td>\n      <td>...</td>\n      <td>0.527784</td>\n      <td>-0.224120</td>\n      <td>0.383600</td>\n      <td>0.272026</td>\n      <td>-1.200093</td>\n      <td>-1.079990</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97558</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.191483</td>\n      <td>0.234689</td>\n      <td>0.468567</td>\n      <td>0.267070</td>\n      <td>0.014484</td>\n      <td>0.221191</td>\n      <td>0.173091</td>\n      <td>...</td>\n      <td>0.302675</td>\n      <td>-0.038073</td>\n      <td>0.239814</td>\n      <td>0.415443</td>\n      <td>2.000154</td>\n      <td>0.778015</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97559</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.191483</td>\n      <td>0.234689</td>\n      <td>0.468567</td>\n      <td>0.267070</td>\n      <td>0.014484</td>\n      <td>0.221191</td>\n      <td>0.173091</td>\n      <td>...</td>\n      <td>0.302675</td>\n      <td>0.094363</td>\n      <td>0.239814</td>\n      <td>0.415443</td>\n      <td>2.000154</td>\n      <td>-0.238459</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97560</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.134590</td>\n      <td>0.164402</td>\n      <td>-0.426393</td>\n      <td>0.054914</td>\n      <td>-0.184825</td>\n      <td>0.041766</td>\n      <td>0.182883</td>\n      <td>...</td>\n      <td>-0.503648</td>\n      <td>0.737667</td>\n      <td>0.181892</td>\n      <td>-0.156702</td>\n      <td>0.160012</td>\n      <td>-1.550755</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97561</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.618888</td>\n      <td>-0.123898</td>\n      <td>-0.095970</td>\n      <td>-0.335539</td>\n      <td>-0.224445</td>\n      <td>-0.237833</td>\n      <td>0.139508</td>\n      <td>...</td>\n      <td>-0.314967</td>\n      <td>-0.451348</td>\n      <td>-0.155692</td>\n      <td>-0.292459</td>\n      <td>-0.400031</td>\n      <td>-2.939050</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97562</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.645872</td>\n      <td>0.110319</td>\n      <td>0.261398</td>\n      <td>-0.019481</td>\n      <td>-0.242438</td>\n      <td>-0.090697</td>\n      <td>0.074936</td>\n      <td>...</td>\n      <td>0.113908</td>\n      <td>0.376945</td>\n      <td>0.152913</td>\n      <td>0.602475</td>\n      <td>1.040080</td>\n      <td>0.216262</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97563</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.796709</td>\n      <td>0.413485</td>\n      <td>0.363348</td>\n      <td>0.176396</td>\n      <td>-0.419405</td>\n      <td>0.241609</td>\n      <td>0.413611</td>\n      <td>...</td>\n      <td>0.587881</td>\n      <td>5.078858</td>\n      <td>0.461233</td>\n      <td>1.641544</td>\n      <td>1.360105</td>\n      <td>0.060659</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97564</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.618888</td>\n      <td>0.123898</td>\n      <td>0.095970</td>\n      <td>0.335539</td>\n      <td>0.224445</td>\n      <td>0.237833</td>\n      <td>-0.139508</td>\n      <td>...</td>\n      <td>0.314967</td>\n      <td>-0.593199</td>\n      <td>0.155692</td>\n      <td>0.292459</td>\n      <td>0.400031</td>\n      <td>-0.592302</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97565</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.134590</td>\n      <td>-0.164402</td>\n      <td>0.426393</td>\n      <td>-0.054914</td>\n      <td>0.184825</td>\n      <td>-0.041766</td>\n      <td>-0.182883</td>\n      <td>...</td>\n      <td>0.503648</td>\n      <td>-0.367107</td>\n      <td>-0.181892</td>\n      <td>0.156702</td>\n      <td>-0.160012</td>\n      <td>0.290767</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97566</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.645872</td>\n      <td>-0.110319</td>\n      <td>-0.261398</td>\n      <td>0.019481</td>\n      <td>0.242438</td>\n      <td>0.090697</td>\n      <td>-0.074936</td>\n      <td>...</td>\n      <td>-0.113908</td>\n      <td>-0.986500</td>\n      <td>-0.152913</td>\n      <td>-0.602475</td>\n      <td>-1.040080</td>\n      <td>-0.669225</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97567</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.796709</td>\n      <td>-0.413485</td>\n      <td>-0.363348</td>\n      <td>-0.176396</td>\n      <td>0.419405</td>\n      <td>-0.241609</td>\n      <td>-0.413611</td>\n      <td>...</td>\n      <td>-0.587881</td>\n      <td>-5.561994</td>\n      <td>-0.461233</td>\n      <td>-1.641544</td>\n      <td>-1.360105</td>\n      <td>-0.772231</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97568</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.618888</td>\n      <td>-0.123898</td>\n      <td>-0.095970</td>\n      <td>-0.335539</td>\n      <td>-0.224445</td>\n      <td>-0.237833</td>\n      <td>0.139508</td>\n      <td>...</td>\n      <td>-0.314967</td>\n      <td>-0.067071</td>\n      <td>-0.155692</td>\n      <td>-0.292459</td>\n      <td>-0.400031</td>\n      <td>-0.096503</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97569</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.645872</td>\n      <td>0.110319</td>\n      <td>0.261398</td>\n      <td>-0.019481</td>\n      <td>-0.242438</td>\n      <td>-0.090697</td>\n      <td>0.074936</td>\n      <td>...</td>\n      <td>0.113908</td>\n      <td>3.995076</td>\n      <td>0.152913</td>\n      <td>0.602475</td>\n      <td>1.040080</td>\n      <td>0.478102</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97570</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.046457</td>\n      <td>0.385664</td>\n      <td>0.602953</td>\n      <td>0.395408</td>\n      <td>-0.271281</td>\n      <td>0.294637</td>\n      <td>0.149685</td>\n      <td>...</td>\n      <td>0.431114</td>\n      <td>-0.100313</td>\n      <td>0.561708</td>\n      <td>1.337135</td>\n      <td>0.400031</td>\n      <td>-1.483503</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97571</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.224278</td>\n      <td>0.151719</td>\n      <td>-0.143636</td>\n      <td>0.116527</td>\n      <td>0.076321</td>\n      <td>0.184806</td>\n      <td>0.124418</td>\n      <td>...</td>\n      <td>0.471735</td>\n      <td>-0.170158</td>\n      <td>0.055217</td>\n      <td>0.596868</td>\n      <td>1.360105</td>\n      <td>-0.014266</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97572</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.344176</td>\n      <td>0.125378</td>\n      <td>-0.190946</td>\n      <td>-0.005436</td>\n      <td>-0.271436</td>\n      <td>-0.050516</td>\n      <td>0.001336</td>\n      <td>...</td>\n      <td>-0.385132</td>\n      <td>0.989780</td>\n      <td>0.186534</td>\n      <td>-0.067549</td>\n      <td>-0.400031</td>\n      <td>-1.602183</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97573</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.046457</td>\n      <td>-0.385664</td>\n      <td>-0.602953</td>\n      <td>-0.395408</td>\n      <td>0.271281</td>\n      <td>-0.294637</td>\n      <td>-0.149685</td>\n      <td>...</td>\n      <td>-0.431114</td>\n      <td>-3.794668</td>\n      <td>-0.561708</td>\n      <td>-1.337135</td>\n      <td>-0.400031</td>\n      <td>0.037762</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97574</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.224278</td>\n      <td>-0.151719</td>\n      <td>0.143636</td>\n      <td>-0.116527</td>\n      <td>-0.076321</td>\n      <td>-0.184806</td>\n      <td>-0.124418</td>\n      <td>...</td>\n      <td>-0.471735</td>\n      <td>-0.222668</td>\n      <td>-0.055217</td>\n      <td>-0.596868</td>\n      <td>-1.360105</td>\n      <td>-0.812037</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97575</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.167106</td>\n      <td>0.119458</td>\n      <td>-0.013971</td>\n      <td>0.007506</td>\n      <td>-0.153676</td>\n      <td>-0.025360</td>\n      <td>0.244807</td>\n      <td>...</td>\n      <td>-0.067229</td>\n      <td>0.582988</td>\n      <td>0.115517</td>\n      <td>0.471349</td>\n      <td>1.040080</td>\n      <td>-0.103216</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97579</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.344176</td>\n      <td>-0.125378</td>\n      <td>0.190946</td>\n      <td>0.005436</td>\n      <td>0.271436</td>\n      <td>0.050516</td>\n      <td>-0.001336</td>\n      <td>...</td>\n      <td>0.385132</td>\n      <td>-0.446045</td>\n      <td>-0.186534</td>\n      <td>0.067549</td>\n      <td>0.400031</td>\n      <td>-0.486797</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97580</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.046457</td>\n      <td>-0.385664</td>\n      <td>-0.602953</td>\n      <td>-0.395408</td>\n      <td>0.271281</td>\n      <td>-0.294637</td>\n      <td>-0.149685</td>\n      <td>...</td>\n      <td>-0.431114</td>\n      <td>-5.307538</td>\n      <td>-0.561708</td>\n      <td>-1.337135</td>\n      <td>-0.400031</td>\n      <td>-2.187672</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97581</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.344176</td>\n      <td>-0.125378</td>\n      <td>0.190946</td>\n      <td>0.005436</td>\n      <td>0.271436</td>\n      <td>0.050516</td>\n      <td>-0.001336</td>\n      <td>...</td>\n      <td>0.385132</td>\n      <td>-0.274861</td>\n      <td>-0.186534</td>\n      <td>0.067549</td>\n      <td>0.400031</td>\n      <td>-0.276921</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97582</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.167106</td>\n      <td>-0.119458</td>\n      <td>0.013971</td>\n      <td>-0.007506</td>\n      <td>0.153676</td>\n      <td>0.025360</td>\n      <td>-0.244807</td>\n      <td>...</td>\n      <td>0.067229</td>\n      <td>-3.729163</td>\n      <td>-0.115517</td>\n      <td>-0.471349</td>\n      <td>-1.040080</td>\n      <td>-0.585793</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97586</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.167106</td>\n      <td>-0.119458</td>\n      <td>0.013971</td>\n      <td>-0.007506</td>\n      <td>0.153676</td>\n      <td>0.025360</td>\n      <td>-0.244807</td>\n      <td>...</td>\n      <td>0.067229</td>\n      <td>-0.723486</td>\n      <td>-0.115517</td>\n      <td>-0.471349</td>\n      <td>-1.040080</td>\n      <td>0.332305</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97591</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.534889</td>\n      <td>0.288002</td>\n      <td>0.219408</td>\n      <td>0.146491</td>\n      <td>0.113026</td>\n      <td>0.237025</td>\n      <td>0.285987</td>\n      <td>...</td>\n      <td>-0.920322</td>\n      <td>-0.089541</td>\n      <td>0.057358</td>\n      <td>-0.920074</td>\n      <td>-1.120086</td>\n      <td>0.692301</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97593</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.534889</td>\n      <td>-0.288002</td>\n      <td>-0.219408</td>\n      <td>-0.146491</td>\n      <td>-0.113026</td>\n      <td>-0.237025</td>\n      <td>-0.285987</td>\n      <td>...</td>\n      <td>0.920322</td>\n      <td>0.014856</td>\n      <td>-0.057358</td>\n      <td>0.920074</td>\n      <td>1.120086</td>\n      <td>-0.847025</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97595</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.534889</td>\n      <td>0.288002</td>\n      <td>0.219408</td>\n      <td>0.146491</td>\n      <td>0.113026</td>\n      <td>0.237025</td>\n      <td>0.285987</td>\n      <td>...</td>\n      <td>-0.920322</td>\n      <td>-0.606747</td>\n      <td>0.057358</td>\n      <td>-0.920074</td>\n      <td>-1.120086</td>\n      <td>0.946926</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>80 rows Ã— 28 columns</p>\n</div>",
      "text/plain": "       fav_win  t1_win  fav_ind  prize_rating_dif  rating_dif  hs_perc_dif  \\\n97490        0       0     -1.0          0.703403   -0.348008    -0.425264   \n97491        1       0     -1.0          1.343076    2.005799     0.646436   \n97492        0       0      1.0         -0.255373    0.010410    -0.169967   \n97493        1       0      1.0         -0.703403    0.348008     0.425264   \n97494        0       1     -1.0          0.255373   -0.010410     0.169967   \n97495        1       0      1.0         -0.970588    0.168361    -0.103084   \n97496        0       0     -1.0          0.970588   -0.168361     0.103084   \n97497        1       0      1.0         -0.693781    0.018406    -0.026862   \n97498        1       1     -1.0          0.771150   -0.803193    -0.494025   \n97501        1       1     -1.0          0.264992   -0.410980     0.347706   \n97502        0       1      1.0         -0.642939    0.724167     0.588465   \n97503        0       0     -1.0          1.021586   -0.288187     0.188016   \n97504        0       1      1.0         -0.771150    0.803193     0.494025   \n97506        1       0      1.0         -0.264992    0.410980    -0.347706   \n97507        1       1     -1.0          0.534258   -0.487020    -0.229087   \n97508        1       0      1.0         -1.990059    0.339997     0.140693   \n97510        1       1     -1.0          0.965587    0.224792     0.418576   \n97511        1       0      1.0         -0.220462    0.111585     0.293026   \n97513        0       1      1.0         -0.771150    0.803193     0.494025   \n97515        0       1      1.0         -1.021586    0.288187    -0.188016   \n97516        1       1     -1.0          0.236261   -0.637298    -0.328874   \n97517        1       1     -1.0          0.799881   -0.556012     0.156693   \n97518        0       0     -1.0          0.721858   -0.150643    -0.186307   \n97520        0       1      1.0         -0.721858    0.150643     0.186307   \n97521        1       0      1.0         -0.236261    0.637298     0.328874   \n97522        0       1      1.0         -0.799881    0.556012    -0.156693   \n97524        1       0      1.0         -0.799881    0.556012    -0.156693   \n97525        1       1     -1.0          1.878984    2.056819     0.847695   \n97526        0       0     -1.0          0.120922    0.894619    -0.090231   \n97527        0       0     -1.0          0.779105   -0.441831    -0.385912   \n...        ...     ...      ...               ...         ...          ...   \n97554        0       1      1.0         -0.771150    0.835409     0.541206   \n97555        1       0      1.0         -0.771150    0.835409     0.541206   \n97556        1       1     -1.0          0.970588   -0.168361     0.103084   \n97557        1       0      1.0         -0.970588    0.168361    -0.103084   \n97558        0       0     -1.0          0.191483    0.234689     0.468567   \n97559        0       0     -1.0          0.191483    0.234689     0.468567   \n97560        0       0      1.0         -0.134590    0.164402    -0.426393   \n97561        1       1     -1.0          0.618888   -0.123898    -0.095970   \n97562        1       1     -1.0          0.645872    0.110319     0.261398   \n97563        1       1     -1.0          0.796709    0.413485     0.363348   \n97564        0       1      1.0         -0.618888    0.123898     0.095970   \n97565        0       1     -1.0          0.134590   -0.164402     0.426393   \n97566        0       1      1.0         -0.645872   -0.110319    -0.261398   \n97567        1       0      1.0         -0.796709   -0.413485    -0.363348   \n97568        1       1     -1.0          0.618888   -0.123898    -0.095970   \n97569        1       1     -1.0          0.645872    0.110319     0.261398   \n97570        1       1      1.0         -0.046457    0.385664     0.602953   \n97571        1       1     -1.0          0.224278    0.151719    -0.143636   \n97572        0       0     -1.0          0.344176    0.125378    -0.190946   \n97573        0       1     -1.0          0.046457   -0.385664    -0.602953   \n97574        1       0      1.0         -0.224278   -0.151719     0.143636   \n97575        0       0     -1.0          0.167106    0.119458    -0.013971   \n97579        1       0      1.0         -0.344176   -0.125378     0.190946   \n97580        1       0     -1.0          0.046457   -0.385664    -0.602953   \n97581        0       1      1.0         -0.344176   -0.125378     0.190946   \n97582        1       0      1.0         -0.167106   -0.119458     0.013971   \n97586        1       0      1.0         -0.167106   -0.119458     0.013971   \n97591        1       0      1.0         -0.534889    0.288002     0.219408   \n97593        0       0     -1.0          0.534889   -0.288002    -0.219408   \n97595        0       1      1.0         -0.534889    0.288002     0.219408   \n\n       kills_per_rd_dif  deaths_per_rd_dif   adr_dif  kast_dif  ...  \\\n97490         -0.416823           0.214299 -0.376182 -0.254745  ...   \n97491          1.535165           1.321096  1.654750  2.972975  ...   \n97492          0.017783          -0.082043 -0.083886  0.005859  ...   \n97493          0.416823          -0.214299  0.376182  0.254745  ...   \n97494         -0.017783           0.082043  0.083886 -0.005859  ...   \n97495          0.014974          -0.523512  0.006196  0.153213  ...   \n97496         -0.014974           0.523512 -0.006196 -0.153213  ...   \n97497          0.029209           0.035314 -0.036594  0.041797  ...   \n97498         -0.387243           0.735595 -0.533368 -0.543970  ...   \n97501         -0.215865           0.624402 -0.281998 -0.114748  ...   \n97502          0.342473          -0.820158  0.171508  0.650176  ...   \n97503         -0.078390           0.612461  0.324453 -0.605446  ...   \n97504          0.387243          -0.735595  0.533368  0.543970  ...   \n97506          0.215865          -0.624402  0.281998  0.114748  ...   \n97507         -0.363300           0.135014 -0.211192 -0.516925  ...   \n97508          0.242682          -0.352651  0.110891  0.325350  ...   \n97510          0.077965          -0.207953  0.017104  0.436578  ...   \n97511          0.269131           0.387246  0.373988 -0.125272  ...   \n97513          0.387243          -0.735595  0.533368  0.543970  ...   \n97515          0.078390          -0.612461 -0.324453  0.605446  ...   \n97516         -0.370155           0.944889 -0.382549 -0.301429  ...   \n97517         -0.232206           0.380123 -0.433129 -0.319243  ...   \n97518         -0.305466           0.045970 -0.088524 -0.036318  ...   \n97520          0.305466          -0.045970  0.088524  0.036318  ...   \n97521          0.370155          -0.944889  0.382549  0.301429  ...   \n97522          0.232206          -0.380123  0.433129  0.319243  ...   \n97524          0.232206          -0.380123  0.433129  0.319243  ...   \n97525          1.564899           1.218667  1.730773  2.796954  ...   \n97526          0.711220          -0.989398  0.539852  0.653537  ...   \n97527         -0.302908           0.546595 -0.234576 -0.368883  ...   \n...                 ...                ...       ...       ...  ...   \n97554          0.432202          -0.742077  0.555418  0.552543  ...   \n97555          0.432202          -0.742077  0.555418  0.552543  ...   \n97556         -0.014974           0.523512 -0.006196 -0.153213  ...   \n97557          0.014974          -0.523512  0.006196  0.153213  ...   \n97558          0.267070           0.014484  0.221191  0.173091  ...   \n97559          0.267070           0.014484  0.221191  0.173091  ...   \n97560          0.054914          -0.184825  0.041766  0.182883  ...   \n97561         -0.335539          -0.224445 -0.237833  0.139508  ...   \n97562         -0.019481          -0.242438 -0.090697  0.074936  ...   \n97563          0.176396          -0.419405  0.241609  0.413611  ...   \n97564          0.335539           0.224445  0.237833 -0.139508  ...   \n97565         -0.054914           0.184825 -0.041766 -0.182883  ...   \n97566          0.019481           0.242438  0.090697 -0.074936  ...   \n97567         -0.176396           0.419405 -0.241609 -0.413611  ...   \n97568         -0.335539          -0.224445 -0.237833  0.139508  ...   \n97569         -0.019481          -0.242438 -0.090697  0.074936  ...   \n97570          0.395408          -0.271281  0.294637  0.149685  ...   \n97571          0.116527           0.076321  0.184806  0.124418  ...   \n97572         -0.005436          -0.271436 -0.050516  0.001336  ...   \n97573         -0.395408           0.271281 -0.294637 -0.149685  ...   \n97574         -0.116527          -0.076321 -0.184806 -0.124418  ...   \n97575          0.007506          -0.153676 -0.025360  0.244807  ...   \n97579          0.005436           0.271436  0.050516 -0.001336  ...   \n97580         -0.395408           0.271281 -0.294637 -0.149685  ...   \n97581          0.005436           0.271436  0.050516 -0.001336  ...   \n97582         -0.007506           0.153676  0.025360 -0.244807  ...   \n97586         -0.007506           0.153676  0.025360 -0.244807  ...   \n97591          0.146491           0.113026  0.237025  0.285987  ...   \n97593         -0.146491          -0.113026 -0.237025 -0.285987  ...   \n97595          0.146491           0.113026  0.237025  0.285987  ...   \n\n       scaled_score_dif_dif  win_rate_map_dif  kd_per_round_dif  \\\n97490             -1.124069         -0.126876         -0.541530   \n97491              0.053010         -0.002953          0.504626   \n97492             -0.770420          0.006047          0.074568   \n97493              1.124069         -0.379142          0.541530   \n97494              0.770420          0.101901         -0.074568   \n97495              0.527784         -0.224120          0.383600   \n97496             -0.527784          0.283226         -0.383600   \n97497              0.531486         -0.272841          0.002416   \n97498             -0.461861          0.172086         -0.881853   \n97501             -1.710859          1.203514         -0.642904   \n97502              0.236444         -0.210784          0.899636   \n97503              0.181880          0.119229         -0.505766   \n97504              0.461861         -0.665881          0.881853   \n97506              1.710859         -0.048629          0.642904   \n97507             -1.324828          0.532710         -0.435448   \n97508              0.072093         -0.012094          0.476165   \n97510              0.409916          0.236192          0.219801   \n97511             -0.000647         -0.046242         -0.021412   \n97513              0.461861         -1.788999          0.881853   \n97515             -0.181880         -0.748435          0.505766   \n97516             -1.488833         -0.259815         -1.013608   \n97517             -0.715729          0.660977         -0.485751   \n97518              1.546883          0.011189         -0.318440   \n97520             -1.546883         -1.528949          0.318440   \n97521              1.488833         -0.280894          1.013608   \n97522              0.715729         -0.363708          0.485751   \n97524              0.715729         -0.628649          0.485751   \n97525             -0.508277          0.308164          0.604775   \n97526              0.690210          0.269808          1.364345   \n97527             -0.848640          0.039446         -0.669468   \n...                     ...               ...               ...   \n97554              0.482713         -0.212935          0.928520   \n97555              0.482713         -0.597832          0.928520   \n97556             -0.527784          0.027652         -0.383600   \n97557              0.527784         -0.224120          0.383600   \n97558              0.302675         -0.038073          0.239814   \n97559              0.302675          0.094363          0.239814   \n97560             -0.503648          0.737667          0.181892   \n97561             -0.314967         -0.451348         -0.155692   \n97562              0.113908          0.376945          0.152913   \n97563              0.587881          5.078858          0.461233   \n97564              0.314967         -0.593199          0.155692   \n97565              0.503648         -0.367107         -0.181892   \n97566             -0.113908         -0.986500         -0.152913   \n97567             -0.587881         -5.561994         -0.461233   \n97568             -0.314967         -0.067071         -0.155692   \n97569              0.113908          3.995076          0.152913   \n97570              0.431114         -0.100313          0.561708   \n97571              0.471735         -0.170158          0.055217   \n97572             -0.385132          0.989780          0.186534   \n97573             -0.431114         -3.794668         -0.561708   \n97574             -0.471735         -0.222668         -0.055217   \n97575             -0.067229          0.582988          0.115517   \n97579              0.385132         -0.446045         -0.186534   \n97580             -0.431114         -5.307538         -0.561708   \n97581              0.385132         -0.274861         -0.186534   \n97582              0.067229         -3.729163         -0.115517   \n97586              0.067229         -0.723486         -0.115517   \n97591             -0.920322         -0.089541          0.057358   \n97593              0.920322          0.014856         -0.057358   \n97595             -0.920322         -0.606747          0.057358   \n\n       scaled_kd_dif  momentum_dif  map_rating_dif  tier_1  tier_2  fav_ind_2  \\\n97490      -0.874358     -1.353951       -1.084605     2.0     4.0        1.0   \n97491       0.025537      2.080160       -0.470765     6.0     5.0       -1.0   \n97492      -0.689170     -1.846296       -0.973705     2.0     2.0       -1.0   \n97493       0.874358      1.353951       -1.892290     4.0     2.0       -1.0   \n97494       0.689170      1.846296        0.354458     2.0     2.0        1.0   \n97495       0.272026     -1.200093       -1.079990     4.0     2.0       -1.0   \n97496      -0.272026      1.200093        1.199989     2.0     4.0        1.0   \n97497       0.576347     -1.144704        0.663215     3.0     2.0       -1.0   \n97498      -0.300319     -0.640049       -1.964203     1.0     2.0        1.0   \n97501      -1.274541     -1.200093       -3.026346     1.0     2.0        1.0   \n97502       0.084383      1.581074        0.117691     6.0     2.0       -1.0   \n97503       0.226516     -1.731043       -2.018751     1.0     3.0        1.0   \n97504       0.300319      0.640049        0.913838     2.0     1.0       -1.0   \n97506       1.274541      1.200093        0.384494     2.0     1.0       -1.0   \n97507      -0.698285     -1.680130       -0.201132     1.0     3.0        1.0   \n97508       0.079838      0.698236       -0.284053     4.0     2.0       -1.0   \n97510       0.201064     -0.480037        2.057520     2.0     3.0       -1.0   \n97511      -0.115335      0.160012       -0.258459     4.0     2.0       -1.0   \n97513       0.300319      0.640049        1.416910     2.0     1.0       -1.0   \n97515      -0.226516      1.731043        0.339029     3.0     1.0       -1.0   \n97516      -1.302067     -1.920148       -3.507660     1.0     2.0        1.0   \n97517      -0.290537      0.320025       -1.103848     1.0     2.0        1.0   \n97518       1.655063     -0.100008       -1.040430     1.0     3.0        1.0   \n97520      -1.655063      0.100008        0.173145     3.0     1.0       -1.0   \n97521       1.302067      1.920148       -0.489226     2.0     1.0       -1.0   \n97522       0.290537     -0.320025        1.198143     2.0     1.0       -1.0   \n97524       0.290537     -0.320025        0.750762     2.0     1.0       -1.0   \n97525      -0.230817      2.880222        2.418439     2.0     5.0       -1.0   \n97526       0.585934      1.753981        1.470844     4.0     5.0       -1.0   \n97527      -0.705324     -0.800062        0.969222     2.0     4.0        1.0   \n...              ...           ...             ...     ...     ...        ...   \n97554       0.304460      0.800062        2.148157     2.0     1.0       -1.0   \n97555       0.304460      0.800062        0.980970     2.0     1.0       -1.0   \n97556      -0.272026      1.200093        0.897223     2.0     4.0        1.0   \n97557       0.272026     -1.200093       -1.079990     4.0     2.0       -1.0   \n97558       0.415443      2.000154        0.778015     2.0     2.0       -1.0   \n97559       0.415443      2.000154       -0.238459     2.0     2.0       -1.0   \n97560      -0.156702      0.160012       -1.550755     1.0     2.0       -1.0   \n97561      -0.292459     -0.400031       -2.939050     1.0     2.0        1.0   \n97562       0.602475      1.040080        0.216262     1.0     3.0       -1.0   \n97563       1.641544      1.360105        0.060659     1.0     3.0       -1.0   \n97564       0.292459      0.400031       -0.592302     2.0     1.0       -1.0   \n97565       0.156702     -0.160012        0.290767     2.0     1.0        1.0   \n97566      -0.602475     -1.040080       -0.669225     3.0     1.0        1.0   \n97567      -1.641544     -1.360105       -0.772231     3.0     1.0        1.0   \n97568      -0.292459     -0.400031       -0.096503     1.0     2.0        1.0   \n97569       0.602475      1.040080        0.478102     1.0     3.0       -1.0   \n97570       1.337135      0.400031       -1.483503     1.0     1.0       -1.0   \n97571       0.596868      1.360105       -0.014266     2.0     3.0       -1.0   \n97572      -0.067549     -0.400031       -1.602183     1.0     3.0       -1.0   \n97573      -1.337135     -0.400031        0.037762     1.0     1.0        1.0   \n97574      -0.596868     -1.360105       -0.812037     3.0     2.0        1.0   \n97575       0.471349      1.040080       -0.103216     1.0     2.0       -1.0   \n97579       0.067549      0.400031       -0.486797     3.0     1.0        1.0   \n97580      -1.337135     -0.400031       -2.187672     1.0     1.0        1.0   \n97581       0.067549      0.400031       -0.276921     3.0     1.0        1.0   \n97582      -0.471349     -1.040080       -0.585793     2.0     1.0        1.0   \n97586      -0.471349     -1.040080        0.332305     2.0     1.0        1.0   \n97591      -0.920074     -1.120086        0.692301     1.0     1.0       -1.0   \n97593       0.920074      1.120086       -0.847025     1.0     1.0        1.0   \n97595      -0.920074     -1.120086        0.946926     1.0     1.0       -1.0   \n\n       fav_ind_3  \n97490        1.0  \n97491       -1.0  \n97492       -1.0  \n97493       -1.0  \n97494        1.0  \n97495       -1.0  \n97496        1.0  \n97497       -1.0  \n97498        1.0  \n97501        1.0  \n97502       -1.0  \n97503        1.0  \n97504       -1.0  \n97506       -1.0  \n97507        1.0  \n97508       -1.0  \n97510       -1.0  \n97511       -1.0  \n97513       -1.0  \n97515       -1.0  \n97516        1.0  \n97517        1.0  \n97518        1.0  \n97520       -1.0  \n97521       -1.0  \n97522       -1.0  \n97524       -1.0  \n97525       -1.0  \n97526       -1.0  \n97527        1.0  \n...          ...  \n97554       -1.0  \n97555       -1.0  \n97556        1.0  \n97557       -1.0  \n97558       -1.0  \n97559       -1.0  \n97560       -1.0  \n97561        1.0  \n97562        1.0  \n97563       -1.0  \n97564       -1.0  \n97565        1.0  \n97566       -1.0  \n97567        1.0  \n97568        1.0  \n97569        1.0  \n97570       -1.0  \n97571       -1.0  \n97572        1.0  \n97573        1.0  \n97574        1.0  \n97575       -1.0  \n97579       -1.0  \n97580        1.0  \n97581       -1.0  \n97582        1.0  \n97586        1.0  \n97591       -1.0  \n97593        1.0  \n97595       -1.0  \n\n[80 rows x 28 columns]"
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do some feature engineering\n",
    "df_all_feat = pd.DataFrame.from_dict(map_training_dict,orient='index')\n",
    "\n",
    "df_all_feat = df_all_feat.fillna(0)\n",
    "\n",
    "#df_train = pd.DataFrame()\n",
    "\n",
    "df_train = df_all_feat[['fav_win','t1_win','fav_ind']].copy()\n",
    "\n",
    "feat_names = ['prize_rating','rating','hs_perc','kills_per_rd','deaths_per_rd','adr',\n",
    "                'kast','assists_per_rd','flash_per_rd','first_kills_dif','team_rank','score_dif','win_rate',\n",
    "                'scaled_win','scaled_rating','scaled_score_dif','win_rate_map','kd_per_round','scaled_kd','momentum','map_rating']\n",
    "\n",
    "dict_normalization = {}\n",
    "for feat in feat_names:\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    avg_0 = 0.0*df_all_feat['t_1_p_0_rating']\n",
    "    avg_1 = 0.0*df_all_feat['t_1_p_1_rating']\n",
    "\n",
    "    vec_both = []\n",
    "\n",
    "    for ind in range(0,5):\n",
    "        # df_train[feat+'_p_'+str(ind)+'_dif'] = df_all_feat['t_0_p_'+str(ind)+'_'+feat] - df_all_feat['t_1_p_'+str(ind)+'_'+feat]\n",
    "\n",
    "        avg_0 = avg_0 + df_all_feat['t_0_p_'+str(ind)+'_'+feat]\n",
    "        avg_1 = avg_1 + df_all_feat['t_1_p_'+str(ind)+'_'+feat]\n",
    "\n",
    "        # Normalized\n",
    "        # avg_0 = avg_0 + (df_all_feat['t_0_p_'+str(ind)+'_'+feat]-df_all_feat['t_0_p_'+str(ind)+'_'+feat].mean())/df_all_feat['t_0_p_'+str(ind)+'_'+feat].std()\n",
    "        # avg_1 = avg_1 + (df_all_feat['t_1_p_'+str(ind)+'_'+feat]-df_all_feat['t_1_p_'+str(ind)+'_'+feat].mean())/df_all_feat['t_1_p_'+str(ind)+'_'+feat].std()\n",
    "\n",
    "        vec_both.append( df_all_feat['t_0_p_'+str(ind)+'_'+feat].values ) \n",
    "        vec_both.append( df_all_feat['t_1_p_'+str(ind)+'_'+feat].values ) \n",
    "   \n",
    "    # Normalize\n",
    "    dict_normalization[feat] = {}\n",
    "    dict_normalization[feat]['mean'] = np.mean(vec_both)\n",
    "    dict_normalization[feat]['std']  = np.std(vec_both)\n",
    "\n",
    "    avg_0 = (avg_0 - np.mean(vec_both)) / np.std(vec_both)\n",
    "    avg_1 = (avg_1 - np.mean(vec_both)) / np.std(vec_both)\n",
    "\n",
    "    df_train[feat+'_dif'] = ( avg_0 - avg_1 ) / 5.0\n",
    "\n",
    "    # Take the first and last\n",
    "    # ind = 0\n",
    "    # df_train[feat+'_p_'+str(ind)+'_dif'] = df_all_feat['t_0_p_'+str(ind)+'_'+feat] - df_all_feat['t_1_p_'+str(ind)+'_'+feat]\n",
    "\n",
    "    # ind = 4\n",
    "    # df_train[feat+'_p_'+str(ind)+'_dif'] = df_all_feat['t_0_p_'+str(ind)+'_'+feat] - df_all_feat['t_1_p_'+str(ind)+'_'+feat]\n",
    "    \n",
    "# df_train['prize_rating_dif'] = np.sqrt( abs( df_train['prize_rating_dif'] )) *np.sign(df_train['prize_rating_dif']) \n",
    "\n",
    "# Does it help if we normalize everything?\n",
    "# df_train['norm_dif'] = 0.0\n",
    "\n",
    "\n",
    "# for column in df_train:\n",
    "#     if (column != 'fav_win') & (column !='t1_win') & (column !='fav_ind') & (column != 'norm_dif'):\n",
    "        \n",
    "#         df_train[column] = (df_train[column]-df_train[column].mean())/df_train[column].std()\n",
    "\n",
    "        # Try using the norm of the difference\n",
    "        # df_train['norm_dif'] = df_train['norm_dif'] + df_train[column]\n",
    "\n",
    "        # Try using a sigmoid function to exagerate the feature difference for small changes\n",
    "        # df_train[column] = np.arctan( 0.50*df_train[column] )\n",
    "\n",
    "# df_train['norm_dif'] = df_train['norm_dif'] * np.sign( df_train['rating_dif'] ) / df_train['norm_dif'].max()\n",
    "# df_train['norm_dif'] = np.sign( df_train['rating_dif'] )* (df_train['rating_dif']**2 + df_train['prize_rating_dif']**2 )\n",
    "\n",
    "df_train['tier_1'] = np.ceil( df_all_feat['t1_rank'] / 20.0 )\n",
    "df_train['tier_2'] = np.ceil( df_all_feat['t2_rank'] / 20.0 )\n",
    "\n",
    "df_train = df_train.fillna(0)\n",
    "\n",
    "\n",
    "df_train['fav_ind'] = -1.0*(df_train['prize_rating_dif']>=0.0) + 1.0*(df_train['prize_rating_dif']<0.0)\n",
    "\n",
    "df_train['fav_ind_2'] = -1.0*(df_train['rating_dif']>=0.0) + 1.0*(df_train['rating_dif']<0.0)\n",
    "\n",
    "df_train['fav_ind_3'] = -1.0*(df_train['kills_per_rd_dif']>=0.0) + 1.0*(df_train['kills_per_rd_dif']<0.0)\n",
    "\n",
    "# df_train['prize_rating_dif'] = np.sign(df_train['prize_rating_dif'])*(np.abs(df_train['prize_rating_dif'])**0.1)\n",
    "\n",
    "# df_train['scaled_win_dif'] = np.sign(df_train['scaled_win_dif'])*(np.abs(df_train['scaled_win_dif'])**2)\n",
    "\n",
    "#print(df_train.keys())\n",
    "\n",
    "print(df_train['t1_win'].mean())\n",
    "\n",
    "df_train.tail(n=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_normalization\n",
    "filename = 'dict_normalization.sav'\n",
    "pickle.dump( dict_normalization, open(filename,'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fav_win</th>\n      <th>t1_win</th>\n      <th>fav_ind</th>\n      <th>map_rating_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>92838</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.810772</td>\n    </tr>\n    <tr>\n      <th>92845</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>1.340639</td>\n    </tr>\n    <tr>\n      <th>92846</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>-0.055113</td>\n    </tr>\n    <tr>\n      <th>92855</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.047574</td>\n    </tr>\n    <tr>\n      <th>92856</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.462612</td>\n    </tr>\n    <tr>\n      <th>92860</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-0.712879</td>\n    </tr>\n    <tr>\n      <th>92868</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.634560</td>\n    </tr>\n    <tr>\n      <th>92873</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-2.254641</td>\n    </tr>\n    <tr>\n      <th>92875</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>2.075389</td>\n    </tr>\n    <tr>\n      <th>92878</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.613673</td>\n    </tr>\n    <tr>\n      <th>92880</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.471019</td>\n    </tr>\n    <tr>\n      <th>92881</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.040758</td>\n    </tr>\n    <tr>\n      <th>92883</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>1.133198</td>\n    </tr>\n    <tr>\n      <th>92884</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.376336</td>\n    </tr>\n    <tr>\n      <th>92885</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.985262</td>\n    </tr>\n    <tr>\n      <th>92887</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.529861</td>\n    </tr>\n    <tr>\n      <th>92888</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-1.585390</td>\n    </tr>\n    <tr>\n      <th>92889</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.738730</td>\n    </tr>\n    <tr>\n      <th>92892</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.190331</td>\n    </tr>\n    <tr>\n      <th>92899</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2.895303</td>\n    </tr>\n    <tr>\n      <th>92901</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.572040</td>\n    </tr>\n    <tr>\n      <th>92919</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.315261</td>\n    </tr>\n    <tr>\n      <th>92921</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.641090</td>\n    </tr>\n    <tr>\n      <th>92922</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.055270</td>\n    </tr>\n    <tr>\n      <th>92926</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>1.996390</td>\n    </tr>\n    <tr>\n      <th>92927</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.143308</td>\n    </tr>\n    <tr>\n      <th>92931</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-1.193533</td>\n    </tr>\n    <tr>\n      <th>92933</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>0.312082</td>\n    </tr>\n    <tr>\n      <th>92936</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.537423</td>\n    </tr>\n    <tr>\n      <th>92938</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.250564</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>93188</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.367379</td>\n    </tr>\n    <tr>\n      <th>93190</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.680072</td>\n    </tr>\n    <tr>\n      <th>93192</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.472651</td>\n    </tr>\n    <tr>\n      <th>93194</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.822891</td>\n    </tr>\n    <tr>\n      <th>93195</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>-0.861858</td>\n    </tr>\n    <tr>\n      <th>93197</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-3.343187</td>\n    </tr>\n    <tr>\n      <th>93199</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.783172</td>\n    </tr>\n    <tr>\n      <th>93202</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.741423</td>\n    </tr>\n    <tr>\n      <th>93205</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.894713</td>\n    </tr>\n    <tr>\n      <th>93219</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-1.907797</td>\n    </tr>\n    <tr>\n      <th>93221</th>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>-0.815719</td>\n    </tr>\n    <tr>\n      <th>93222</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.549974</td>\n    </tr>\n    <tr>\n      <th>93223</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>1.000610</td>\n    </tr>\n    <tr>\n      <th>93224</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>-2.931727</td>\n    </tr>\n    <tr>\n      <th>93227</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.558413</td>\n    </tr>\n    <tr>\n      <th>93231</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.128152</td>\n    </tr>\n    <tr>\n      <th>93232</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.959282</td>\n    </tr>\n    <tr>\n      <th>93233</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.403972</td>\n    </tr>\n    <tr>\n      <th>93234</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.064911</td>\n    </tr>\n    <tr>\n      <th>93236</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>0.337628</td>\n    </tr>\n    <tr>\n      <th>93237</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.144763</td>\n    </tr>\n    <tr>\n      <th>93239</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>-1.508545</td>\n    </tr>\n    <tr>\n      <th>93240</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.153564</td>\n    </tr>\n    <tr>\n      <th>93241</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.249217</td>\n    </tr>\n    <tr>\n      <th>93242</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-2.870814</td>\n    </tr>\n    <tr>\n      <th>93243</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0</td>\n      <td>2.378578</td>\n    </tr>\n    <tr>\n      <th>93246</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.573548</td>\n    </tr>\n    <tr>\n      <th>93247</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-2.106820</td>\n    </tr>\n    <tr>\n      <th>93251</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-2.233881</td>\n    </tr>\n    <tr>\n      <th>93262</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.0</td>\n      <td>-0.464523</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 4 columns</p>\n</div>",
      "text/plain": "       fav_win  t1_win  fav_ind  map_rating_dif\n92838        1       0      1.0       -0.810772\n92845        0       0     -1.0        1.340639\n92846        1       1     -1.0       -0.055113\n92855        1       1     -1.0        0.047574\n92856        1       0      1.0        0.462612\n92860        0       1      1.0       -0.712879\n92868        1       1     -1.0        0.634560\n92873        0       0      1.0       -2.254641\n92875        0       0     -1.0        2.075389\n92878        1       0      1.0        0.613673\n92880        1       0     -1.0       -0.471019\n92881        1       0      1.0       -0.040758\n92883        1       1     -1.0        1.133198\n92884        0       0     -1.0       -0.376336\n92885        0       0      1.0        0.985262\n92887        0       0     -1.0       -0.529861\n92888        0       0     -1.0       -1.585390\n92889        1       0      1.0        1.738730\n92892        0       0     -1.0        0.190331\n92899        0       0      1.0        2.895303\n92901        0       0      1.0        1.572040\n92919        0       1      1.0        2.315261\n92921        1       0      1.0        1.641090\n92922        1       0      1.0       -0.055270\n92926        0       0     -1.0        1.996390\n92927        0       1      1.0        0.143308\n92931        0       1      1.0       -1.193533\n92933        1       1     -1.0        0.312082\n92936        0       1      1.0        0.537423\n92938        0       0     -1.0       -0.250564\n...        ...     ...      ...             ...\n93188        1       1      1.0        2.367379\n93190        1       0      1.0       -0.680072\n93192        1       1      1.0        0.472651\n93194        1       0     -1.0        0.822891\n93195        0       1     -1.0       -0.861858\n93197        0       0      1.0       -3.343187\n93199        1       0     -1.0       -0.783172\n93202        0       0     -1.0        0.741423\n93205        1       0     -1.0       -0.894713\n93219        1       0      1.0       -1.907797\n93221        0       1     -1.0       -0.815719\n93222        0       0     -1.0       -0.549974\n93223        1       1     -1.0        1.000610\n93224        1       1     -1.0       -2.931727\n93227        1       1      1.0        0.558413\n93231        0       1      1.0        0.128152\n93232        0       0      1.0        0.959282\n93233        1       0      1.0       -0.403972\n93234        1       0      1.0        1.064911\n93236        0       0     -1.0        0.337628\n93237        0       0     -1.0       -0.144763\n93239        0       1      1.0       -1.508545\n93240        0       0     -1.0       -0.153564\n93241        0       0     -1.0       -0.249217\n93242        1       0      1.0       -2.870814\n93243        1       1     -1.0        2.378578\n93246        0       1      1.0        1.573548\n93247        1       0      1.0       -2.106820\n93251        0       0     -1.0       -2.233881\n93262        1       0     -1.0       -0.464523\n\n[100 rows x 4 columns]"
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['fav_win', 't1_win', 'fav_ind', 'map_rating_dif']].head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fav_win</th>\n      <th>t1_win</th>\n      <th>fav_ind</th>\n      <th>prize_rating_dif</th>\n      <th>rating_dif</th>\n      <th>hs_perc_dif</th>\n      <th>kills_per_rd_dif</th>\n      <th>deaths_per_rd_dif</th>\n      <th>adr_dif</th>\n      <th>kast_dif</th>\n      <th>assists_per_rd_dif</th>\n      <th>flash_per_rd_dif</th>\n      <th>first_kills_dif_dif</th>\n      <th>team_rank_dif</th>\n      <th>score_dif_dif</th>\n      <th>win_rate_dif</th>\n      <th>scaled_win_dif</th>\n      <th>scaled_rating_dif</th>\n      <th>scaled_score_dif_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n      <td>1.000000e+03</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.605000</td>\n      <td>0.495000</td>\n      <td>1.506000</td>\n      <td>-5.329071e-18</td>\n      <td>3.552714e-18</td>\n      <td>1.776357e-18</td>\n      <td>1.065814e-17</td>\n      <td>3.552714e-18</td>\n      <td>7.105427e-18</td>\n      <td>-1.065814e-17</td>\n      <td>-4.440892e-18</td>\n      <td>1.332268e-17</td>\n      <td>1.065814e-17</td>\n      <td>-7.105427e-18</td>\n      <td>3.552714e-17</td>\n      <td>-3.197442e-17</td>\n      <td>3.552714e-18</td>\n      <td>-3.552714e-18</td>\n      <td>-2.309264e-17</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.489095</td>\n      <td>0.500225</td>\n      <td>0.500214</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-4.294535e+00</td>\n      <td>-7.087553e+00</td>\n      <td>-6.435974e+00</td>\n      <td>-7.735193e+00</td>\n      <td>-7.817805e+00</td>\n      <td>-8.118688e+00</td>\n      <td>-8.099462e+00</td>\n      <td>-5.048495e+00</td>\n      <td>-6.284287e+00</td>\n      <td>-4.760382e+00</td>\n      <td>-3.583056e+00</td>\n      <td>-4.217007e+00</td>\n      <td>-3.724805e+00</td>\n      <td>-1.128112e+01</td>\n      <td>-5.958851e+00</td>\n      <td>-7.928005e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-4.468085e-01</td>\n      <td>-3.924654e-01</td>\n      <td>-4.381664e-01</td>\n      <td>-3.243540e-01</td>\n      <td>-3.263356e-01</td>\n      <td>-2.815156e-01</td>\n      <td>-2.678898e-01</td>\n      <td>-5.383833e-01</td>\n      <td>-5.687058e-01</td>\n      <td>-5.859129e-01</td>\n      <td>-4.851744e-01</td>\n      <td>-5.956345e-01</td>\n      <td>-6.036420e-01</td>\n      <td>-1.568888e-01</td>\n      <td>-2.703883e-01</td>\n      <td>-1.762917e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.314647e-03</td>\n      <td>3.299395e-02</td>\n      <td>2.559003e-02</td>\n      <td>4.188134e-02</td>\n      <td>4.807448e-02</td>\n      <td>3.512834e-02</td>\n      <td>6.239436e-02</td>\n      <td>1.682523e-02</td>\n      <td>1.543661e-02</td>\n      <td>-2.020498e-02</td>\n      <td>1.221722e-02</td>\n      <td>2.430884e-02</td>\n      <td>3.373862e-03</td>\n      <td>7.291137e-02</td>\n      <td>-1.931123e-03</td>\n      <td>1.044269e-01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>4.447292e-01</td>\n      <td>4.738720e-01</td>\n      <td>5.452574e-01</td>\n      <td>4.213216e-01</td>\n      <td>4.289761e-01</td>\n      <td>3.676461e-01</td>\n      <td>3.678864e-01</td>\n      <td>5.776503e-01</td>\n      <td>5.976279e-01</td>\n      <td>6.196910e-01</td>\n      <td>4.602681e-01</td>\n      <td>5.966732e-01</td>\n      <td>6.204561e-01</td>\n      <td>2.741010e-01</td>\n      <td>2.377506e-01</td>\n      <td>3.057342e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>4.291147e+00</td>\n      <td>6.331305e+00</td>\n      <td>6.316330e+00</td>\n      <td>7.090528e+00</td>\n      <td>7.788433e+00</td>\n      <td>7.684703e+00</td>\n      <td>7.808019e+00</td>\n      <td>5.311427e+00</td>\n      <td>6.438761e+00</td>\n      <td>4.460250e+00</td>\n      <td>4.320149e+00</td>\n      <td>3.464001e+00</td>\n      <td>4.228936e+00</td>\n      <td>4.603757e+00</td>\n      <td>5.245539e+00</td>\n      <td>8.470654e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "           fav_win       t1_win      fav_ind  prize_rating_dif    rating_dif  \\\ncount  1000.000000  1000.000000  1000.000000      1.000000e+03  1.000000e+03   \nmean      0.605000     0.495000     1.506000     -5.329071e-18  3.552714e-18   \nstd       0.489095     0.500225     0.500214      1.000000e+00  1.000000e+00   \nmin       0.000000     0.000000     1.000000     -4.294535e+00 -7.087553e+00   \n25%       0.000000     0.000000     1.000000     -4.468085e-01 -3.924654e-01   \n50%       1.000000     0.000000     2.000000      1.314647e-03  3.299395e-02   \n75%       1.000000     1.000000     2.000000      4.447292e-01  4.738720e-01   \nmax       1.000000     1.000000     2.000000      4.291147e+00  6.331305e+00   \n\n        hs_perc_dif  kills_per_rd_dif  deaths_per_rd_dif       adr_dif  \\\ncount  1.000000e+03      1.000000e+03       1.000000e+03  1.000000e+03   \nmean   1.776357e-18      1.065814e-17       3.552714e-18  7.105427e-18   \nstd    1.000000e+00      1.000000e+00       1.000000e+00  1.000000e+00   \nmin   -6.435974e+00     -7.735193e+00      -7.817805e+00 -8.118688e+00   \n25%   -4.381664e-01     -3.243540e-01      -3.263356e-01 -2.815156e-01   \n50%    2.559003e-02      4.188134e-02       4.807448e-02  3.512834e-02   \n75%    5.452574e-01      4.213216e-01       4.289761e-01  3.676461e-01   \nmax    6.316330e+00      7.090528e+00       7.788433e+00  7.684703e+00   \n\n           kast_dif  assists_per_rd_dif  flash_per_rd_dif  \\\ncount  1.000000e+03        1.000000e+03      1.000000e+03   \nmean  -1.065814e-17       -4.440892e-18      1.332268e-17   \nstd    1.000000e+00        1.000000e+00      1.000000e+00   \nmin   -8.099462e+00       -5.048495e+00     -6.284287e+00   \n25%   -2.678898e-01       -5.383833e-01     -5.687058e-01   \n50%    6.239436e-02        1.682523e-02      1.543661e-02   \n75%    3.678864e-01        5.776503e-01      5.976279e-01   \nmax    7.808019e+00        5.311427e+00      6.438761e+00   \n\n       first_kills_dif_dif  team_rank_dif  score_dif_dif  win_rate_dif  \\\ncount         1.000000e+03   1.000000e+03   1.000000e+03  1.000000e+03   \nmean          1.065814e-17  -7.105427e-18   3.552714e-17 -3.197442e-17   \nstd           1.000000e+00   1.000000e+00   1.000000e+00  1.000000e+00   \nmin          -4.760382e+00  -3.583056e+00  -4.217007e+00 -3.724805e+00   \n25%          -5.859129e-01  -4.851744e-01  -5.956345e-01 -6.036420e-01   \n50%          -2.020498e-02   1.221722e-02   2.430884e-02  3.373862e-03   \n75%           6.196910e-01   4.602681e-01   5.966732e-01  6.204561e-01   \nmax           4.460250e+00   4.320149e+00   3.464001e+00  4.228936e+00   \n\n       scaled_win_dif  scaled_rating_dif  scaled_score_dif_dif  \ncount    1.000000e+03       1.000000e+03          1.000000e+03  \nmean     3.552714e-18      -3.552714e-18         -2.309264e-17  \nstd      1.000000e+00       1.000000e+00          1.000000e+00  \nmin     -1.128112e+01      -5.958851e+00         -7.928005e+00  \n25%     -1.568888e-01      -2.703883e-01         -1.762917e-01  \n50%      7.291137e-02      -1.931123e-03          1.044269e-01  \n75%      2.741010e-01       2.377506e-01          3.057342e-01  \nmax      4.603757e+00       5.245539e+00          8.470654e+00  "
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame.describe()\n",
    "df_train.describe()\n",
    "\n",
    "# %matplotlib qt\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "# sns.set(font_scale=3)\n",
    "\n",
    "# plt.figure(figsize=(30,20));\n",
    "# ax = plt.axes()\n",
    "\n",
    "# df_train.hist(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prize_rating_dif</th>\n      <th>scaled_win_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>495.000000</td>\n      <td>495.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.227814</td>\n      <td>0.156660</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.972410</td>\n      <td>0.921393</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-4.294535</td>\n      <td>-6.860687</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.252996</td>\n      <td>-0.037509</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.177084</td>\n      <td>0.134580</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.584404</td>\n      <td>0.352129</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.114808</td>\n      <td>4.603757</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       prize_rating_dif  scaled_win_dif\ncount        495.000000      495.000000\nmean           0.227814        0.156660\nstd            0.972410        0.921393\nmin           -4.294535       -6.860687\n25%           -0.252996       -0.037509\n50%            0.177084        0.134580\n75%            0.584404        0.352129\nmax            4.114808        4.603757"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['t1_win']==1][['prize_rating_dif','scaled_win_dif']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prize_rating_dif</th>\n      <th>scaled_win_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>505.000000</td>\n      <td>505.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-0.223303</td>\n      <td>-0.153558</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.976928</td>\n      <td>1.049901</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-4.294535</td>\n      <td>-11.281118</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.638892</td>\n      <td>-0.259403</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.178711</td>\n      <td>0.002556</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.245448</td>\n      <td>0.183122</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.291147</td>\n      <td>3.242652</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       prize_rating_dif  scaled_win_dif\ncount        505.000000      505.000000\nmean          -0.223303       -0.153558\nstd            0.976928        1.049901\nmin           -4.294535      -11.281118\n25%           -0.638892       -0.259403\n50%           -0.178711        0.002556\n75%            0.245448        0.183122\nmax            4.291147        3.242652"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['t1_win']==0][['prize_rating_dif','scaled_win_dif']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x7ff1cb782f60>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = df_train.drop(['fav_win'],axis=1).corr()\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=1)\n",
    "plt.figure(figsize=(20,16));\n",
    "ax = plt.axes()\n",
    "\n",
    "sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70}, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t1_win</th>\n      <th>fav_win</th>\n      <th>fav_ind</th>\n      <th>map</th>\n      <th>score_dif</th>\n      <th>t1_rank</th>\n      <th>t2_rank</th>\n      <th>t_0_p_0_prize_rating</th>\n      <th>t_0_p_1_prize_rating</th>\n      <th>t_0_p_2_prize_rating</th>\n      <th>...</th>\n      <th>t_1_p_0_scaled_rating</th>\n      <th>t_1_p_1_scaled_rating</th>\n      <th>t_1_p_2_scaled_rating</th>\n      <th>t_1_p_3_scaled_rating</th>\n      <th>t_1_p_4_scaled_rating</th>\n      <th>t_1_p_0_scaled_score_dif</th>\n      <th>t_1_p_1_scaled_score_dif</th>\n      <th>t_1_p_2_scaled_score_dif</th>\n      <th>t_1_p_3_scaled_score_dif</th>\n      <th>t_1_p_4_scaled_score_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>96641</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>trn</td>\n      <td>-10</td>\n      <td>97</td>\n      <td>41</td>\n      <td>0.663651</td>\n      <td>0.629862</td>\n      <td>0.569225</td>\n      <td>...</td>\n      <td>0.043875</td>\n      <td>0.039537</td>\n      <td>0.034187</td>\n      <td>0.032402</td>\n      <td>0.029116</td>\n      <td>-0.098653</td>\n      <td>-0.102610</td>\n      <td>-0.102610</td>\n      <td>-0.102906</td>\n      <td>-0.102906</td>\n    </tr>\n    <tr>\n      <th>96639</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>inf</td>\n      <td>8</td>\n      <td>41</td>\n      <td>97</td>\n      <td>0.721413</td>\n      <td>0.709766</td>\n      <td>0.709069</td>\n      <td>...</td>\n      <td>0.097023</td>\n      <td>0.008072</td>\n      <td>0.007778</td>\n      <td>0.007525</td>\n      <td>0.006096</td>\n      <td>-0.016848</td>\n      <td>-0.021450</td>\n      <td>-0.021450</td>\n      <td>-0.022641</td>\n      <td>-0.820086</td>\n    </tr>\n    <tr>\n      <th>96638</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>mrg</td>\n      <td>8</td>\n      <td>35</td>\n      <td>48</td>\n      <td>0.818040</td>\n      <td>0.799803</td>\n      <td>0.799803</td>\n      <td>...</td>\n      <td>0.026508</td>\n      <td>0.022743</td>\n      <td>0.022248</td>\n      <td>0.021855</td>\n      <td>0.019667</td>\n      <td>-0.024231</td>\n      <td>-0.024263</td>\n      <td>-0.024263</td>\n      <td>-0.025765</td>\n      <td>-0.033227</td>\n    </tr>\n    <tr>\n      <th>96634</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>trn</td>\n      <td>-2</td>\n      <td>48</td>\n      <td>35</td>\n      <td>0.726537</td>\n      <td>0.726537</td>\n      <td>0.720326</td>\n      <td>...</td>\n      <td>0.063923</td>\n      <td>0.055480</td>\n      <td>0.054788</td>\n      <td>0.052385</td>\n      <td>0.028128</td>\n      <td>-0.077340</td>\n      <td>-0.136681</td>\n      <td>-0.136681</td>\n      <td>-0.136681</td>\n      <td>-0.139161</td>\n    </tr>\n    <tr>\n      <th>96624</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>nuke</td>\n      <td>-3</td>\n      <td>84</td>\n      <td>35</td>\n      <td>0.699892</td>\n      <td>0.697520</td>\n      <td>0.697520</td>\n      <td>...</td>\n      <td>0.063923</td>\n      <td>0.055480</td>\n      <td>0.054788</td>\n      <td>0.052385</td>\n      <td>0.028128</td>\n      <td>-0.077340</td>\n      <td>-0.136681</td>\n      <td>-0.136681</td>\n      <td>-0.136681</td>\n      <td>-0.139161</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 167 columns</p>\n</div>",
      "text/plain": "       t1_win  fav_win  fav_ind   map  score_dif  t1_rank  t2_rank  \\\n96641       0        1        2   trn        -10       97       41   \n96639       1        1        1   inf          8       41       97   \n96638       1        1        1   mrg          8       35       48   \n96634       0        1        2   trn         -2       48       35   \n96624       0        1        2  nuke         -3       84       35   \n\n       t_0_p_0_prize_rating  t_0_p_1_prize_rating  t_0_p_2_prize_rating  ...  \\\n96641              0.663651              0.629862              0.569225  ...   \n96639              0.721413              0.709766              0.709069  ...   \n96638              0.818040              0.799803              0.799803  ...   \n96634              0.726537              0.726537              0.720326  ...   \n96624              0.699892              0.697520              0.697520  ...   \n\n       t_1_p_0_scaled_rating  t_1_p_1_scaled_rating  t_1_p_2_scaled_rating  \\\n96641               0.043875               0.039537               0.034187   \n96639               0.097023               0.008072               0.007778   \n96638               0.026508               0.022743               0.022248   \n96634               0.063923               0.055480               0.054788   \n96624               0.063923               0.055480               0.054788   \n\n       t_1_p_3_scaled_rating  t_1_p_4_scaled_rating  t_1_p_0_scaled_score_dif  \\\n96641               0.032402               0.029116                 -0.098653   \n96639               0.007525               0.006096                 -0.016848   \n96638               0.021855               0.019667                 -0.024231   \n96634               0.052385               0.028128                 -0.077340   \n96624               0.052385               0.028128                 -0.077340   \n\n       t_1_p_1_scaled_score_dif  t_1_p_2_scaled_score_dif  \\\n96641                 -0.102610                 -0.102610   \n96639                 -0.021450                 -0.021450   \n96638                 -0.024263                 -0.024263   \n96634                 -0.136681                 -0.136681   \n96624                 -0.136681                 -0.136681   \n\n       t_1_p_3_scaled_score_dif  t_1_p_4_scaled_score_dif  \n96641                 -0.102906                 -0.102906  \n96639                 -0.022641                 -0.820086  \n96638                 -0.025765                 -0.033227  \n96634                 -0.136681                 -0.139161  \n96624                 -0.136681                 -0.139161  \n\n[5 rows x 167 columns]"
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ct_start = pd.DataFrame.from_dict( map_training_dict,orient='index')\n",
    "len( df_ct_start )\n",
    "\n",
    "# type(data['roundHistory'][0]['ctTeam'])\n",
    "df_ct_start.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.49171270718232046\n0.574\n"
    }
   ],
   "source": [
    "df_ct_start.tail(n=20)\n",
    "print(df_ct_start[df_ct_start['map']=='inf']['t1_win'].mean())\n",
    "#df_ct_start['t1_rank'].max()\n",
    "print(df_ct_start['fav_win'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fav_win</th>\n      <th>tier_1</th>\n      <th>tier_2</th>\n      <th>team_rank_dif</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>95444</th>\n      <td>1</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>-0.444945</td>\n    </tr>\n    <tr>\n      <th>95448</th>\n      <td>0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>0.582442</td>\n    </tr>\n    <tr>\n      <th>95451</th>\n      <td>0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>-0.336810</td>\n    </tr>\n    <tr>\n      <th>95454</th>\n      <td>1</td>\n      <td>9.0</td>\n      <td>12.0</td>\n      <td>0.041625</td>\n    </tr>\n    <tr>\n      <th>95455</th>\n      <td>1</td>\n      <td>12.0</td>\n      <td>9.0</td>\n      <td>0.209711</td>\n    </tr>\n    <tr>\n      <th>95457</th>\n      <td>0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>0.158154</td>\n    </tr>\n    <tr>\n      <th>95458</th>\n      <td>0</td>\n      <td>10.0</td>\n      <td>9.0</td>\n      <td>0.949587</td>\n    </tr>\n    <tr>\n      <th>95463</th>\n      <td>0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>-0.318071</td>\n    </tr>\n    <tr>\n      <th>95466</th>\n      <td>0</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>-0.996637</td>\n    </tr>\n    <tr>\n      <th>95488</th>\n      <td>0</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>-1.530299</td>\n    </tr>\n    <tr>\n      <th>95489</th>\n      <td>1</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>-0.785422</td>\n    </tr>\n    <tr>\n      <th>95490</th>\n      <td>1</td>\n      <td>8.0</td>\n      <td>6.0</td>\n      <td>1.384378</td>\n    </tr>\n    <tr>\n      <th>95491</th>\n      <td>1</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>-0.203632</td>\n    </tr>\n    <tr>\n      <th>95492</th>\n      <td>1</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>-1.184397</td>\n    </tr>\n    <tr>\n      <th>95493</th>\n      <td>1</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>-1.045390</td>\n    </tr>\n    <tr>\n      <th>95494</th>\n      <td>0</td>\n      <td>8.0</td>\n      <td>5.0</td>\n      <td>-0.277107</td>\n    </tr>\n    <tr>\n      <th>95495</th>\n      <td>0</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>-1.989885</td>\n    </tr>\n    <tr>\n      <th>95496</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>1.006644</td>\n    </tr>\n    <tr>\n      <th>95507</th>\n      <td>1</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>-0.333311</td>\n    </tr>\n    <tr>\n      <th>95509</th>\n      <td>0</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>0.569866</td>\n    </tr>\n    <tr>\n      <th>95512</th>\n      <td>1</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>-0.315236</td>\n    </tr>\n    <tr>\n      <th>95514</th>\n      <td>1</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>-1.777143</td>\n    </tr>\n    <tr>\n      <th>95516</th>\n      <td>0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>0.433163</td>\n    </tr>\n    <tr>\n      <th>95520</th>\n      <td>0</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>-0.087839</td>\n    </tr>\n    <tr>\n      <th>95521</th>\n      <td>1</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>-0.261706</td>\n    </tr>\n    <tr>\n      <th>95525</th>\n      <td>0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>0.152257</td>\n    </tr>\n    <tr>\n      <th>95529</th>\n      <td>1</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>0.086812</td>\n    </tr>\n    <tr>\n      <th>95531</th>\n      <td>1</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>0.122923</td>\n    </tr>\n    <tr>\n      <th>95534</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.053074</td>\n    </tr>\n    <tr>\n      <th>95535</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.037637</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       fav_win  tier_1  tier_2  team_rank_dif\n95444        1     6.0     7.0      -0.444945\n95448        0     5.0     3.0       0.582442\n95451        0     3.0     5.0      -0.336810\n95454        1     9.0    12.0       0.041625\n95455        1    12.0     9.0       0.209711\n95457        0     6.0     3.0       0.158154\n95458        0    10.0     9.0       0.949587\n95463        0     6.0     3.0      -0.318071\n95466        0     9.0    10.0      -0.996637\n95488        0     6.0     8.0      -1.530299\n95489        1     6.0     8.0      -0.785422\n95490        1     8.0     6.0       1.384378\n95491        1     5.0     8.0      -0.203632\n95492        1     9.0    15.0      -1.184397\n95493        1     3.0     2.0      -1.045390\n95494        0     8.0     5.0      -0.277107\n95495        0     9.0    15.0      -1.989885\n95496        1     2.0     3.0       1.006644\n95507        1     5.0     8.0      -0.333311\n95509        0    15.0     9.0       0.569866\n95512        1     6.0     5.0      -0.315236\n95514        1     9.0    15.0      -1.777143\n95516        0     5.0     6.0       0.433163\n95520        0     9.0    15.0      -0.087839\n95521        1     6.0     5.0      -0.261706\n95525        0     5.0     3.0       0.152257\n95529        1     3.0     5.0       0.086812\n95531        1     5.0     3.0       0.122923\n95534        1     2.0     1.0       0.053074\n95535        0     1.0     2.0       0.037637"
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['fav_win','tier_1','tier_2','team_rank_dif']].head(n=30)\n",
    "# X = df_train.iloc[:, 2:15]\n",
    "# np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array(['fav_win', 't1_win', 'fav_ind', 'prize_rating_dif', 'rating_dif',\n       'hs_perc_dif', 'kills_per_rd_dif', 'deaths_per_rd_dif', 'adr_dif',\n       'kast_dif', 'assists_per_rd_dif', 'flash_per_rd_dif',\n       'first_kills_dif_dif', 'team_rank_dif', 'score_dif_dif',\n       'win_rate_dif', 'scaled_win_dif', 'scaled_rating_dif',\n       'scaled_score_dif_dif', 'win_rate_map_dif', 'kd_per_round_dif',\n       'scaled_kd_dif', 'momentum_dif', 'map_rating_dif', 'tier_1',\n       'tier_2', 'fav_ind_2', 'fav_ind_3'], dtype=object)"
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.keys().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6373\nAUC Score (Train): 0.681221\n\n>              precision    recall  f1-score   support\n\n     T Start       0.61      0.62      0.62      6985\n    CT Start       0.62      0.61      0.61      7007\n\n   micro avg       0.62      0.62      0.62     13992\n   macro avg       0.62      0.62      0.62     13992\nweighted avg       0.62      0.62      0.62     13992\n\nAccuracy (Test): 0.6156\nAUC Score (Test): 0.615642\n--------------------------------------------------------------\nXGBoost\n\nAccuracy (Train): 0.6262\nAUC Score (Train): 0.674829\n\n>              precision    recall  f1-score   support\n\n     T Start       0.64      0.61      0.62      7028\n    CT Start       0.62      0.65      0.64      6964\n\n   micro avg       0.63      0.63      0.63     13992\n   macro avg       0.63      0.63      0.63     13992\nweighted avg       0.63      0.63      0.63     13992\n\nAccuracy (Test): 0.6306\nAUC Score (Test): 0.630680\n--------------------------------------------------------------\nAvg accuracy (Test) = 0.623106\n--------------------------------------------------------------\n"
    }
   ],
   "source": [
    "# Quick XGBoost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "#from sklearn.metrics import cross_validation   #Additional scklearn functions\n",
    "#from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import plot_roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# model = XGBClassifier(\n",
    "#     learning_rate =0.1,\n",
    "#     n_estimators=600,\n",
    "#     max_depth=5,\n",
    "#     min_child_weight=5,\n",
    "#     gamma=1,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     objective= 'binary:logistic',\n",
    "#     nthread=4,\n",
    "#     scale_pos_weight=1,\n",
    "#     seed=0)\n",
    "\n",
    "\n",
    "\n",
    "all_keys = ['fav_ind','prize_rating_dif', 'rating_dif', 'hs_perc_dif',\n",
    "       'kills_per_rd_dif', 'deaths_per_rd_dif', 'adr_dif', 'kast_dif',\n",
    "       'assists_per_rd_dif', 'flash_per_rd_dif', 'first_kills_dif_dif',\n",
    "       'team_rank_dif', 'score_dif_dif', 'win_rate_dif',\n",
    "       'scaled_win_dif','scaled_rating_dif','scaled_score_dif']\n",
    "\n",
    "features = ['t1_win','tier_1','tier_2','fav_ind','fav_ind_2','fav_ind_3','prize_rating_dif', 'rating_dif', 'deaths_per_rd_dif', \n",
    "       'assists_per_rd_dif', 'flash_per_rd_dif', 'first_kills_dif_dif',\n",
    "       'score_dif_dif', 'win_rate_dif','scaled_win_dif',\n",
    "       'scaled_rating_dif','scaled_score_dif_dif','win_rate_map_dif']\n",
    "\n",
    "\n",
    "features = ['t1_win', 'fav_ind', 'prize_rating_dif', 'rating_dif',\n",
    "       'hs_perc_dif', 'kills_per_rd_dif', 'deaths_per_rd_dif', 'adr_dif',\n",
    "       'kast_dif', 'assists_per_rd_dif', 'flash_per_rd_dif',\n",
    "       'first_kills_dif_dif', 'team_rank_dif', 'score_dif_dif',\n",
    "       'win_rate_dif', 'scaled_win_dif', 'scaled_rating_dif',\n",
    "       'scaled_score_dif_dif', 'win_rate_map_dif', 'kd_per_round_dif',\n",
    "       'scaled_kd_dif', 'tier_1', 'tier_2', 'fav_ind_2', 'fav_ind_3','momentum_dif','map_rating_dif']\n",
    "\n",
    "# These features are giving 63%\n",
    "features = ['t1_win', 'prize_rating_dif',  'score_dif_dif', \n",
    "        'win_rate_dif', 'scaled_win_dif', 'scaled_rating_dif','win_rate_map_dif', 'kd_per_round_dif',\n",
    "        'momentum_dif']\n",
    "\n",
    "labels = ['Team 1 win', '$\\Delta$ Prize Rating', '$\\Delta$ Score', '$\\Delta$ Win %', '$\\Delta$ Scaled Win',\n",
    "        '$\\Delta$ Scaled Rating', '$\\Delta$ Win % Map', '$\\Delta$ KD per Round', '$\\Delta$ Momentum']\n",
    "\n",
    "\n",
    "\n",
    "# features = ['t1_win', 'fav_ind', 'prize_rating_dif', 'rating_dif',\n",
    "#        'hs_perc_dif', 'kills_per_rd_dif', 'deaths_per_rd_dif', 'adr_dif',\n",
    "#        'kast_dif', 'assists_per_rd_dif', 'flash_per_rd_dif',\n",
    "#        'first_kills_dif_dif', 'team_rank_dif', 'score_dif_dif',\n",
    "#        'win_rate_dif', 'scaled_win_dif', 'scaled_rating_dif',\n",
    "#        'scaled_score_dif_dif', 'win_rate_map_dif', 'kd_per_round_dif',\n",
    "#        'scaled_kd_dif', 'momentum_dif', 'map_rating_dif', 'tier_1',\n",
    "#        'tier_2', 'fav_ind_2', 'fav_ind_3']\n",
    "\n",
    "\n",
    "# features = ['t1_win','fav_ind','fav_ind_2']\n",
    "\n",
    "# features = ['t1_win','win_rate_dif','team_rank_dif']\n",
    "\n",
    "# df_tmp = df_train[ features ].head(n=10000).copy()\n",
    "\n",
    "df_tmp = df_train[ features ].copy()\n",
    "\n",
    "# Get rid of outliers\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['win_rate_map_dif'])<3.0  ]\n",
    "df_tmp = df_tmp[ np.abs(df_tmp['scaled_rating_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['adr_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['assists_per_rd_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['deaths_per_rd_dif'])<3.0  ]\n",
    "df_tmp = df_tmp[ np.abs(df_tmp['kd_per_round_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['map_rating_dif'])<3.0  ]\n",
    "df_tmp = df_tmp[ np.abs(df_tmp['scaled_win_dif'])<3.0  ]\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['scaled_score_dif_dif'])<3.0  ]\n",
    "\n",
    "\n",
    "# df_tmp = df_tmp[ np.abs(df_tmp['team_rank_dif'])<1.5]\n",
    "\n",
    "n = len(df_tmp.keys())\n",
    "X = df_tmp.iloc[:, 1:n].values\n",
    "y = df_tmp.iloc[:, 0].values\n",
    "\n",
    "# Scale all variables to [0,1]\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=2, random_state=None, shuffle=True)\n",
    "\n",
    "\n",
    "model = XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=500,\n",
    "        max_depth=3,\n",
    "        min_child_weight=1,\n",
    "        gamma=10,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1 )\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200,max_depth=3, random_state=1, bootstrap=False,n_jobs=6)\n",
    "\n",
    "\n",
    "accuracy_vec = []\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "#for test_index, train_index in cv.split(X):\n",
    "    # print(\"Train Index: \", train_index, \"\\n\")\n",
    "    # print(\"Test Index: \", test_index)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X[train_index,:], X[test_index,:], y[train_index], y[test_index]\n",
    "    # best_svr.fit(X_train, y_train)\n",
    "    # scores.append(best_svr.score(X_test, y_test))\n",
    "    \n",
    "#     model.fit(X_train,y_train) \n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_train_pred = model.predict(X_train)\n",
    "#     y_train_proba = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_train_proba = clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('XGBoost')\n",
    "    print('')\n",
    "    print('Accuracy (Train): %.4g' % accuracy_score(y_train, y_train_pred) )\n",
    "    print('AUC Score (Train): %f' % roc_auc_score(y_train, y_train_proba) )\n",
    "    print('')\n",
    "    print('>'+classification_report(y_test,predictions,target_names=['T Start','CT Start']))\n",
    "\n",
    "    print('Accuracy (Test): %.4g' % accuracy_score(y_test, y_pred) )\n",
    "    print('AUC Score (Test): %f' % roc_auc_score(y_test, y_pred) )\n",
    "\n",
    "    accuracy_vec.append( accuracy_score(y_test, y_pred) )\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------')\n",
    "print('Avg accuracy (Test) = %f' % np.mean(accuracy_vec) )\n",
    "print('--------------------------------------------------------------')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=5)\n",
    "\n",
    "plt.figure(figsize=(20,18));\n",
    "\n",
    "\n",
    "\n",
    "mpl.rcParams.update({'text.color' : \"white\",\n",
    "                     'axes.labelcolor' : \"white\",\n",
    "                     'xtick.color' : \"white\",\n",
    "                     'ytick.color' : \"white\"})\n",
    "\n",
    "ax = plt.axes()\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, clf.predict_proba(X_test)[:,1] )  # doctest: +SKIP\n",
    "\n",
    "ax.plot( fpr, tpr, lw=10, color='g',\n",
    "        label='AUC = %.2f' % (roc_auc_score(y_test, y_pred)), alpha=.8 )\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=10, color='r',\n",
    "        label='Chance', alpha=.8 )\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.savefig('ROC_feb9.png', transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, ..., 0, 0, 1])"
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final_model_rand_forest_feb_9_2020.sav'\n",
    "pickle.dump( clf, open(filename,'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'0.20.3'"
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "#              axis=0)\n",
    "\n",
    "order = np.argsort( clf.feature_importances_ )[::-1][:n]\n",
    "feat_new = [labels[order[ind]+1] for ind in range(0,n-1)]\n",
    "\n",
    "# df = pd.DataFrame( {'importance':np.array(clf.feature_importances_[order]), \n",
    "#     'features':feat_new, 'std':std[order]} )\n",
    "\n",
    "\n",
    "dic_aux = {}\n",
    "dic_aux['Feature'] = []\n",
    "dic_aux['Importance'] = []\n",
    "count = -1\n",
    "for tree in clf.estimators_:\n",
    "    count = count+1\n",
    "    \n",
    "    for ind in range(0,n-1):\n",
    "        dic_aux['Feature'].append( labels[ind+1] )\n",
    "        dic_aux['Importance'].append( tree.feature_importances_[ind] )\n",
    "\n",
    "df = pd.DataFrame.from_dict(dic_aux,orient=\"columns\")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=4)\n",
    "\n",
    "\n",
    "mpl.rcParams.update({'text.color' : \"white\",\n",
    "                     'axes.labelcolor' : \"white\",\n",
    "                     'xtick.color' : \"white\",\n",
    "                     'ytick.color' : \"white\"})\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12*1.5,8*1.5));\n",
    "ax = plt.axes()\n",
    "ax = sns.barplot(x=\"Importance\", y=\"Feature\", data=df, order=feat_new, capsize=0.2 )\n",
    "\n",
    "plt.savefig('importance_feb9.png', transparent=True, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/emmanuel/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3296: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared\n  exec(code_obj, self.user_global_ns, self.user_ns)\n-2.9953354787687565\n"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "ax = plt.axes()\n",
    "df_tmp.hist(ax=ax,bins=40)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "print(df_tmp['win_rate_map_dif'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df_tmp.drop(['t1_win'],axis=1).corr()\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=2.5)\n",
    "plt.figure(figsize=(15,15));\n",
    "\n",
    "mpl.rcParams.update({'text.color' : \"white\",\n",
    "                     'axes.labelcolor' : \"white\",\n",
    "                     'xtick.color' : \"white\",\n",
    "                     'ytick.color' : \"white\"})\n",
    "ax = plt.axes()\n",
    "\n",
    "sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70}, ax=ax,\n",
    "                xticklabels=labels[1:n],\n",
    "                yticklabels=labels[1:n])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('correlations_feb9.png', transparent=True, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"711.737812pt\" version=\"1.1\" viewBox=\"0 0 713.058281 711.737812\" width=\"713.058281pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 711.737812 \nL 713.058281 711.737812 \nL 713.058281 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 96.833281 663.1 \nL 699.858281 663.1 \nL 699.858281 13.2 \nL 96.833281 13.2 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 179.063963 663.1 \nL 179.063963 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- âˆ’4 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(154.739354 697.674844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 288.704872 663.1 \nL 288.704872 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- âˆ’2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(264.380263 697.674844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 398.345781 663.1 \nL 398.345781 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(387.847656 697.674844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 507.98669 663.1 \nL 507.98669 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(497.488565 697.674844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 617.627599 663.1 \nL 617.627599 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(607.129474 697.674844)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 96.833281 586.332463 \nL 699.858281 586.332463 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- âˆ’1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 598.869884)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 96.833281 462.241231 \nL 699.858281 462.241231 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- âˆ’0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 474.778653)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 96.833281 338.15 \nL 699.858281 338.15 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.852969 350.687422)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 96.833281 214.058769 \nL 699.858281 214.058769 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.5 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.852969 226.596191)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 96.833281 89.967537 \nL 699.858281 89.967537 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.852969 102.504959)scale(0.33 -0.33)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#pa64e8e35e1)\" d=\"M 124.243509 633.559091 \nL 135.229572 630.00651 \nL 145.117028 626.5859 \nL 155.004485 622.932724 \nL 164.891942 619.024189 \nL 173.680792 615.314943 \nL 182.469643 611.363703 \nL 191.258493 607.148201 \nL 198.948737 603.223481 \nL 206.638981 599.05926 \nL 214.329225 594.635497 \nL 222.01947 589.93024 \nL 228.611107 585.654966 \nL 235.202745 581.138844 \nL 241.794383 576.364192 \nL 248.386021 571.312012 \nL 254.977659 565.961957 \nL 261.569296 560.292321 \nL 268.160934 554.280066 \nL 273.653966 548.990643 \nL 279.146997 543.431787 \nL 284.640029 537.588399 \nL 290.13306 531.445058 \nL 295.626092 524.986193 \nL 301.119123 518.196307 \nL 306.612155 511.06026 \nL 312.105186 503.563623 \nL 317.598218 495.693109 \nL 323.091249 487.437084 \nL 328.584281 478.786161 \nL 334.077312 469.733874 \nL 339.570344 460.277411 \nL 345.063376 450.418394 \nL 350.556407 440.163653 \nL 357.148045 427.354064 \nL 363.739683 414.028065 \nL 370.331321 400.235013 \nL 378.021565 383.639417 \nL 387.909021 361.703613 \nL 421.965817 285.488672 \nL 429.656061 269.113751 \nL 436.247699 255.547118 \nL 442.839337 242.474088 \nL 449.430974 229.936591 \nL 454.924006 219.918147 \nL 460.417037 210.300193 \nL 465.910069 201.086342 \nL 471.4031 192.275462 \nL 476.896132 183.862457 \nL 482.389163 175.839023 \nL 487.882195 168.19435 \nL 493.375227 160.915745 \nL 498.868258 153.98918 \nL 504.36129 147.399753 \nL 509.854321 141.132072 \nL 515.347353 135.170566 \nL 520.840384 129.499731 \nL 526.333416 124.10432 \nL 532.925053 117.972521 \nL 539.516691 112.191302 \nL 546.108329 106.737198 \nL 552.699967 101.588054 \nL 559.291605 96.72307 \nL 565.883243 92.122801 \nL 572.47488 87.769129 \nL 580.165124 82.979128 \nL 587.855369 78.477266 \nL 595.545613 74.241015 \nL 603.235857 70.249806 \nL 610.926101 66.48487 \nL 619.714951 62.437268 \nL 628.503802 58.639738 \nL 637.292652 55.071424 \nL 647.180109 51.307727 \nL 657.067566 47.786404 \nL 666.955022 44.486063 \nL 672.448054 42.740909 \nL 672.448054 42.740909 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 96.833281 663.1 \nL 96.833281 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 699.858281 663.1 \nL 699.858281 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 96.833281 663.1 \nL 699.858281 663.1 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 96.833281 13.2 \nL 699.858281 13.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa64e8e35e1\">\n   <rect height=\"649.9\" width=\"603.025\" x=\"96.833281\" y=\"13.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": "<Figure size 720x720 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "alpha = 0.1\n",
    "\n",
    "# In units of STD\n",
    "n = 500\n",
    "x_max = 5.0\n",
    "x = np.array( [ -x_max + (2.0*x_max)*ind/(n-1) for ind in range(0,n) ] )\n",
    "\n",
    "# y = alpha + (1.0-alpha)*(420-x)/420.0\n",
    "y = np.arctan((x)*0.50)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.0"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(11/10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}